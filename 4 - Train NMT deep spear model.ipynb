{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "import pickle as cPickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim.models as g\n",
    "#from util import *\n",
    "# from sonnet_model import SonnetModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.corpus import cmudict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# constants\n",
    "pad_symbol = \"<pad>\"\n",
    "end_symbol = \"<eos>\"\n",
    "unk_symbol = \"<unk>\"\n",
    "dummy_symbols = [pad_symbol, end_symbol, unk_symbol]\n",
    "rhyme_thresholds = [0.9, 0.8, 0.7, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "###preprocessing options###\n",
    "word_minfreq=3\n",
    "\n",
    "###hyper-parameters###\n",
    "seed=0\n",
    "batch_size=4\n",
    "keep_prob=0.7\n",
    "epoch_size=1\n",
    "max_grad_norm=5\n",
    "#language model\n",
    "word_embedding_dim=100\n",
    "word_embedding_model=\"pretrain_word2vec/dim100/word2vec.bin\"\n",
    "lm_enc_dim=200\n",
    "lm_dec_dim=600\n",
    "lm_dec_layer_size=1\n",
    "lm_attend_dim=25\n",
    "lm_learning_rate=0.2\n",
    "#pentameter model\n",
    "char_embedding_dim=150\n",
    "pm_enc_dim=50\n",
    "pm_dec_dim=200\n",
    "pm_attend_dim=50\n",
    "pm_learning_rate=0.001\n",
    "repeat_loss_scale=1.0\n",
    "cov_loss_scale=1.0\n",
    "cov_loss_threshold=0.7\n",
    "sigma=1.00\n",
    "#rhyme model\n",
    "rm_dim=100\n",
    "rm_neg=5 #extra randomly sampled negative examples\n",
    "rm_delta=0.5\n",
    "rm_learning_rate=0.001\n",
    "\n",
    "###sonnet hyper-parameters###\n",
    "bptt_truncate=2 #number of sonnet lines to truncate bptt\n",
    "doc_lines=14 #total number of lines for a sonnet\n",
    "\n",
    "###misc###\n",
    "verbose=False\n",
    "save_model=True\n",
    "\n",
    "###input/output###\n",
    "output_dir=\"output\"\n",
    "train_data=\"datasets/gutenberg/debug.txt\"#\"datasets/gutenberg/sonnet_train.txt\"\n",
    "valid_data=\"datasets/gutenberg/debug.txt\"#\"datasets/gutenberg/sonnet_valid.txt\"\n",
    "test_data=\"datasets/gutenberg/debug.txt\"#\"datasets/gutenberg/sonnet_test.txt\"\n",
    "output_prefix=\"wmin%d_sd%d_bat%d_kp%.1f_eph%d_grd%d_wdim%d_lmedim%d_lmddim%d_lmdlayer%d_lmadim%d_lmlr%.1f_cdim%d_pmedim%d_pmddim%d_pmadim%d_pmlr%.1E_loss%.1f-%.1f-%.1f_sm%.2f_rmdim%d_rmn%d_rmd%.1f_rmlr%.1E_son%d-%d\" % \\\n",
    "    (word_minfreq, seed, batch_size, keep_prob, epoch_size, max_grad_norm, word_embedding_dim, lm_enc_dim,\n",
    "    lm_dec_dim, lm_dec_layer_size, lm_attend_dim, lm_learning_rate,\n",
    "    char_embedding_dim, pm_enc_dim, pm_dec_dim, pm_attend_dim, pm_learning_rate, repeat_loss_scale,\n",
    "    cov_loss_scale, cov_loss_threshold, sigma, rm_dim, rm_neg, rm_delta, rm_learning_rate, bptt_truncate, doc_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import sonnet_nmt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(string):\n",
    "    return \" \".join(\"\".join([ item for item in string if (item.isalpha() or item == \" \") ]).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import codecs\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_vocab(corpus, word_minfreq, dummy_symbols):\n",
    "    idxword, idxchar = [], []\n",
    "    wordxid, charxid = defaultdict(int), defaultdict(int)\n",
    "    word_freq, char_freq = defaultdict(int), defaultdict(int)\n",
    "    wordxchar = defaultdict(list)\n",
    "\n",
    "    def update_dic(symbol, idxvocab, vocabxid):\n",
    "        if symbol not in vocabxid:\n",
    "            idxvocab.append(symbol)\n",
    "            vocabxid[symbol] = len(idxvocab) - 1 \n",
    "\n",
    "    for line_id, line in enumerate(codecs.open(corpus, \"r\", \"utf-8\")):\n",
    "        for word in line.strip().split():\n",
    "            word_freq[word] += 1\n",
    "        for char in line.strip():\n",
    "            char_freq[char] += 1\n",
    "\n",
    "    #add in dummy symbols into dictionaries\n",
    "    for s in dummy_symbols:\n",
    "        update_dic(s, idxword, wordxid)\n",
    "        update_dic(s, idxchar, charxid)\n",
    "\n",
    "    #remove low fequency words/chars\n",
    "    def collect_vocab(vocab_freq, idxvocab, vocabxid):\n",
    "        for w, f in sorted(list(vocab_freq.items()), key=operator.itemgetter(1), reverse=True):\n",
    "            if f < word_minfreq:\n",
    "                break\n",
    "            else:\n",
    "                update_dic(w, idxvocab, vocabxid)\n",
    "\n",
    "    collect_vocab(word_freq, idxword, wordxid)\n",
    "    collect_vocab(char_freq, idxchar, charxid)\n",
    "\n",
    "    #word id to [char ids]\n",
    "    dummy_symbols_set = set(dummy_symbols)\n",
    "    for wi, w in enumerate(idxword):\n",
    "        if w in dummy_symbols:\n",
    "            wordxchar[wi] = [wi]\n",
    "        else:\n",
    "            for c in w:\n",
    "                wordxchar[wi].append(charxid[c] if c in charxid else charxid[dummy_symbols[2]])\n",
    "\n",
    "    return idxword, wordxid, idxchar, charxid, wordxchar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First pass to collect word and character vocabulary...\n",
      "\n",
      "Word type size = 22\n",
      "\n",
      "Char type size = 29\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "print(\"\\nFirst pass to collect word and character vocabulary...\")\n",
    "idxword, wordxid, idxchar, charxid, wordxchar = load_vocab(cf.train_data, cf.word_minfreq, dummy_symbols)\n",
    "print(\"\\nWord type size =\", len(idxword))\n",
    "print(\"\\nChar type size =\", len(idxchar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading word embedding model...\n",
      "\n",
      "First pass to collect word and character vocabulary...\n",
      "\n",
      "Word type size = 22\n",
      "\n",
      "Char type size = 29\n",
      "\n",
      "Loading train and valid data...\n",
      "\n",
      "Train statistics:\n",
      "  Number of documents         = 2\n",
      "  Number of rhyme examples    = 24\n",
      "  Total number of word tokens = 278\n",
      "  Mean/min/max words per line = 9.93/7/13\n",
      "  Total number of char tokens = 1193\n",
      "  Mean/min/max chars per line = 42.61/37/51\n",
      "\n",
      "Valid statistics:\n",
      "  Number of documents         = 2\n",
      "  Number of rhyme examples    = 24\n",
      "  Total number of word tokens = 278\n",
      "  Mean/min/max words per line = 9.93/7/13\n",
      "  Total number of char tokens = 1193\n",
      "  Mean/min/max chars per line = 42.61/37/51\n",
      "\n",
      "Test statistics:\n",
      "  Number of documents         = 2\n",
      "  Number of rhyme examples    = 24\n",
      "  Total number of word tokens = 278\n",
      "  Mean/min/max words per line = 9.93/7/13\n",
      "  Total number of char tokens = 1193\n",
      "  Mean/min/max chars per line = 42.61/37/51\n"
     ]
    }
   ],
   "source": [
    "global wordxid, idxword, charxid, idxchar, \\\n",
    "wordxchar, train_lm, train_pm, train_rm\n",
    "\n",
    "random.seed(cf.seed)\n",
    "np.random.seed(cf.seed)\n",
    "\n",
    "if cf.word_embedding_model:\n",
    "    print(\"\\nLoading word embedding model...\")\n",
    "    mword = g.Word2Vec.load(cf.word_embedding_model)\n",
    "    cf.word_embedding_dim = mword.vector_size\n",
    "\n",
    "# load vocab\n",
    "print(\"\\nFirst pass to collect word and character vocabulary...\")\n",
    "idxword, wordxid, idxchar, charxid, wordxchar = load_vocab(cf.train_data, cf.word_minfreq, dummy_symbols)\n",
    "print(\"\\nWord type size =\", len(idxword))\n",
    "print(\"\\nChar type size =\", len(idxchar))\n",
    "\n",
    "\n",
    "# load train and valid data\n",
    "print(\"\\nLoading train and valid data...\")\n",
    "\n",
    "train_word_data, train_char_data, train_rhyme_data, train_nwords, train_nchars = \\\n",
    "    load_data(cf.train_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "valid_word_data, valid_char_data, valid_rhyme_data, valid_nwords, valid_nchars = \\\n",
    "    load_data(cf.valid_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "print_stats(\"\\nTrain\", train_word_data, train_rhyme_data, train_nwords, train_nchars)\n",
    "print_stats(\"\\nValid\", valid_word_data, valid_rhyme_data, valid_nwords, valid_nchars)\n",
    "\n",
    "# load test data if it's given\n",
    "if cf.test_data:\n",
    "    test_word_data, test_char_data, test_rhyme_data, test_nwords, test_nchars = \\\n",
    "        load_data(cf.test_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "    print_stats(\"\\nTest\", test_word_data, test_rhyme_data, test_nwords, test_nchars)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.sonnet_nmt_model import  SonnetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(sess, word_batches, char_batches, rhyme_batches, model, pname,\n",
    "              is_training):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # lm variables\n",
    "    lm_costs = 0.0\n",
    "    total_words = 0\n",
    "    zero_state = sess.run(model.lm_initial_state)\n",
    "    model_state = None\n",
    "    prev_doc = -1\n",
    "    lm_train_op = model.lm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    # pm variables\n",
    "    #     pm_costs = 0.0\n",
    "    #     pm_train_op = model.pm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    #     # rm variables\n",
    "    #     rm_costs = 0.0\n",
    "    #     rm_train_op = model.rm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    # mix lm and pm batches\n",
    "    mixed_batch_types = [0] * len(word_batches) + [1] * len(\n",
    "        char_batches) + [2] * len(rhyme_batches)\n",
    "    random.shuffle(mixed_batch_types)\n",
    "    mixed_batches = [word_batches, char_batches, rhyme_batches]\n",
    "\n",
    "    word_batch_id = 0\n",
    "    char_batch_id = 0\n",
    "    rhyme_batch_id = 0\n",
    "\n",
    "    # cmu pronounciation dictionary for stress and rhyme evaluation\n",
    "    cmu = cmudict.dict()\n",
    "\n",
    "    # stress prediction\n",
    "    stress_acc = [[], [], []]  # buckets for char length: [1-4], [5-8], [9-inf]\n",
    "\n",
    "    # rhyme predition\n",
    "    rhyme_pr = {}  # precision/recall for each rhyme threshold\n",
    "    for rt in rhyme_thresholds:\n",
    "        rhyme_pr[rt] = [[], []]\n",
    "\n",
    "    # rhyme pattern\n",
    "    rhyme_pattern = []  # collection of cosine similarities\n",
    "    for i in range(12):\n",
    "        rhyme_pattern.append([])\n",
    "        for j in range(4):\n",
    "            if i % 4 == j:\n",
    "                rhyme_pattern[i].append([-2.0])\n",
    "            else:\n",
    "                rhyme_pattern[i].append([])\n",
    "\n",
    "    print('stress_acc', stress_acc)\n",
    "    print('rhyme_pr', rhyme_pr)\n",
    "    print('rhyme_pattern', rhyme_pattern)\n",
    "\n",
    "    print('mixed_batch_types', mixed_batch_types)\n",
    "    print('len mixed_batch_types', len(mixed_batch_types))\n",
    "\n",
    "    for bi, batch_type in tqdm(enumerate(mixed_batch_types)):\n",
    "\n",
    "        if batch_type == 0 and train_lm:\n",
    "\n",
    "            b = mixed_batches[batch_type][word_batch_id]\n",
    "\n",
    "            # reset model state if it's a different set of documents\n",
    "            if prev_doc != b[2][0]:\n",
    "                model_state = zero_state\n",
    "                prev_doc = b[2][0]\n",
    "\n",
    "            # preprocess character input to [batch_size*doc_len, char_len]\n",
    "            pm_enc_x = np.array(b[5]).reshape((cf.batch_size * max(b[3]), -1))\n",
    "\n",
    "            feed_dict = {\n",
    "                model.lm_x: b[0],\n",
    "                model.lm_y: b[1],\n",
    "                model.lm_xlen: b[3],\n",
    "                model.pm_enc_x: pm_enc_x,\n",
    "                model.pm_enc_xlen: np.array(b[6]).reshape((-1)),\n",
    "                model.lm_initial_state: model_state,\n",
    "                model.lm_hist: b[7],\n",
    "                model.lm_hlen: b[8]\n",
    "            }\n",
    "\n",
    "            cost, model_state, attns, _ = sess.run([\n",
    "                model.lm_cost, model.lm_final_state, model.lm_attentions,\n",
    "                lm_train_op\n",
    "            ], feed_dict)\n",
    "\n",
    "            lm_costs += cost * cf.batch_size  # keep track of full cost\n",
    "            total_words += sum(b[3])\n",
    "\n",
    "            word_batch_id += 1\n",
    "\n",
    "\n",
    "#         elif batch_type == 1 and train_pm:\n",
    "\n",
    "#             b = mixed_batches[batch_type][char_batch_id]\n",
    "\n",
    "#             feed_dict = {model.pm_enc_x: b[0], model.pm_enc_xlen: b[1], model.pm_cov_mask: b[2]}\n",
    "#             cost, attns, _, = sess.run([model.pm_mean_cost, model.pm_attentions, pm_train_op], feed_dict)\n",
    "#             pm_costs += cost\n",
    "\n",
    "#             char_batch_id += 1\n",
    "\n",
    "#             if not is_training:\n",
    "#                 eval_stress(stress_acc, cmu, attns, model.pentameter, b[0], idxchar, charxid, pad_symbol, cf)\n",
    "\n",
    "#         elif batch_type == 2 and train_rm:\n",
    "\n",
    "#             b = mixed_batches[batch_type][rhyme_batch_id]\n",
    "#             num_c = 3 + cf.rm_neg\n",
    "\n",
    "#             feed_dict = {model.pm_enc_x: b[0], model.pm_enc_xlen: b[1], model.rm_num_context: num_c}\n",
    "#             cost, attns, _ = sess.run([model.rm_cost, model.rm_attentions, rm_train_op], feed_dict)\n",
    "#             rm_costs += cost\n",
    "\n",
    "#             rhyme_batch_id += 1\n",
    "\n",
    "#             # if rhyme_batch_id < 10 and not is_training:\n",
    "#             #    print_rm_attention(b, cf.batch_size, num_c, attns, charxid[pad_symbol], idxchar)\n",
    "\n",
    "#             if not is_training:\n",
    "#                 eval_rhyme(rhyme_pr, rhyme_thresholds, cmu, attns, b, idxchar, charxid, pad_symbol, cf)\n",
    "#             else:\n",
    "#                 collect_rhyme_pattern(rhyme_pattern, attns, b, cf.batch_size, num_c, idxchar, charxid[pad_symbol])\n",
    "\n",
    "        if (((bi % 10) == 0)\n",
    "                and cf.verbose) or (bi == len(mixed_batch_types) - 1):\n",
    "\n",
    "            partition = \"  \" + pname\n",
    "            sent_end = \"\\n\" if bi == (len(mixed_batch_types) - 1) else \"\\r\"\n",
    "            speed = (bi + 1) / (time.time() - start_time)\n",
    "\n",
    "            sys.stdout.write(\"%s %d/%d: lm ppl = %.1f; pm loss = %.2f; rm loss = %.2f; batch/sec = %.1f%s\" % \\\n",
    "                             (partition, bi + 1, len(mixed_batch_types), np.exp(lm_costs / max(total_words, 1)),\n",
    "                              pm_costs / max(char_batch_id, 1), rm_costs / max(rhyme_batch_id, 1), speed, sent_end))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if not is_training and (bi == len(mixed_batch_types) - 1):\n",
    "\n",
    "                if train_pm:\n",
    "                    all_acc = [\n",
    "                        item for sublist in stress_acc for item in sublist\n",
    "                    ]\n",
    "                    stress_acc.append(all_acc)\n",
    "                    for acci, acc in enumerate(stress_acc):\n",
    "                        sys.stdout.write(\"    Stress acc [%d]   = %.3f (%d)\\n\"\n",
    "                                         % (acci, np.mean(acc), len(acc)))\n",
    "\n",
    "                if train_rm:\n",
    "                    for t in rhyme_thresholds:\n",
    "                        p = np.mean(rhyme_pr[t][0])\n",
    "                        r = np.mean(rhyme_pr[t][1])\n",
    "                        f = 2 * p * r / (p + r) if (p != 0.0\n",
    "                                                    and r != 0.0) else 0.0\n",
    "                        sys.stdout.write(\n",
    "                            \"    Rhyme P/R/F@%.1f  = %.3f / %.3f / %.3f\\n\" %\n",
    "                            (t, p, r, f))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    # return avg batch loss for lm, pm and rm\n",
    "    return lm_costs / max(word_batch_id, 1), pm_costs / max(char_batch_id, 1), rm_costs / max(rhyme_batch_id, 1), \\\n",
    "           rhyme_pattern, (np.mean(stress_acc[-1]) if not is_training else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ivan_kharitonov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/Yandex.Disk.localized/AWS_buffer_folder/my_deepspear/re collected deepspear/models/sonnet_nmt_model.py:120: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/Yandex.Disk.localized/AWS_buffer_folder/my_deepspear/re collected deepspear/models/sonnet_nmt_model.py:126: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/Yandex.Disk.localized/AWS_buffer_folder/my_deepspear/re collected deepspear/models/sonnet_nmt_model.py:142: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/Yandex.Disk.localized/AWS_buffer_folder/my_deepspear/re collected deepspear/models/sonnet_nmt_model.py:159: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/ivan_kharitonov/Yandex.Disk.localized/AWS_buffer_folder/my_deepspear/re collected deepspear/models/sonnet_nmt_model.py:183: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "\n",
      "Epoch = 1\n",
      "BATCH_LEN 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stress_acc [[], [], []]\n",
      "rhyme_pr {0.9: [[], []], 0.8: [[], []], 0.7: [[], []], 0.6: [[], []]}\n",
      "rhyme_pattern [[[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]], [[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]], [[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]]]\n",
      "mixed_batch_types [2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2]\n",
      "len mixed_batch_types 13\n",
      "  TRAIN 13/13: lm ppl = 1.0; pm loss = 0.00; rm loss = 0.00; batch/sec = 14.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 11817.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "\n",
    "        #         if cf.save_model:\n",
    "        #             if not os.path.exists(os.path.join(cf.output_dir, cf.output_prefix)):\n",
    "        #                 os.makedirs(os.path.join(cf.output_dir, cf.output_prefix))\n",
    "        #             # create saver object to save model\n",
    "        #             saver = tf.train.Saver(max_to_keep=0)\n",
    "\n",
    "        # train model\n",
    "        \n",
    "        \n",
    "# training parameters\n",
    "train_lm = True\n",
    "train_pm = True\n",
    "train_rm = True\n",
    "\n",
    "pm_costs ,rm_costs= 0.0, 0.0\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    tf.set_random_seed(cf.seed)\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n",
    "                             charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\n",
    "#                              charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mgen = SonnetModel(False, 1, len(idxword), len(\n",
    "#             idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # initialise word embedding\n",
    "    if cf.word_embedding_model:\n",
    "        word_emb = init_embedding(mword, idxword)\n",
    "        sess.run(mtrain.word_embedding.assign(word_emb))\n",
    "    # train model\n",
    "    prev_lm_loss, prev_pm_loss, prev_rm_loss, rhyme_pattern = None, None, None, None\n",
    "    for i in range(1):\n",
    "\n",
    "        print(\"\\nEpoch =\", i + 1)\n",
    "\n",
    "        # create batches for language model\n",
    "        train_word_batch = create_word_batch(train_word_data, cf.batch_size,\n",
    "                                             cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "                                             wordxid[end_symbol], wordxid[unk_symbol], True)\n",
    "#         valid_word_batch = create_word_batch(train_word_data, 1,\n",
    "#                                              cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "#                                              wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "\n",
    "#         # create batches for pentameter model\n",
    "        train_char_batch = create_char_batch(train_char_data, cf.batch_size,\n",
    "                                             charxid[pad_symbol], mtrain.pentameter, idxchar, True)\n",
    "#         valid_char_batch = create_char_batch(valid_char_data, cf.batch_size,\n",
    "#                                              charxid[pad_symbol], mtrain.pentameter, idxchar, False)\n",
    "\n",
    "#         # create batches for rhyme model\n",
    "        train_rhyme_batch = create_rhyme_batch(train_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "                                               cf.rm_neg, True)\n",
    "#         valid_rhyme_batch = create_rhyme_batch(valid_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "#                                                cf.rm_neg, False)\n",
    "\n",
    "        # train an epoch\n",
    "        _, _, _, new_rhyme_pattern, _ = run_epoch(sess, train_word_batch, train_char_batch,\n",
    "                                                  train_rhyme_batch, mtrain, \"TRAIN\", True)\n",
    "#         lm_loss, pm_loss, rm_loss, _, sacc = run_epoch(sess, valid_word_batch, valid_char_batch,\n",
    "#                                                        valid_rhyme_batch, mvalid, \"VALID\", False)\n",
    "\n",
    "#         # create batch for test model and run an epoch if it's given\n",
    "#         if cf.test_data:\n",
    "#             test_word_batch = create_word_batch(test_word_data, cf.batch_size,\n",
    "#                                                 cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "#                                                 wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "#             test_char_batch = create_char_batch(test_char_data, cf.batch_size,\n",
    "#                                                 charxid[pad_symbol], mtrain.pentameter, idxchar, False)\n",
    "#             test_rhyme_batch = create_rhyme_batch(test_rhyme_data, cf.batch_size,\n",
    "#                                                   charxid[pad_symbol], wordxchar, cf.rm_neg, False)\n",
    "#             run_epoch(sess, test_word_batch, test_char_batch,\n",
    "#                       test_rhyme_batch, mvalid, \"TEST\", False)\n",
    "\n",
    "#         # if pm performance is really poor, re-initialize network weights\n",
    "#         if train_pm and sacc < stress_acc_threshold and prev_pm_loss == None:\n",
    "#             print(\n",
    "#                 \"\\n  Valid stress accuracy performance is very poor; re-initializing network with random weights...\")\n",
    "#             tf.global_variables_initializer().run()\n",
    "#             continue\n",
    "\n",
    "#         # save model\n",
    "#         if cf.save_model:\n",
    "#             if prev_lm_loss == None or prev_pm_loss == None or prev_rm_loss == None or \\\n",
    "#                     ((lm_loss <= prev_lm_loss or not train_lm) and\n",
    "#                      (pm_loss <= prev_pm_loss * reset_scale or not train_pm) and\n",
    "#                      (rm_loss <= prev_rm_loss * reset_scale or not train_rm)):\n",
    "#                 saver.save(sess, os.path.join(\n",
    "#                     cf.output_dir, cf.output_prefix, \"model.ckpt\"))\n",
    "#                 prev_lm_loss, prev_pm_loss, prev_rm_loss = lm_loss, pm_loss, rm_loss\n",
    "#                 rhyme_pattern = new_rhyme_pattern\n",
    "#             else:\n",
    "#                 saver.restore(sess, os.path.join(\n",
    "#                     cf.output_dir, cf.output_prefix, \"model.ckpt\"))\n",
    "#                 print(\n",
    "#                     \"New valid performance is worse; restoring previous parameters...\")\n",
    "#                 print(\"  lm loss: %.5f --> %.5f\" % (prev_lm_loss, lm_loss))\n",
    "#                 print(\"  pm loss: %.5f --> %.5f\" % (prev_pm_loss, pm_loss))\n",
    "#                 print(\"  rm loss: %.5f --> %.5f\" % (prev_rm_loss, rm_loss))\n",
    "#                 sys.stdout.flush()\n",
    "#         else:\n",
    "#             rhyme_pattern = new_rhyme_pattern\n",
    "\n",
    "#     # print global rhyme pattern\n",
    "#     if train_rm:\n",
    "#         print(\"\\nAggregated Rhyme Pattern:\")\n",
    "#         for i in range(len(rhyme_pattern)):\n",
    "#             if i % 4 == 0:\n",
    "#                 print(\"\\n  Quatrain\", i / 4, \":\")\n",
    "#             print(\"    Line %02d =\" % i),\n",
    "#             for j in range(len(rhyme_pattern[i])):\n",
    "#                 print(\"%.2f\" % np.mean(rhyme_pattern[i][j])).rjust(7),\n",
    "#         print\n",
    "\n",
    "#     # save vocab information and config\n",
    "#     if cf.save_model:\n",
    "#         # vocab\n",
    "#         cPickle.dump((idxword, idxchar, wordxchar),\n",
    "#                      open(os.path.join(cf.output_dir, cf.output_prefix, \"vocabs.pickle\"), \"w\"))\n",
    "\n",
    "#         # create a dictionary object for config\n",
    "#         cf_dict = {}\n",
    "#         for k, v in vars(cf).items():\n",
    "#             if not k.startswith(\"__\"):\n",
    "#                 cf_dict[k] = v\n",
    "#         cPickle.dump(cf_dict, open(os.path.join(\n",
    "#             cf.output_dir, cf.output_prefix, \"config.pickle\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_LEN 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stress_acc [[], [], []]\n",
      "rhyme_pr {0.9: [[], []], 0.8: [[], []], 0.7: [[], []], 0.6: [[], []]}\n",
      "rhyme_pattern [[[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]], [[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]], [[-2.0], [], [], []], [[], [-2.0], [], []], [[], [], [-2.0], []], [[], [], [], [-2.0]]]\n",
      "mixed_batch_types [1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1]\n",
      "len mixed_batch_types 13\n",
      "  TRAIN 13/13: lm ppl = 1.0; pm loss = 0.00; rm loss = 0.00; batch/sec = 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 13416.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.set_random_seed(cf.seed)\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n",
    "                             charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\n",
    "#                              charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mgen = SonnetModel(False, 1, len(idxword), len(\n",
    "#             idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # create saver object to save model\n",
    "    saver = tf.train.Saver(max_to_keep=0)\n",
    "    pm_costs = 0.0\n",
    "    rm_costs = 0.0\n",
    "      # create batches for language model\n",
    "    train_word_batch = create_word_batch(train_word_data, cf.batch_size,\n",
    "                                             cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "                                             wordxid[end_symbol], wordxid[unk_symbol], True)\n",
    "#         valid_word_batch = create_word_batch(train_word_data, 1,\n",
    "#                                              cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "#                                              wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "\n",
    "#         # create batches for pentameter model\n",
    "    train_char_batch = create_char_batch(train_char_data, cf.batch_size,\n",
    "                                             charxid[pad_symbol], mtrain.pentameter, idxchar, True)\n",
    "#         valid_char_batch = create_char_batch(valid_char_data, cf.batch_size,\n",
    "#                                              charxid[pad_symbol], mtrain.pentameter, idxchar, False)\n",
    "\n",
    "#         # create batches for rhyme model\n",
    "    train_rhyme_batch = create_rhyme_batch(train_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "                                               cf.rm_neg, True)\n",
    "#         valid_rhyme_batch = create_rhyme_batch(valid_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "#                                                cf.rm_neg, False)\n",
    "\n",
    "    _, _, _, new_rhyme_pattern, _ = run_epoch(sess, train_word_batch, train_char_batch,\n",
    "                                                  train_rhyme_batch, mtrain, \"TRAIN\", True)\n",
    "    #saver.save(sess, \"/tmp/deepspear_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c9591e883b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/deepspear_model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1170\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    saver.save(sess, \"/tmp/deepspear_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\"/tmp/model1.ckpt\", tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\"/tmp/model1.ckpt\", tensor_name='', all_tensors=False,all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\"/tmp/deepspear_model.ckpt\", tensor_name='', all_tensors=False,all_tensor_names=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess,\"/tmp/model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model\", reuse=None):\n",
    "    mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n",
    "                             charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\n",
    "#                              charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mgen = SonnetModel(False, 1, len(idxword), len(\n",
    "#             idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "    tf.global_variables_initializer().run(session=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VARS_DICT = {'selector_network/c2w/var1': var1}\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver(var_list=VARS_DICT)\n",
    "    saver.restore(sess, '/tmp/model1.ckpt')\n",
    "# get the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()\n",
    "w1 = g.get_tensor_by_name('some_variable_name as per your definition in the model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
