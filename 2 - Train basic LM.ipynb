{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-152b50a4a1ee>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-152b50a4a1ee>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tf.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tf.keras.layers.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'datasets/OpenSubtitlesv2018/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstn=10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lines = []\n",
    "ru_lines = []\n",
    "with open(TRAIN_DIR + 'train.bpe.ru', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ru_lines.append(line)\n",
    "        if i > firstn:\n",
    "            break\n",
    "        \n",
    "with open(TRAIN_DIR + 'train.bpe.en', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        en_lines.append(line)\n",
    "        if i > firstn:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_out = np.array(ru_lines[:firstn])\n",
    "#data_inp = np.array(en_lines[:firstn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_inp.shape, data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: Looks like they did . DA@@ PH@@ N@@ E :\n",
      "\n",
      "out: Похоже , им это удалось .\n",
      "\n",
      "\n",
      "inp: Bill Carson !\n",
      "\n",
      "out: Билл Кар@@ сон !\n",
      "\n",
      "\n",
      "inp: She knows every corner of that room .\n",
      "\n",
      "out: Она знает каждый угол в той комнате .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(en_lines, ru_lines, test_size=10000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel:\n",
    "    def __init__(self, name, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        A simple encoder-decoder model\n",
    "        \"\"\"\n",
    "        self.name, self.inp_voc, self.out_voc = name, inp_voc, out_voc\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "\n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(\n",
    "                self.prev_state, self.prev_tokens)\n",
    "\n",
    "        self.weights = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :returns: initial decoder state tensors, one or many\n",
    "        \"\"\"\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            _, enc_last = tf.nn.dynamic_rnn(\n",
    "                self.enc0,\n",
    "                inp_emb,\n",
    "                sequence_length=inp_lengths,\n",
    "                dtype=inp_emb.dtype)\n",
    "        dec_start = self.dec_start(enc_last)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        [prev_dec] = prev_state\n",
    "        prev_emb = self.emb_out(prev_tokens[:, None])[:, 0]\n",
    "        with tf.variable_scope('dec0'):\n",
    "            new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n",
    "        output_logits = self.logits(new_dec_out)\n",
    "        return [new_dec_state], output_logits\n",
    "\n",
    "    def translate_lines(self, inp_lines, sess, inp_voc, out_voc, max_len=100):\n",
    "        \"\"\"\n",
    "        Translates a list of lines by greedily selecting most likely next token at each step\n",
    "        :returns: a list of output lines, a sequence of model states at each step\n",
    "        \"\"\"\n",
    "        state = sess.run(self.initial_state,\n",
    "                         {self.inp: inp_voc.to_matrix(inp_lines)})\n",
    "        outputs = [[self.out_voc.bos_ix] for _ in range(len(inp_lines))]\n",
    "        all_states = [state]\n",
    "        finished = [False] * len(inp_lines)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            state, logits = sess.run(\n",
    "                [self.next_state, self.next_logits], {\n",
    "                    **dict(zip(self.prev_state, state)), self.prev_tokens:\n",
    "                    [out_i[-1] for out_i in outputs]\n",
    "                })\n",
    "            next_tokens = np.argmax(logits, axis=-1)\n",
    "            all_states.append(state)\n",
    "            for i in range(len(next_tokens)):\n",
    "                outputs[i].append(next_tokens[i])\n",
    "                finished[i] |= next_tokens[i] == self.out_voc.eos_ix\n",
    "        return out_voc.to_lines(outputs), all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size, activ=tf.tanh,):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size # num units in encoder state\n",
    "        self.dec_size = dec_size # num units in decoder state\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # Tensorflow can define input sizes by itself\n",
    "            self.enc_att = L.Dense(self.hid_size)\n",
    "            self.dec_att = L.Dense(self.hid_size)\n",
    "            self.out_att = L.Dense(self.hid_size)\n",
    "\n",
    "    def __call__(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            # Compute logits\n",
    "            # We will use multiple logits for each attention sentence, may be it will improve score.\n",
    "            a = self.out_att(self.activ(self.enc_att(enc) + tf.expand_dims(self.dec_att(dec),1)))\n",
    "            # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "            # You may need tf.where\n",
    "            a = tf.where(inp_mask, a, tf.fill(tf.shape(a), -1e9))\n",
    "            # Compute attention probabilities (softmax)\n",
    "            \n",
    "            probs = tf.nn.softmax(a, axis=1)\n",
    "            \n",
    "            # Compute attention response using enc and probs\n",
    "            attn = tf.reduce_sum(tf.multiply(probs, a), axis=1)\n",
    "            \n",
    "            return attn, probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearTranslate(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc, config,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.hid_size=hid_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # define model layers\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            with tf.variable_scope(\"forward\"):\n",
    "                self.enc0 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "            with tf.variable_scope(\"backward\"):\n",
    "                self.enc1 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "                \n",
    "            self.attention = AttentionLayer('Attention', hid_size *4, hid_size, hid_size)\n",
    "            self.dec_start = L.Dense(hid_size*2)\n",
    "            \n",
    "            self.dec0 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        mask = tf.expand_dims(tf.cast(infer_mask(inp, self.inp_voc.eos_ix), tf.bool), axis=2)\n",
    "\n",
    "        inp_mask = tf.tile(mask, (1, 1, self.hid_size))\n",
    "        #inp_mask = tf.cast(infer_mask(inp, self.inp_voc.eos_ix), tf.bool)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        \n",
    "        with tf.variable_scope('enc0'):\n",
    "            enc_outputs_t, enc_last_t = tf.nn.bidirectional_dynamic_rnn(\n",
    "                              self.enc0, self.enc1, inp_emb,\n",
    "                              sequence_length=inp_lengths,\n",
    "                              dtype=inp_emb.dtype)\n",
    "            #[batch, time, hid_size*4]\n",
    "            enc_outputs = tf.concat(enc_outputs_t, 2)\n",
    "            #[batch, hid_size * 4]\n",
    "            enc_last = tf.concat(enc_last_t, 1)\n",
    "            \n",
    "        dec_start = self.dec_start(enc_last)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        attn, first_attn_probas = self.attention(enc_outputs, dec_start, inp_mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_outputs, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order that you've packed in encode\n",
    "        [prev_dec, enc_outputs, inp_mask, first_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and probas given previous decoder state\n",
    "        # * use prev tokens and next attn response to update decoder states\n",
    "        # * predict logits\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        \n",
    "        next_attn_response, next_attn_probas = self.attention(enc_outputs, prev_dec, inp_mask)\n",
    "        \n",
    "        with tf.variable_scope('dec0'):\n",
    "            dec_input = tf.concat([prev_emb, next_attn_response], axis=-1)\n",
    "            new_dec_out, new_dec_state = self.dec0(dec_input, prev_dec)\n",
    "        \n",
    "        output_logits = self.logits(new_dec_out)\n",
    "        \n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        \n",
    "        next_state = [new_dec_state, enc_outputs, inp_mask, next_attn_probas]\n",
    "        return next_state, output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_length(seq, eos_ix, time_major=False, dtype=tf.int32):\n",
    "    \"\"\"\n",
    "    compute length given output indices and eos code\n",
    "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
    "    :param eos_ix: integer index of end-of-sentence token\n",
    "    :returns: lengths, int32 vector of shape [batch]\n",
    "    \"\"\"\n",
    "    axis = 0 if time_major else 1\n",
    "    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n",
    "    count_eos = tf.cumsum(is_eos,axis=axis,exclusive=True)\n",
    "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos,0),dtype),axis=axis)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def infer_mask(seq, eos_ix, time_major=False, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    compute mask given output indices and eos code\n",
    "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
    "    :param eos_ix: integer index of end-of-sentence token\n",
    "    :returns: mask, float32 matrix with '0's and '1's of same shape as seq\n",
    "    \"\"\"\n",
    "    axis = 0 if time_major else 1\n",
    "    lengths = infer_length(seq, eos_ix, time_major=time_major)\n",
    "    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[axis], dtype=dtype)\n",
    "    if time_major: mask = tf.transpose(mask)\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.enc_dim=64\n",
    "#config.dec_dim=64\n",
    "#config.dec_layer_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan_kharitonov/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x1b25024898>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x1b25024860>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x1b25024668>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "model = SpearTranslate('Spear', inp_voc, out_voc, config, emb_size=100, hid_size=256, attn_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inp, out, out_voc, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    In order to pass the tests, your function should\n",
    "    * include loss at first EOS but not the subsequent ones\n",
    "    * divide sum of losses by a sum of input lengths (use infer_length or infer_mask)\n",
    "    \"\"\"\n",
    "    mask = infer_mask(out, out_voc.eos_ix)    \n",
    "    logits_seq = compute_logits(model, inp, out, **flags)\n",
    "    \n",
    "    # Compute loss as per instructions above\n",
    "    print(mask.shape)\n",
    "    print(logits_seq.shape)\n",
    "    sparce_softmax = tf.losses.sparse_softmax_cross_entropy(labels=out, logits=logits_seq)\n",
    "    return tf.reduce_sum(sparce_softmax * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "def compute_bleu(model, inp_lines, out_lines, sess, inp_voc, out_voc, bpe_sep='@@ ', **flags):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
    "    translations, _ = model.translate_lines(inp_lines, sess, inp_voc, out_voc, **flags)\n",
    "    # Note: if you experience out-of-memory error, split input lines into batches and translate separately\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100\n",
    "\n",
    "def compute_logits(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    :returns: logits of shape [batch, time, voc_size]\n",
    "    \n",
    "    * logits must be a linear output of your neural network.\n",
    "    * logits [:, 0, :] should always predic BOS\n",
    "    * logits [:, -1, :] should be probabilities of last token in out\n",
    "    This function should NOT return logits predicted when taking out[:, -1] as y_prev\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(inp)[0]\n",
    "    \n",
    "    # Encode inp, get initial state\n",
    "    first_state = model.encode(inp, **flags)\n",
    "    \n",
    "    # initial logits: always predict BOS\n",
    "    first_logits = tf.log(tf.one_hot(tf.fill([batch_size], model.out_voc.bos_ix),\n",
    "                                     len(model.out_voc)) + 1e-30)\n",
    "    \n",
    "    \n",
    "    # Decode step\n",
    "    def step(blob, y_prev):\n",
    "        # Given previous state, obtain next state and next token logits\n",
    "        # prev_tokens int vector of batch size??\n",
    "        # return state and logits\n",
    "        prev_state, logits = blob\n",
    "        state, logits = model.decode(prev_state, y_prev)\n",
    "        return [state, logits]\n",
    "      \n",
    "    # Decode step\n",
    "    #def step(prev_state, y_prev):\n",
    "        # Given previous state, obtain next state and next token logits\n",
    "        # prev_tokens int vector of batch size??\n",
    "        # return state and logits\n",
    "\n",
    "    #    return model.decode(prev_state, y_prev)\n",
    "\n",
    "    # You can now use tf.scan to run step several times.\n",
    "    # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "    \n",
    "    \n",
    "    _, logits_seq = tf.scan(step, \n",
    "           elems=tf.transpose(out)[:-1],\n",
    "           initializer=[first_state, first_logits])\n",
    "    \n",
    "    # prepend first_logits to logits_seq\n",
    "    logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
    "    \n",
    "    # Make sure you convert logits_seq from [time, batch, voc_size] to [batch, time, voc_size]\n",
    "    logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
    "    \n",
    "    return logits_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "(?, ?, 10701)\n"
     ]
    }
   ],
   "source": [
    "inp = tf.placeholder('int32', [None, None])\n",
    "out = tf.placeholder('int32', [None, None])\n",
    "\n",
    "loss = compute_loss(model, inp, out, out_voc)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'train_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar(\"train_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': []}\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990002"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "INP_SIZE = len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('./tmp/tensorboard/', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./tmp/tensorboard/', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_n = tf.Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/78046 [01:01<212:56:02,  9.82s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e3329ed72068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcur_ix\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH):\n",
    "    #while cur_ix < INP_SIZE:\n",
    "    cur_ix = 0\n",
    "    for _ in trange(INP_SIZE//batch_size):\n",
    "        step = i * INP_SIZE + cur_ix\n",
    "        feed_dict = {\n",
    "            inp: inp_voc.to_matrix(train_inp[cur_ix: cur_ix+batch_size]),\n",
    "            out: out_voc.to_matrix(train_out[cur_ix: cur_ix+batch_size]),\n",
    "        }\n",
    "        cur_ix += batch_size\n",
    "        \n",
    "        loss_t, _, summary = sess.run([loss, train_step, merged], feed_dict)\n",
    "        \n",
    "        \n",
    "        train_writer.add_summary(summary, step)\n",
    "        metrics['train_loss'].append((step, loss_t))\n",
    "        if cur_ix % 500 == 5:\n",
    "            bleus = []\n",
    "            cur_dev_ix = 0\n",
    "            dev_len = len(dev_inp)\n",
    "            while cur_dev_ix < dev_len:\n",
    "                bleus.append(compute_bleu(model, \n",
    "                                          dev_inp[cur_dev_ix:cur_dev_ix+batch_size], \n",
    "                                          dev_out[cur_dev_ix:cur_dev_ix+batch_size], \n",
    "                                          sess, inp_voc, out_voc))\n",
    "                cur_dev_ix += batch_size\n",
    "            \n",
    "            metrics['dev_bleu'].append((step, np.mean(bleus)))\n",
    "           \n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show()\n",
    "            print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-100:], axis=0)[1], flush=True)\n",
    "            print(\"EPOCH: \", i)\n",
    "            # Tensorboard\n",
    "            summary_n.value.add(tag=tagname, simple_value=metrics['dev_bleu'][-1])\n",
    "            train_writer.add_summary(summary_n, step)\n",
    "            train_writer.flush()\n",
    "        #if cur_ix % 1000 == 0:\n",
    "            saver.save(sess, \"/tmp/model1.ckpt\")\n",
    "            with open('/tmp/metrics.pkl', 'wb') as f:\n",
    "                pickle.dump(metrics, f)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "            inp: inp_voc.to_matrix(train_inp[:batch_size]),\n",
    "            out: out_voc.to_matrix(train_out[:batch_size]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t, _, summary = sess.run([loss, train_step, merged], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/model1.ckpt'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"/tmp/model1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model1.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"/tmp/model1.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Check the values of the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  Spear/Attention_1/dense_1/bias\n",
      "[ 0.00099998 -0.00099997  0.00099995  0.00099997  0.00099997  0.00099998\n",
      "  0.00099997 -0.00099997  0.00099998  0.00099997 -0.00099999  0.00099998\n",
      "  0.00099999  0.00099997 -0.00099993 -0.00099999  0.00099999  0.00099999\n",
      " -0.00099998 -0.0009998   0.00099959 -0.00099998 -0.00099996 -0.00099997\n",
      "  0.00099996  0.00099996  0.00099991  0.00099995 -0.00099999  0.00099996\n",
      " -0.00099996  0.00099999  0.00099991  0.0009999   0.00099997 -0.00099999\n",
      " -0.00099994  0.00099995 -0.00099998 -0.00099997 -0.00099997  0.00099696\n",
      "  0.00099998  0.00099997  0.00099998 -0.00099996 -0.00099998 -0.00099993\n",
      " -0.00099994 -0.00099999 -0.00099997  0.00099994  0.00099999  0.00099998\n",
      " -0.00099989  0.00099981 -0.00099995 -0.00099995  0.00099995 -0.00099996\n",
      "  0.00099994 -0.00099996 -0.0009924  -0.00099989  0.00099994  0.00099995\n",
      " -0.00099999  0.00099998  0.00099892 -0.00099998 -0.00099993  0.00099998\n",
      " -0.00099999 -0.00099966  0.00099952  0.00099999 -0.00099984 -0.00099998\n",
      " -0.00099993  0.00099999 -0.00099996  0.00099998  0.00099997  0.00099999\n",
      "  0.00099999  0.00099999  0.00099995 -0.00099998  0.00099998  0.00099998\n",
      "  0.00099992  0.00099999 -0.00099996  0.00099951 -0.00099999  0.0009999\n",
      "  0.00099994  0.00099995  0.00099998 -0.00099998  0.00099988  0.00099991\n",
      " -0.00099996 -0.00099995 -0.00099999  0.00099998 -0.00099999  0.00099997\n",
      "  0.00099996  0.00099997  0.00099997  0.00099978  0.00098692  0.00099819\n",
      " -0.0009952  -0.00099999  0.00099998  0.00099997  0.00099996  0.00099993\n",
      "  0.00099998  0.00099996 -0.00099999  0.00099996  0.00099994  0.00099986\n",
      "  0.00099998 -0.00099999 -0.00099995  0.00099996 -0.00099998  0.00099999\n",
      " -0.00099998  0.00099985 -0.00099999 -0.00099968 -0.00099997 -0.00099996\n",
      "  0.00099993  0.00099998 -0.00099999 -0.00099989 -0.00099995 -0.00099993\n",
      "  0.00099984 -0.00099998 -0.00099998 -0.00099941  0.00099997  0.00099997\n",
      "  0.00099999 -0.00099994 -0.00099996  0.00099999 -0.00099998  0.00099989\n",
      "  0.00099995  0.0009997   0.00099995 -0.00099996 -0.00099996  0.00099932\n",
      " -0.00099989 -0.00099995 -0.00099994  0.00099997  0.00099996 -0.00099993\n",
      "  0.00099999 -0.00099997 -0.00099998  0.00099999 -0.00099999 -0.00099968\n",
      "  0.00099996 -0.00099998 -0.00099996  0.00099998 -0.00099999  0.0009995\n",
      " -0.00099999 -0.00099999 -0.00099987 -0.00099992 -0.00099998 -0.00099997\n",
      " -0.00099997 -0.00099998 -0.00099996 -0.00099996 -0.00099997  0.00099999\n",
      "  0.00099997  0.00099993  0.00099984  0.00099997  0.00099998  0.00099997\n",
      " -0.00099999  0.00099998 -0.00099998 -0.00099998  0.00099995  0.00099997\n",
      "  0.00099998 -0.00099998  0.00099998  0.00099999 -0.00099999 -0.00099999\n",
      "  0.00099999 -0.00099999  0.00099999 -0.00099994 -0.00099999  0.0009999\n",
      "  0.00099997 -0.00099998 -0.0009997   0.00099994 -0.00099999 -0.00099996\n",
      "  0.00099996  0.00099861 -0.00099998 -0.00099993  0.00099999  0.00099969\n",
      " -0.00099907  0.00099971 -0.00099998 -0.00099984  0.00099999  0.00099997\n",
      " -0.00099996  0.00099983  0.00099997 -0.00099999 -0.00099998  0.00099999\n",
      "  0.00099998  0.00099996  0.00099998  0.00099997  0.00099997  0.00099996\n",
      " -0.00099999  0.00099999  0.00099997  0.00099999  0.00099849 -0.00099986\n",
      " -0.00099998  0.00099998 -0.00099999  0.00099999]\n",
      "tensor_name:  Spear/Attention_1/dense_1/bias/Adam\n",
      "[-1.57743169e-03  1.15897693e-03 -6.31721399e-04 -9.92236193e-04\n",
      " -1.15759461e-03 -1.61805097e-03 -1.23774784e-03  1.12914457e-03\n",
      " -1.54028274e-03 -9.74584895e-04  2.18810164e-03 -1.45004049e-03\n",
      " -2.80965539e-03 -1.04127929e-03  4.29092732e-04  2.70891562e-03\n",
      " -3.50941392e-03 -3.13190813e-03  1.83472026e-03  1.61948075e-04\n",
      " -7.74240834e-05  1.73427281e-03  7.85188400e-04  1.21053564e-03\n",
      " -7.99464644e-04 -8.24135845e-04 -3.45807901e-04 -6.72228402e-04\n",
      "  2.99472502e-03 -8.90587282e-04  8.24955758e-04 -2.54957634e-03\n",
      " -3.57007171e-04 -3.25880421e-04 -1.10314344e-03  2.60736956e-03\n",
      "  5.04591386e-04 -6.07195194e-04  1.52513327e-03  1.10760808e-03\n",
      "  9.20204038e-04 -1.03873008e-05 -1.76153006e-03 -1.05416914e-03\n",
      " -2.02974072e-03  8.98837519e-04  1.86713343e-03  4.44659643e-04\n",
      "  5.59024920e-04  4.30338923e-03  1.04759948e-03 -4.98939480e-04\n",
      " -2.24240776e-03 -1.73901301e-03  2.75634171e-04 -1.66718208e-04\n",
      "  6.92805159e-04  6.30971801e-04 -5.86943119e-04  7.27833307e-04\n",
      " -5.56698360e-04  8.98951141e-04  4.12847839e-06  3.00886924e-04\n",
      " -5.12562634e-04 -6.07434078e-04  2.82106735e-03 -1.26680243e-03\n",
      " -2.92493296e-05  1.49241497e-03  4.70590108e-04 -1.33944442e-03\n",
      "  2.31305277e-03  9.29118323e-05 -6.55298572e-05 -2.53523933e-03\n",
      "  1.94970286e-04  1.88592507e-03  4.74440050e-04 -2.62505095e-03\n",
      "  7.96854205e-04 -2.07672594e-03 -1.16628723e-03 -2.13100808e-03\n",
      " -2.19253963e-03 -2.19932711e-03 -5.78471518e-04  1.63106620e-03\n",
      " -1.41060492e-03 -1.67193392e-03 -3.80544399e-04 -2.64360267e-03\n",
      "  7.21714110e-04 -6.48119603e-05  3.55212321e-03 -3.13063560e-04\n",
      " -5.02195442e-04 -6.19661354e-04 -1.89933949e-03  1.82793208e-03\n",
      " -2.60898523e-04 -3.40518920e-04  7.93700689e-04  6.01945794e-04\n",
      "  3.06808460e-03 -1.60298706e-03  2.43136333e-03 -1.24734838e-03\n",
      " -7.54527980e-04 -1.25742680e-03 -9.35336517e-04 -1.45540253e-04\n",
      " -2.38622874e-06 -1.74505058e-05  6.56138445e-06  3.46004404e-03\n",
      " -2.02432461e-03 -1.19217986e-03 -8.55099526e-04 -4.35676338e-04\n",
      " -1.54280930e-03 -7.33620545e-04  3.89089971e-03 -8.82393506e-04\n",
      " -5.07367367e-04 -2.21687311e-04 -1.27299642e-03  2.38627894e-03\n",
      "  5.78267209e-04 -8.23942188e-04  1.58337841e-03 -2.23328592e-03\n",
      "  2.04198575e-03 -2.06359735e-04  3.85334063e-03  9.80416808e-05\n",
      "  1.01862755e-03  7.31917273e-04 -4.61906486e-04 -1.91030069e-03\n",
      "  3.74464178e-03  2.77224259e-04  6.72285503e-04  4.85141703e-04\n",
      " -2.01006929e-04  1.31511572e-03  1.61120947e-03  5.32971935e-05\n",
      " -1.17342535e-03 -1.16305507e-03 -3.60248517e-03  5.44352457e-04\n",
      "  8.18262808e-04 -3.15258908e-03  1.59108080e-03 -2.95778562e-04\n",
      " -6.81556761e-04 -1.06142419e-04 -6.66167587e-04  8.99590668e-04\n",
      "  7.79664435e-04 -4.68032013e-05  2.81596120e-04  6.50556525e-04\n",
      "  4.96757566e-04 -1.02494401e-03 -8.50492448e-04  4.37195151e-04\n",
      " -2.51524593e-03  1.23863749e-03  1.84243219e-03 -2.94269738e-03\n",
      "  2.98514590e-03  9.81016346e-05 -8.52784142e-04  1.85448513e-03\n",
      "  7.90818129e-04 -1.85798935e-03  4.57993429e-03 -6.37070261e-05\n",
      "  3.11831362e-03  2.29311944e-03  2.40146750e-04  3.99475073e-04\n",
      "  1.35304930e-03  9.28550784e-04  9.97257768e-04  1.49725401e-03\n",
      "  7.61628326e-04  8.72209319e-04  1.10171793e-03 -3.45481653e-03\n",
      " -1.24464964e-03 -4.65416495e-04 -1.93017389e-04 -1.21940172e-03\n",
      " -1.39703183e-03 -9.42535931e-04  2.76829558e-03 -1.62815012e-03\n",
      "  1.98205234e-03  1.31402689e-03 -6.22910622e-04 -1.22455927e-03\n",
      " -1.95020600e-03  1.74556789e-03 -1.41814223e-03 -4.14307415e-03\n",
      "  2.38984032e-03  4.31665173e-03 -2.47042114e-03  2.40761531e-03\n",
      " -3.23784980e-03  5.33049984e-04  2.32724706e-03 -3.30074283e-04\n",
      " -1.11113850e-03  1.93318177e-03  1.06037434e-04 -5.17641718e-04\n",
      "  2.78548058e-03  7.66774756e-04 -7.59181101e-04 -2.27785731e-05\n",
      "  1.27470354e-03  4.27286141e-04 -2.83569586e-03 -1.03518112e-04\n",
      "  3.40700281e-05 -1.10833185e-04  1.98702235e-03  2.00731374e-04\n",
      " -3.14111565e-03 -1.10797037e-03  7.82357354e-04 -1.80688934e-04\n",
      " -1.02985860e-03  2.72312760e-03  1.72113033e-03 -3.68556799e-03\n",
      " -1.49201206e-03 -7.21979653e-04 -1.96854398e-03 -9.57687269e-04\n",
      " -1.06676191e-03 -8.37073021e-04  2.96312408e-03 -2.13504513e-03\n",
      " -1.03979977e-03 -2.36977404e-03 -2.08736565e-05  2.20150221e-04\n",
      "  2.02113506e-03 -1.51272409e-03  2.30697286e-03 -2.42706016e-03]\n",
      "tensor_name:  Spear/Attention_1/dense_1/bias/Adam_1\n",
      "[2.48825756e-07 1.34320956e-07 3.99066629e-08 9.84519417e-08\n",
      " 1.34000715e-07 2.61805411e-07 1.53199920e-07 1.27495056e-07\n",
      " 2.37243924e-07 9.49803010e-08 4.78772563e-07 2.10258946e-07\n",
      " 7.89405817e-07 1.08424814e-07 1.84118125e-08 7.33812556e-07\n",
      " 1.23158213e-06 9.80871846e-07 3.36615329e-07 2.62268274e-09\n",
      " 5.99440886e-10 3.00766203e-07 6.16512565e-08 1.46537715e-07\n",
      " 6.39135180e-08 6.79190748e-08 1.19581518e-08 4.51884965e-08\n",
      " 8.96825838e-07 7.93135158e-08 6.80542911e-08 6.50025243e-07\n",
      " 1.27452413e-08 1.06196625e-08 1.21690931e-07 6.79828474e-07\n",
      " 2.54609080e-08 3.68681121e-08 2.32600044e-07 1.22677918e-07\n",
      " 8.46764081e-08 1.07894579e-11 3.10294638e-07 1.11125772e-07\n",
      " 4.11979244e-07 8.07898104e-08 3.48614094e-07 1.97719565e-08\n",
      " 3.12504689e-08 1.85189117e-06 1.09745002e-07 2.48937262e-08\n",
      " 5.02832506e-07 3.02412616e-07 7.59731833e-09 2.77945911e-09\n",
      " 4.79972577e-08 3.98120115e-08 3.44497622e-08 5.29734301e-08\n",
      " 3.09908934e-08 8.08102314e-08 1.70441059e-12 9.05317421e-09\n",
      " 2.62716924e-08 3.68971200e-08 7.95831454e-07 1.60476688e-07\n",
      " 8.55511911e-11 2.22727280e-07 2.21452083e-08 1.79408744e-07\n",
      " 5.35014181e-07 8.63249361e-10 4.29410479e-10 6.42735188e-07\n",
      " 3.80129039e-09 3.55666600e-07 2.25090346e-08 6.89080082e-07\n",
      " 6.34968131e-08 4.31273321e-07 1.36020773e-07 4.54113490e-07\n",
      " 4.80716608e-07 4.83697534e-07 3.34624808e-08 2.66034135e-07\n",
      " 1.98977958e-07 2.79532571e-07 1.44812109e-08 6.98854137e-07\n",
      " 5.20864312e-08 4.20053381e-10 1.26174109e-06 9.80074777e-09\n",
      " 2.52196930e-08 3.83975056e-08 3.60744252e-07 3.34129084e-07\n",
      " 6.80671342e-09 1.15951586e-08 6.29952410e-08 3.62333878e-08\n",
      " 9.41301721e-07 2.56953314e-07 5.91144840e-07 1.55585724e-07\n",
      " 5.69304852e-08 1.58110097e-07 8.74842740e-08 2.11816809e-09\n",
      " 5.69401134e-13 3.04516065e-11 4.30511920e-12 1.19717447e-06\n",
      " 4.09783496e-07 1.42127391e-07 7.31185423e-08 1.89811331e-08\n",
      " 2.38022892e-07 5.38191962e-08 1.51388986e-06 7.78607969e-08\n",
      " 2.57418229e-08 4.91446039e-09 1.62049830e-07 5.69425083e-07\n",
      " 3.34388517e-08 6.78871714e-08 2.50705369e-07 4.98750012e-07\n",
      " 4.16965008e-07 4.25837721e-09 1.48480365e-06 9.61204227e-10\n",
      " 1.03758836e-07 5.35695754e-08 2.13354756e-08 3.64919970e-07\n",
      " 1.40221550e-06 7.68522668e-09 4.51961775e-08 2.35359323e-08\n",
      " 4.04032496e-09 1.72950621e-07 2.59596135e-07 2.84055307e-10\n",
      " 1.37690876e-07 1.35267911e-07 1.29777254e-06 2.96315630e-08\n",
      " 6.69545059e-08 9.93868525e-07 2.53150432e-07 8.74837980e-09\n",
      " 4.64513370e-08 1.12660625e-09 4.43773338e-08 8.09252612e-08\n",
      " 6.07868458e-08 2.19051055e-10 7.92953081e-09 4.23218154e-08\n",
      " 2.46764778e-08 1.05049622e-07 7.23327744e-08 1.91137044e-08\n",
      " 6.32637693e-07 1.53420245e-07 3.39451105e-07 8.65935135e-07\n",
      " 8.91097727e-07 9.62380065e-10 7.27231111e-08 3.43906919e-07\n",
      " 6.25384970e-08 3.45207837e-07 2.09755194e-06 4.05853101e-10\n",
      " 9.72375005e-07 5.25832604e-07 5.76696957e-09 1.59578217e-08\n",
      " 1.83071791e-07 8.62195151e-08 9.94509790e-08 2.24173959e-07\n",
      " 5.80069965e-08 7.60738956e-08 1.21376615e-07 1.19355968e-06\n",
      " 1.54913195e-07 2.16609628e-08 3.72552122e-09 1.48692067e-07\n",
      " 1.95167203e-07 8.88362095e-08 7.66335802e-07 2.65083742e-07\n",
      " 3.92847909e-07 1.72664357e-07 3.88012467e-08 1.49952541e-07\n",
      " 3.80325247e-07 3.04696641e-07 2.01110055e-07 1.71648333e-06\n",
      " 5.71126122e-07 1.86332340e-06 6.10289874e-07 5.79653431e-07\n",
      " 1.04835317e-06 2.84138455e-08 5.41600684e-07 1.08947580e-08\n",
      " 1.23461220e-07 3.73714187e-07 1.12437870e-09 2.67949343e-08\n",
      " 7.75879926e-07 5.87935638e-08 5.76348249e-08 5.18856416e-11\n",
      " 1.62484753e-07 1.82571025e-08 8.04106321e-07 1.07158560e-09\n",
      " 1.16075143e-10 1.22838306e-09 3.94820546e-07 4.02925471e-09\n",
      " 9.86647592e-07 1.22758195e-07 6.12074871e-08 3.26480509e-09\n",
      " 1.06059460e-07 7.41532403e-07 2.96225011e-07 1.35832306e-06\n",
      " 2.22607042e-07 5.21247649e-08 3.87511307e-07 9.17152647e-08\n",
      " 1.13796581e-07 7.00681895e-08 8.77998616e-07 4.55835732e-07\n",
      " 1.08116907e-07 5.61575348e-07 4.35703695e-11 4.84654761e-09\n",
      " 4.08493264e-07 2.28830345e-07 5.32205206e-07 5.89054252e-07]\n",
      "tensor_name:  Spear/Attention_1/dense_1/kernel\n",
      "[[ 1.6056413e-02  5.2952174e-02  1.2229910e-02 ...  2.3434864e-02\n",
      "  -1.5009911e-02  1.4422876e-03]\n",
      " [ 3.3301979e-02  8.4342994e-02 -8.5691335e-03 ... -2.7239950e-02\n",
      "  -5.7842955e-02 -3.1013375e-02]\n",
      " [ 7.1598120e-02 -7.9122603e-02 -6.5610923e-02 ... -6.1006341e-02\n",
      "   3.3431020e-02  5.4747790e-02]\n",
      " ...\n",
      " [ 2.6876481e-02  8.7340474e-03 -1.7169314e-02 ... -9.5405132e-03\n",
      "  -5.2520078e-02  5.4637112e-02]\n",
      " [-3.4332583e-03  1.4402528e-02  8.3465926e-02 ... -5.1053263e-02\n",
      "   5.7829581e-02 -3.3702675e-02]\n",
      " [-4.3319310e-03  6.9769688e-02  6.6239215e-02 ...  9.1576949e-05\n",
      "   6.2045690e-02 -8.5932709e-02]]\n",
      "tensor_name:  Spear/Attention_1/dense_1/kernel/Adam\n",
      "[[ 3.5319351e-06 -2.6510222e-06  1.4500923e-06 ...  3.4023572e-06\n",
      "  -5.2417622e-06  5.5435826e-06]\n",
      " [-6.5022982e-07  5.4369644e-07 -2.5945863e-07 ... -6.6435558e-07\n",
      "   1.0049755e-06 -1.1010027e-06]\n",
      " [-3.9141087e-06  3.0268081e-06 -1.6050070e-06 ... -3.7945431e-06\n",
      "   5.8617620e-06 -6.2382655e-06]\n",
      " ...\n",
      " [ 1.1431636e-06 -9.2286342e-07  4.5965714e-07 ...  1.1069828e-06\n",
      "  -1.7520108e-06  1.8737262e-06]\n",
      " [-1.8873080e-06  1.4775848e-06 -7.5352568e-07 ... -1.8313626e-06\n",
      "   2.8325471e-06 -3.0199640e-06]\n",
      " [-1.8124140e-06  1.2545737e-06 -6.9224512e-07 ... -1.6864069e-06\n",
      "   2.5307654e-06 -2.6786593e-06]]\n",
      "tensor_name:  Spear/Attention_1/dense_1/kernel/Adam_1\n",
      "[[1.2474399e-12 7.0278245e-13 2.1027396e-13 ... 1.1575880e-12\n",
      "  2.7475706e-12 3.0730897e-12]\n",
      " [4.2279321e-14 2.9560186e-14 6.7317888e-15 ... 4.4136244e-14\n",
      "  1.0099623e-13 1.2121909e-13]\n",
      " [1.5320043e-12 9.1614455e-13 2.5760129e-13 ... 1.4398364e-12\n",
      "  3.4359794e-12 3.8915442e-12]\n",
      " ...\n",
      " [1.3068054e-13 8.5166547e-14 2.1128187e-14 ... 1.2253946e-13\n",
      "  3.0695010e-13 3.5108028e-13]\n",
      " [3.5618841e-13 2.1832277e-13 5.6779336e-14 ... 3.3538442e-13\n",
      "  8.0232164e-13 9.1200615e-13]\n",
      " [3.2848005e-13 1.5739340e-13 4.7919693e-14 ... 2.8439303e-13\n",
      "  6.4046880e-13 7.1751193e-13]]\n",
      "tensor_name:  Spear/Attention_1/dense_2/bias\n",
      "[ 0.00099998 -0.00099997  0.00099995  0.00099997  0.00099997  0.00099998\n",
      "  0.00099997 -0.00099997  0.00099998  0.00099997 -0.00099999  0.00099998\n",
      "  0.00099999  0.00099997 -0.00099993 -0.00099999  0.00099999  0.00099999\n",
      " -0.00099998 -0.0009998   0.00099959 -0.00099998 -0.00099996 -0.00099997\n",
      "  0.00099996  0.00099996  0.00099991  0.00099995 -0.00099999  0.00099996\n",
      " -0.00099996  0.00099999  0.00099991  0.0009999   0.00099997 -0.00099999\n",
      " -0.00099994  0.00099995 -0.00099998 -0.00099997 -0.00099997  0.00099696\n",
      "  0.00099998  0.00099997  0.00099998 -0.00099997 -0.00099998 -0.00099993\n",
      " -0.00099994 -0.00099999 -0.00099997  0.00099994  0.00099999  0.00099998\n",
      " -0.00099989  0.00099981 -0.00099995 -0.00099995  0.00099995 -0.00099996\n",
      "  0.00099994 -0.00099996 -0.0009924  -0.00099989  0.00099994  0.00099995\n",
      " -0.00099999  0.00099998  0.00099892 -0.00099998 -0.00099993  0.00099998\n",
      " -0.00099999 -0.00099966  0.00099952  0.00099999 -0.00099984 -0.00099998\n",
      " -0.00099993  0.00099999 -0.00099996  0.00099998  0.00099997  0.00099999\n",
      "  0.00099999  0.00099999  0.00099995 -0.00099998  0.00099998  0.00099998\n",
      "  0.00099992  0.00099999 -0.00099996  0.00099951 -0.00099999  0.0009999\n",
      "  0.00099994  0.00099995  0.00099998 -0.00099998  0.00099988  0.00099991\n",
      " -0.00099996 -0.00099995 -0.00099999  0.00099998 -0.00099999  0.00099997\n",
      "  0.00099996  0.00099997  0.00099997  0.00099978  0.00098692  0.00099819\n",
      " -0.0009952  -0.00099999  0.00099998  0.00099997  0.00099996  0.00099993\n",
      "  0.00099998  0.00099996 -0.00099999  0.00099996  0.00099994  0.00099986\n",
      "  0.00099998 -0.00099999 -0.00099995  0.00099996 -0.00099998  0.00099999\n",
      " -0.00099998  0.00099985 -0.00099999 -0.00099968 -0.00099997 -0.00099996\n",
      "  0.00099993  0.00099998 -0.00099999 -0.00099989 -0.00099995 -0.00099993\n",
      "  0.00099984 -0.00099998 -0.00099998 -0.00099941  0.00099997  0.00099997\n",
      "  0.00099999 -0.00099994 -0.00099996  0.00099999 -0.00099998  0.00099989\n",
      "  0.00099995  0.0009997   0.00099995 -0.00099996 -0.00099996  0.00099932\n",
      " -0.00099989 -0.00099995 -0.00099994  0.00099997  0.00099996 -0.00099993\n",
      "  0.00099999 -0.00099997 -0.00099998  0.00099999 -0.00099999 -0.00099968\n",
      "  0.00099996 -0.00099998 -0.00099996  0.00099998 -0.00099999  0.0009995\n",
      " -0.00099999 -0.00099999 -0.00099987 -0.00099992 -0.00099998 -0.00099997\n",
      " -0.00099997 -0.00099998 -0.00099996 -0.00099996 -0.00099997  0.00099999\n",
      "  0.00099997  0.00099993  0.00099984  0.00099997  0.00099998  0.00099997\n",
      " -0.00099999  0.00099998 -0.00099998 -0.00099998  0.00099995  0.00099997\n",
      "  0.00099998 -0.00099998  0.00099998  0.00099999 -0.00099999 -0.00099999\n",
      "  0.00099999 -0.00099999  0.00099999 -0.00099994 -0.00099999  0.0009999\n",
      "  0.00099997 -0.00099998 -0.0009997   0.00099994 -0.00099999 -0.00099996\n",
      "  0.00099996  0.00099861 -0.00099998 -0.00099993  0.00099999  0.00099969\n",
      " -0.00099907  0.00099971 -0.00099998 -0.00099984  0.00099999  0.00099997\n",
      " -0.00099996  0.00099983  0.00099997 -0.00099999 -0.00099998  0.00099999\n",
      "  0.00099998  0.00099996  0.00099998  0.00099997  0.00099997  0.00099996\n",
      " -0.00099999  0.00099999  0.00099997  0.00099999  0.00099849 -0.00099986\n",
      " -0.00099998  0.00099998 -0.00099999  0.00099999]\n",
      "tensor_name:  Spear/Attention_1/dense_2/bias/Adam\n",
      "[-1.57743134e-03  1.15897681e-03 -6.31721399e-04 -9.92236077e-04\n",
      " -1.15759450e-03 -1.61805027e-03 -1.23774761e-03  1.12914457e-03\n",
      " -1.54028239e-03 -9.74584778e-04  2.18810164e-03 -1.45004061e-03\n",
      " -2.80965492e-03 -1.04127941e-03  4.29092819e-04  2.70891539e-03\n",
      " -3.50941345e-03 -3.13190883e-03  1.83472084e-03  1.61948046e-04\n",
      " -7.74242362e-05  1.73427258e-03  7.85188575e-04  1.21053576e-03\n",
      " -7.99464644e-04 -8.24136194e-04 -3.45807901e-04 -6.72228285e-04\n",
      "  2.99472525e-03 -8.90587282e-04  8.24955874e-04 -2.54957587e-03\n",
      " -3.57007142e-04 -3.25880304e-04 -1.10314379e-03  2.60736863e-03\n",
      "  5.04591386e-04 -6.07195252e-04  1.52513327e-03  1.10760785e-03\n",
      "  9.20204038e-04 -1.03872717e-05 -1.76152983e-03 -1.05416891e-03\n",
      " -2.02974095e-03  8.98837694e-04  1.86713308e-03  4.44659498e-04\n",
      "  5.59024920e-04  4.30338876e-03  1.04759948e-03 -4.98939364e-04\n",
      " -2.24240753e-03 -1.73901359e-03  2.75634142e-04 -1.66718135e-04\n",
      "  6.92805334e-04  6.30971801e-04 -5.86943002e-04  7.27833132e-04\n",
      " -5.56698535e-04  8.98951315e-04  4.12860618e-06  3.00886953e-04\n",
      " -5.12562692e-04 -6.07434195e-04  2.82106735e-03 -1.26680278e-03\n",
      " -2.92493969e-05  1.49241486e-03  4.70589992e-04 -1.33944442e-03\n",
      "  2.31305277e-03  9.29119196e-05 -6.55299300e-05 -2.53523956e-03\n",
      "  1.94970358e-04  1.88592507e-03  4.74440079e-04 -2.62505002e-03\n",
      "  7.96854380e-04 -2.07672594e-03 -1.16628711e-03 -2.13100878e-03\n",
      " -2.19253986e-03 -2.19932734e-03 -5.78471576e-04  1.63106644e-03\n",
      " -1.41060492e-03 -1.67193392e-03 -3.80544312e-04 -2.64360197e-03\n",
      "  7.21714168e-04 -6.48120986e-05  3.55212321e-03 -3.13063472e-04\n",
      " -5.02195326e-04 -6.19661587e-04 -1.89933949e-03  1.82793220e-03\n",
      " -2.60898378e-04 -3.40519124e-04  7.93700572e-04  6.01945620e-04\n",
      "  3.06808529e-03 -1.60298683e-03  2.43136263e-03 -1.24734850e-03\n",
      " -7.54527922e-04 -1.25742645e-03 -9.35336691e-04 -1.45540238e-04\n",
      " -2.38621351e-06 -1.74504476e-05  6.56145448e-06  3.46004451e-03\n",
      " -2.02432461e-03 -1.19217986e-03 -8.55099410e-04 -4.35676397e-04\n",
      " -1.54280919e-03 -7.33620429e-04  3.89089831e-03 -8.82393622e-04\n",
      " -5.07367251e-04 -2.21687311e-04 -1.27299642e-03  2.38627917e-03\n",
      "  5.78267092e-04 -8.23942304e-04  1.58337853e-03 -2.23328592e-03\n",
      "  2.04198598e-03 -2.06359764e-04  3.85333993e-03  9.80416226e-05\n",
      "  1.01862755e-03  7.31916982e-04 -4.61906311e-04 -1.91030081e-03\n",
      "  3.74464155e-03  2.77224433e-04  6.72285620e-04  4.85141500e-04\n",
      " -2.01007031e-04  1.31511583e-03  1.61120924e-03  5.32972044e-05\n",
      " -1.17342523e-03 -1.16305496e-03 -3.60248517e-03  5.44352457e-04\n",
      "  8.18262808e-04 -3.15258978e-03  1.59108057e-03 -2.95778620e-04\n",
      " -6.81556819e-04 -1.06142637e-04 -6.66167762e-04  8.99590668e-04\n",
      "  7.79664493e-04 -4.68032122e-05  2.81596003e-04  6.50556525e-04\n",
      "  4.96757508e-04 -1.02494389e-03 -8.50492739e-04  4.37195064e-04\n",
      " -2.51524616e-03  1.23863772e-03  1.84243196e-03 -2.94269738e-03\n",
      "  2.98514543e-03  9.81016929e-05 -8.52784317e-04  1.85448572e-03\n",
      "  7.90818303e-04 -1.85798935e-03  4.57993429e-03 -6.37069897e-05\n",
      "  3.11831292e-03  2.29311944e-03  2.40146663e-04  3.99475073e-04\n",
      "  1.35304953e-03  9.28550900e-04  9.97258001e-04  1.49725424e-03\n",
      "  7.61628500e-04  8.72209144e-04  1.10171793e-03 -3.45481653e-03\n",
      " -1.24464964e-03 -4.65416466e-04 -1.93017331e-04 -1.21940149e-03\n",
      " -1.39703194e-03 -9.42535815e-04  2.76829582e-03 -1.62814953e-03\n",
      "  1.98205188e-03  1.31402700e-03 -6.22910564e-04 -1.22455950e-03\n",
      " -1.95020612e-03  1.74556789e-03 -1.41814270e-03 -4.14307415e-03\n",
      "  2.38984078e-03  4.31665126e-03 -2.47042114e-03  2.40761484e-03\n",
      " -3.23784980e-03  5.33049984e-04  2.32724706e-03 -3.30074341e-04\n",
      " -1.11113850e-03  1.93318212e-03  1.06037420e-04 -5.17641660e-04\n",
      "  2.78548058e-03  7.66774756e-04 -7.59180693e-04 -2.27786459e-05\n",
      "  1.27470377e-03  4.27286053e-04 -2.83569563e-03 -1.03518018e-04\n",
      "  3.40700062e-05 -1.10833236e-04  1.98702258e-03  2.00731403e-04\n",
      " -3.14111565e-03 -1.10797025e-03  7.82357412e-04 -1.80688919e-04\n",
      " -1.02985848e-03  2.72312667e-03  1.72113068e-03 -3.68556660e-03\n",
      " -1.49201183e-03 -7.21979479e-04 -1.96854398e-03 -9.57687444e-04\n",
      " -1.06676179e-03 -8.37073021e-04  2.96312454e-03 -2.13504536e-03\n",
      " -1.03979965e-03 -2.36977474e-03 -2.08736601e-05  2.20150221e-04\n",
      "  2.02113437e-03 -1.51272374e-03  2.30697310e-03 -2.42705923e-03]\n",
      "tensor_name:  Spear/Attention_1/dense_2/bias/Adam_1\n",
      "[2.48825643e-07 1.34320928e-07 3.99066593e-08 9.84519275e-08\n",
      " 1.34000700e-07 2.61805184e-07 1.53199849e-07 1.27495056e-07\n",
      " 2.37243810e-07 9.49802867e-08 4.78772449e-07 2.10258975e-07\n",
      " 7.89405533e-07 1.08424835e-07 1.84118196e-08 7.33812442e-07\n",
      " 1.23158190e-06 9.80872301e-07 3.36615557e-07 2.62268207e-09\n",
      " 5.99443217e-10 3.00766146e-07 6.16512850e-08 1.46537730e-07\n",
      " 6.39135180e-08 6.79191388e-08 1.19581518e-08 4.51884858e-08\n",
      " 8.96825952e-07 7.93135158e-08 6.80543053e-08 6.50025072e-07\n",
      " 1.27452395e-08 1.06196554e-08 1.21690988e-07 6.79827963e-07\n",
      " 2.54609080e-08 3.68681157e-08 2.32600044e-07 1.22677875e-07\n",
      " 8.46764081e-08 1.07893972e-11 3.10294581e-07 1.11125729e-07\n",
      " 4.11979329e-07 8.07898388e-08 3.48613952e-07 1.97719441e-08\n",
      " 3.12504689e-08 1.85189060e-06 1.09745002e-07 2.48937173e-08\n",
      " 5.02832449e-07 3.02412786e-07 7.59731744e-09 2.77945644e-09\n",
      " 4.79972861e-08 3.98120115e-08 3.44497479e-08 5.29734017e-08\n",
      " 3.09909147e-08 8.08102740e-08 1.70451619e-12 9.05317510e-09\n",
      " 2.62716995e-08 3.68971378e-08 7.95831454e-07 1.60476787e-07\n",
      " 8.55515866e-11 2.22727252e-07 2.21451995e-08 1.79408744e-07\n",
      " 5.35014181e-07 8.63250915e-10 4.29411479e-10 6.42735301e-07\n",
      " 3.80129306e-09 3.55666600e-07 2.25090400e-08 6.89079627e-07\n",
      " 6.34968487e-08 4.31273321e-07 1.36020759e-07 4.54113746e-07\n",
      " 4.80716665e-07 4.83697647e-07 3.34624914e-08 2.66034220e-07\n",
      " 1.98977986e-07 2.79532571e-07 1.44812038e-08 6.98853853e-07\n",
      " 5.20864383e-08 4.20055213e-10 1.26174109e-06 9.80074244e-09\n",
      " 2.52196770e-08 3.83975340e-08 3.60744252e-07 3.34129140e-07\n",
      " 6.80670498e-09 1.15951737e-08 6.29952126e-08 3.62333701e-08\n",
      " 9.41302119e-07 2.56953257e-07 5.91144556e-07 1.55585738e-07\n",
      " 5.69304781e-08 1.58110026e-07 8.74843025e-08 2.11816764e-09\n",
      " 5.69393870e-13 3.04514018e-11 4.30521114e-12 1.19717481e-06\n",
      " 4.09783496e-07 1.42127391e-07 7.31185281e-08 1.89811367e-08\n",
      " 2.38022864e-07 5.38191749e-08 1.51388861e-06 7.78608111e-08\n",
      " 2.57418087e-08 4.91446039e-09 1.62049830e-07 5.69425254e-07\n",
      " 3.34388339e-08 6.78871857e-08 2.50705426e-07 4.98750012e-07\n",
      " 4.16965094e-07 4.25837809e-09 1.48480297e-06 9.61203117e-10\n",
      " 1.03758815e-07 5.35695328e-08 2.13354578e-08 3.64920055e-07\n",
      " 1.40221516e-06 7.68523556e-09 4.51961881e-08 2.35359146e-08\n",
      " 4.04032852e-09 1.72950649e-07 2.59596078e-07 2.84055418e-10\n",
      " 1.37690847e-07 1.35267882e-07 1.29777254e-06 2.96315630e-08\n",
      " 6.69545059e-08 9.93869094e-07 2.53150375e-07 8.74838246e-09\n",
      " 4.64513512e-08 1.12661092e-09 4.43773516e-08 8.09252612e-08\n",
      " 6.07868671e-08 2.19051152e-10 7.92952459e-09 4.23218154e-08\n",
      " 2.46764742e-08 1.05049601e-07 7.23328171e-08 1.91136955e-08\n",
      " 6.32637864e-07 1.53420288e-07 3.39451049e-07 8.65935135e-07\n",
      " 8.91097443e-07 9.62381397e-10 7.27231395e-08 3.43907118e-07\n",
      " 6.25385255e-08 3.45207837e-07 2.09755194e-06 4.05852629e-10\n",
      " 9.72374551e-07 5.25832604e-07 5.76696468e-09 1.59578217e-08\n",
      " 1.83071847e-07 8.62195293e-08 9.94510287e-08 2.24174030e-07\n",
      " 5.80070250e-08 7.60738601e-08 1.21376615e-07 1.19355968e-06\n",
      " 1.54913195e-07 2.16609575e-08 3.72551945e-09 1.48692010e-07\n",
      " 1.95167217e-07 8.88361953e-08 7.66335916e-07 2.65083543e-07\n",
      " 3.92847767e-07 1.72664386e-07 3.88012360e-08 1.49952584e-07\n",
      " 3.80325332e-07 3.04696641e-07 2.01110183e-07 1.71648333e-06\n",
      " 5.71126293e-07 1.86332306e-06 6.10289874e-07 5.79653261e-07\n",
      " 1.04835317e-06 2.84138508e-08 5.41600571e-07 1.08947624e-08\n",
      " 1.23461220e-07 3.73714329e-07 1.12437848e-09 2.67949289e-08\n",
      " 7.75879926e-07 5.87935638e-08 5.76347610e-08 5.18859747e-11\n",
      " 1.62484795e-07 1.82570936e-08 8.04106207e-07 1.07158371e-09\n",
      " 1.16074976e-10 1.22838406e-09 3.94820631e-07 4.02925560e-09\n",
      " 9.86647592e-07 1.22758166e-07 6.12075013e-08 3.26480465e-09\n",
      " 1.06059439e-07 7.41532006e-07 2.96225124e-07 1.35832192e-06\n",
      " 2.22606943e-07 5.21247365e-08 3.87511420e-07 9.17153002e-08\n",
      " 1.13796560e-07 7.00681895e-08 8.77999014e-07 4.55835817e-07\n",
      " 1.08116886e-07 5.61575746e-07 4.35703834e-11 4.84654761e-09\n",
      " 4.08492951e-07 2.28830274e-07 5.32205377e-07 5.89053798e-07]\n",
      "tensor_name:  Spear/Attention_1/dense_2/kernel\n",
      "[[-0.03353123 -0.02994264 -0.0512132  ... -0.05761769 -0.03151089\n",
      "  -0.03528309]\n",
      " [-0.04574068 -0.05746223 -0.02018761 ... -0.01823202 -0.0583444\n",
      "   0.04963071]\n",
      " [ 0.03000848 -0.01226256  0.00066976 ...  0.06121765  0.06319541\n",
      "   0.06226748]\n",
      " ...\n",
      " [-0.07165837 -0.07679103 -0.05651225 ...  0.00454187 -0.0734456\n",
      "  -0.08359207]\n",
      " [ 0.00700808 -0.0574719   0.02232169 ...  0.01669665  0.02753354\n",
      "   0.00698041]\n",
      " [-0.01015368 -0.06105354 -0.07774609 ...  0.03818039  0.01393611\n",
      "   0.06046563]]\n",
      "tensor_name:  Spear/Attention_1/dense_2/kernel/Adam\n",
      "[[-3.5168739e-06 -9.8653138e-05  1.0158388e-05 ...  3.6724356e-05\n",
      "  -7.2702809e-05  1.0680532e-04]\n",
      " [ 7.2322541e-06  1.5237086e-05  3.4349378e-06 ... -1.1671544e-06\n",
      "   1.9645188e-06 -4.4949220e-06]\n",
      " [-1.5483862e-05  7.8128403e-07 -8.2482184e-06 ... -9.7177071e-06\n",
      "   1.8031362e-05 -1.9898391e-05]\n",
      " ...\n",
      " [ 3.5894934e-06 -1.7493774e-05  4.3088426e-06 ...  8.3037139e-06\n",
      "  -1.7992594e-05  2.3990273e-05]\n",
      " [-3.7975792e-06  1.8612842e-05 -3.3020447e-06 ... -1.1175780e-05\n",
      "   1.7440465e-05 -2.3765433e-05]\n",
      " [-8.4967587e-06  6.7649657e-05 -1.3057106e-05 ... -2.9622499e-05\n",
      "   6.2336570e-05 -8.5664018e-05]]\n",
      "tensor_name:  Spear/Attention_1/dense_2/kernel/Adam_1\n",
      "[[1.2368237e-12 9.7323116e-10 1.0319147e-11 ... 1.3486604e-10\n",
      "  5.2856280e-10 1.1407224e-09]\n",
      " [5.2304801e-12 2.3216568e-11 1.1798640e-12 ... 1.3622315e-13\n",
      "  3.8592830e-13 2.0204055e-12]\n",
      " [2.3974676e-11 6.1039668e-14 6.8032190e-12 ... 9.4432578e-12\n",
      "  3.2512566e-11 3.9594068e-11]\n",
      " ...\n",
      " [1.2884291e-12 3.0602802e-11 1.8565878e-12 ... 6.8950748e-12\n",
      "  3.2372910e-11 5.7552550e-11]\n",
      " [1.4421417e-12 3.4643323e-11 1.0903353e-12 ... 1.2489639e-11\n",
      "  3.0416579e-11 5.6478822e-11]\n",
      " [7.2193939e-12 4.5764154e-10 1.7048574e-11 ... 8.7748080e-11\n",
      "  3.8857961e-10 7.3382261e-10]]\n",
      "tensor_name:  Spear/Attention_1/dense_3/bias\n",
      "[-0.00099998  0.00099886  0.00099998 -0.00099997  0.00099997 -0.00099997\n",
      " -0.00099999  0.00099989 -0.00099701  0.00099998  0.00099998 -0.00099995\n",
      " -0.00099943  0.00099996  0.00099999  0.00099998  0.00099998 -0.00099995\n",
      "  0.00099993  0.00099995 -0.00099998 -0.00099999  0.00099987 -0.00099999\n",
      " -0.00099996  0.0009993   0.00099999  0.00099999  0.00099997  0.00099997\n",
      " -0.00099994  0.00099982 -0.00099997 -0.00099992  0.00099962  0.00099998\n",
      " -0.00099998  0.00099995  0.00099998 -0.00099998 -0.00099998 -0.00099999\n",
      " -0.00099994  0.00099999  0.00099998  0.00099982 -0.00099999 -0.00099998\n",
      "  0.00099998 -0.00099998 -0.00099993 -0.00099998  0.00099995 -0.00099995\n",
      " -0.00099997  0.00099998 -0.00099993  0.0009998   0.00099998 -0.00099998\n",
      " -0.00099985 -0.00099994 -0.00099997 -0.00099997 -0.00099999 -0.00099999\n",
      "  0.00099996 -0.00099997 -0.00099988 -0.00099999  0.00099999 -0.00099964\n",
      "  0.00099997 -0.00099998 -0.00099999 -0.00099999  0.00099996  0.00099978\n",
      "  0.00099995 -0.00099998  0.00099991 -0.00099999 -0.00099998 -0.00099998\n",
      "  0.00099996  0.00099997 -0.00099996  0.00099998 -0.00099732  0.00099996\n",
      "  0.00099998 -0.00099994 -0.00099998  0.00099993 -0.00099994 -0.00099998\n",
      " -0.0009997   0.00099998  0.00099999  0.00099997 -0.0009997   0.00099991\n",
      "  0.00099999 -0.00099974 -0.00099998  0.00099997  0.00099991 -0.00099999\n",
      " -0.00099997 -0.00099998  0.00099999 -0.00099998 -0.00099995 -0.00099988\n",
      " -0.00099996  0.00099999 -0.00099997  0.00099997  0.00099998 -0.00099832\n",
      " -0.00099995  0.00099997 -0.00099994  0.00099998  0.00099999  0.00099999\n",
      " -0.00099991 -0.00099993 -0.0009999   0.00099996  0.00099998 -0.00099999\n",
      " -0.00099992 -0.00099998 -0.00099986  0.00099999  0.00099979 -0.00099997\n",
      " -0.00099998  0.00099997  0.00099992 -0.00099999  0.00099997 -0.00099998\n",
      "  0.00099998 -0.00099996 -0.00099997  0.00099997 -0.00099997  0.00099999\n",
      " -0.00099996 -0.00099999  0.00099998  0.00099999  0.00099998  0.00099998\n",
      "  0.0009997  -0.00099997 -0.00099998  0.00099995  0.0009998   0.00099996\n",
      " -0.00099995  0.00099999 -0.00099999 -0.0009999  -0.00099996 -0.00099969\n",
      "  0.00099999  0.00099998 -0.00099998  0.00099998  0.00099995  0.00099993\n",
      "  0.00099844  0.00099998  0.00099999 -0.00099998 -0.00099996  0.00099998\n",
      "  0.00099999  0.00099898 -0.00099999  0.00099997  0.00099998  0.00099999\n",
      "  0.00099997  0.00099996  0.00099999  0.00099991 -0.00099998  0.00099995\n",
      " -0.00099999  0.00099975 -0.00099996 -0.00099999 -0.00099998 -0.00099969\n",
      " -0.00099998  0.00099998 -0.00099998 -0.00099991  0.00099999  0.00099982\n",
      "  0.00099987  0.00099902  0.00099997  0.00099995  0.00099998  0.00099999\n",
      "  0.00099998  0.00099997 -0.0009996  -0.00099993  0.00099998  0.00099997\n",
      "  0.00099998  0.00099998  0.00099999 -0.00099996  0.00099996  0.00099999\n",
      "  0.00099994  0.00099989  0.00099995  0.00099999  0.00099998 -0.00099998\n",
      " -0.00099991 -0.00099998  0.00099998 -0.00099973 -0.00099999  0.00099999\n",
      "  0.00099998  0.00099996 -0.00099999  0.00099999 -0.00099999  0.00099999\n",
      " -0.00099999  0.00099801  0.00099992 -0.00099997  0.00099999 -0.00099997\n",
      " -0.00099998  0.00099974  0.00099998  0.00099999  0.00099996  0.00099998\n",
      "  0.00099992 -0.00099997  0.00099994  0.00099999]\n",
      "tensor_name:  Spear/Attention_1/dense_3/bias/Adam\n",
      "[ 1.46008714e-03 -2.76630872e-05 -1.49033766e-03  9.16978752e-04\n",
      " -1.12493557e-03  1.10757141e-03  2.48867786e-03 -2.82123394e-04\n",
      "  1.05292038e-05 -1.57839688e-03 -2.00052699e-03  6.42548956e-04\n",
      "  5.57112653e-05 -7.78841844e-04 -3.08691175e-03 -1.27008709e-03\n",
      " -1.87467609e-03  6.48216286e-04 -4.78728092e-04 -6.97005540e-04\n",
      "  1.28757395e-03  2.88345781e-03 -2.36031963e-04  2.57705362e-03\n",
      "  7.97480345e-04 -4.50015759e-05 -2.52012699e-03 -2.19586981e-03\n",
      " -1.20225456e-03 -1.02997303e-03  5.39057888e-04 -1.79301234e-04\n",
      "  1.22297031e-03  3.79144942e-04 -8.29346100e-05 -2.03759363e-03\n",
      "  1.49194582e-03 -6.90715329e-04 -1.86520466e-03  1.32015755e-03\n",
      "  1.28224853e-03  2.44483212e-03  5.22755843e-04 -2.69505125e-03\n",
      " -1.95250637e-03 -1.80049276e-04  3.72117246e-03  1.42354623e-03\n",
      " -1.40076713e-03  1.65512320e-03  4.36374889e-04  1.68993778e-03\n",
      " -6.69826637e-04  5.75108628e-04  9.44298750e-04 -2.00505252e-03\n",
      "  4.77360212e-04 -1.59230083e-04 -1.66433770e-03  1.97618641e-03\n",
      "  2.09716411e-04  5.57891559e-04  1.17055723e-03  9.96883609e-04\n",
      "  3.53435287e-03  3.34530743e-03 -7.36352580e-04  9.99041367e-04\n",
      "  2.54841521e-04  2.64495611e-03 -4.08442831e-03  8.74047837e-05\n",
      " -9.98411444e-04  1.88858551e-03  2.25186488e-03  2.91672000e-03\n",
      " -7.81543727e-04 -1.46788705e-04 -6.95667812e-04  1.70829741e-03\n",
      " -3.35535733e-04  3.03067965e-03  1.39799342e-03  1.50961359e-03\n",
      " -7.42676959e-04 -1.17298786e-03  8.98383209e-04 -1.58668554e-03\n",
      "  1.17611135e-05 -7.38759467e-04 -1.38874771e-03  5.13143430e-04\n",
      "  1.74832495e-03 -4.85903467e-04  5.55013539e-04  1.46936893e-03\n",
      "  1.06051448e-04 -1.95770618e-03 -2.82377377e-03 -1.18726457e-03\n",
      "  1.05262967e-04 -3.35342542e-04 -3.16775357e-03  1.23758437e-04\n",
      "  1.51592074e-03 -1.15470006e-03 -3.62215826e-04  3.90120316e-03\n",
      "  9.56385978e-04  1.75294338e-03 -2.29449151e-03  1.52511720e-03\n",
      "  6.39302423e-04  2.58803775e-04  7.41263735e-04 -2.13273079e-03\n",
      "  1.22141652e-03 -1.00910990e-03 -1.57476531e-03  1.87579935e-05\n",
      "  6.62101491e-04 -1.06603664e-03  5.14994666e-04 -1.49491837e-03\n",
      " -3.37070785e-03 -2.44351500e-03  3.34959535e-04  4.78873437e-04\n",
      "  3.17266939e-04 -7.06992811e-04 -1.28645252e-03  3.34972749e-03\n",
      "  3.79839126e-04  1.40523433e-03  2.22352945e-04 -2.26958492e-03\n",
      " -1.53725865e-04  1.23426854e-03  1.80689606e-03 -1.05147378e-03\n",
      " -3.89786786e-04  4.07758215e-03 -9.75477393e-04  1.55288307e-03\n",
      " -1.56435836e-03  7.96975452e-04  9.48021421e-04 -1.03210204e-03\n",
      "  1.23306306e-03 -2.77179573e-03  7.08578329e-04  2.25074519e-03\n",
      " -1.97431771e-03 -2.24829977e-03 -1.32473290e-03 -1.45140884e-03\n",
      " -1.03634447e-04  1.10566325e-03  1.68606453e-03 -5.85110043e-04\n",
      " -1.57761082e-04 -8.95392091e-04  6.76421449e-04 -2.50835507e-03\n",
      "  2.32261326e-03  3.31860181e-04  8.28085642e-04  1.00923564e-04\n",
      " -2.15173163e-03 -1.66392757e-03  2.02665664e-03 -1.59311539e-03\n",
      " -6.54799631e-04 -4.28349624e-04 -2.02229930e-05 -1.99603662e-03\n",
      " -3.29176080e-03  1.50225277e-03  7.79742724e-04 -1.81036221e-03\n",
      " -4.41057840e-03 -3.09796960e-05  2.42894585e-03 -1.04061700e-03\n",
      " -1.39447837e-03 -3.52435722e-03 -1.02467893e-03 -7.52350606e-04\n",
      " -3.38403694e-03 -3.68873356e-04  1.65117090e-03 -5.99886756e-04\n",
      "  2.31046742e-03 -1.25566556e-04  7.14884081e-04  3.85424844e-03\n",
      "  2.06641131e-03  1.00531382e-04  1.95585261e-03 -1.41444732e-03\n",
      "  2.02784035e-03  3.55415832e-04 -2.19693710e-03 -1.76081870e-04\n",
      " -2.44606577e-04 -3.22247288e-05 -1.15861848e-03 -6.67496992e-04\n",
      " -1.31356216e-03 -2.76838499e-03 -1.86278077e-03 -1.01027323e-03\n",
      "  7.87140671e-05  4.67634789e-04 -1.93000701e-03 -9.03617707e-04\n",
      " -1.84600439e-03 -1.51698221e-03 -2.36111134e-03  8.40147142e-04\n",
      " -8.26429867e-04 -3.76319792e-03 -5.29493962e-04 -2.85566872e-04\n",
      " -6.88897213e-04 -2.12549907e-03 -1.93642161e-03  1.65897235e-03\n",
      "  3.41032719e-04  1.65540411e-03 -1.81688427e-03  1.16560936e-04\n",
      "  2.50534178e-03 -3.56599875e-03 -1.74133223e-03 -7.20900542e-04\n",
      "  3.11136129e-03 -2.26419535e-03  2.23701983e-03 -4.13969439e-03\n",
      "  2.67883018e-03 -1.58794464e-05 -3.77382297e-04  9.15318029e-04\n",
      " -2.91760126e-03  1.25040510e-03  2.03904114e-03 -1.20200901e-04\n",
      " -1.28680060e-03 -2.29616440e-03 -8.54535610e-04 -1.56618247e-03\n",
      " -3.98590608e-04  1.13113725e-03 -5.07089542e-04 -2.44678347e-03]\n",
      "tensor_name:  Spear/Attention_1/dense_3/bias/Adam_1\n",
      "[2.13182574e-07 7.65236138e-11 2.22107658e-07 8.40838794e-08\n",
      " 1.26546325e-07 1.22669817e-07 6.19343439e-07 7.95925370e-09\n",
      " 1.10862656e-11 2.49130352e-07 4.00205494e-07 4.12863628e-08\n",
      " 3.10370341e-10 6.06586497e-08 9.52889650e-07 1.61309970e-07\n",
      " 3.51436341e-07 4.20178701e-08 2.29177530e-08 4.85810183e-08\n",
      " 1.65782467e-07 8.31421744e-07 5.57103386e-09 6.64111724e-07\n",
      " 6.35966373e-08 2.02511480e-10 6.35095546e-07 4.82177938e-07\n",
      " 1.44539683e-07 1.06083021e-07 2.90579507e-08 3.21485016e-09\n",
      " 1.49563647e-07 1.43748959e-08 6.87805757e-10 4.15173247e-07\n",
      " 2.22587246e-07 4.77081272e-08 3.47894201e-07 1.74279265e-07\n",
      " 1.64413947e-07 5.97712415e-07 2.73269993e-08 7.26320479e-07\n",
      " 3.81223003e-07 3.24173066e-09 1.38469397e-06 2.02645666e-07\n",
      " 1.96212241e-07 2.73939634e-07 1.90420497e-08 2.85585145e-07\n",
      " 4.48661694e-08 3.30745529e-08 8.91688217e-08 4.02018173e-07\n",
      " 2.27869723e-08 2.53538834e-09 2.76998321e-07 3.90526026e-07\n",
      " 4.39803838e-09 3.11238821e-08 1.37018574e-07 9.93763720e-08\n",
      " 1.24914834e-06 1.11909321e-06 5.42207914e-08 9.98070391e-08\n",
      " 6.49433440e-09 6.99569966e-07 1.66823315e-06 7.63949459e-10\n",
      " 9.96812162e-08 3.56670768e-07 5.07082746e-07 8.50714287e-07\n",
      " 6.10802431e-08 2.15466356e-09 4.83947247e-08 2.91824136e-07\n",
      " 1.12582716e-08 9.18489604e-07 1.95435959e-07 2.27890283e-07\n",
      " 5.51561747e-08 1.37588216e-07 8.07081619e-08 2.51753733e-07\n",
      " 1.38321958e-11 5.45758247e-08 1.92859432e-07 2.63312643e-08\n",
      " 3.05659938e-07 2.36099016e-08 3.08035908e-08 2.15901636e-07\n",
      " 1.12467602e-09 3.83256179e-07 7.97359235e-07 1.40957823e-07\n",
      " 1.10801446e-09 1.12453113e-08 1.00345289e-06 1.53159463e-09\n",
      " 2.29798502e-07 1.33331426e-07 1.31198545e-08 1.52191831e-06\n",
      " 9.14661982e-08 3.07276963e-07 5.26462145e-07 2.32595141e-07\n",
      " 4.08702157e-08 6.69785027e-09 5.49464616e-08 4.54847964e-07\n",
      " 1.49183847e-07 1.01828931e-07 2.47985270e-07 3.51857606e-11\n",
      " 4.38372538e-08 1.13641903e-07 2.65215938e-08 2.23475098e-07\n",
      " 1.13615204e-06 5.97068549e-07 1.12196394e-08 2.29316690e-08\n",
      " 1.00656967e-08 4.99832211e-08 1.65493788e-07 1.12205237e-06\n",
      " 1.44275827e-08 1.97465710e-07 4.94401720e-09 5.15094655e-07\n",
      " 2.36313258e-09 1.52339837e-07 3.26482962e-07 1.10558240e-07\n",
      " 1.51931712e-08 1.66264556e-06 9.51543413e-08 2.41141379e-07\n",
      " 2.44718450e-07 6.35161470e-08 8.98732608e-08 1.06522045e-07\n",
      " 1.52042404e-07 7.68274958e-07 5.02076531e-08 5.06578601e-07\n",
      " 3.89787857e-07 5.05478454e-07 1.75489390e-07 2.10655955e-07\n",
      " 1.07399556e-09 1.22247499e-07 2.84277576e-07 3.42349153e-08\n",
      " 2.48882248e-09 8.01716311e-08 4.57539819e-08 6.29176157e-07\n",
      " 5.39446035e-07 1.10129710e-08 6.85716728e-08 1.01854292e-09\n",
      " 4.62988766e-07 2.76861783e-07 4.10728205e-07 2.53798248e-07\n",
      " 4.28756834e-08 1.83480946e-08 4.08964008e-11 3.98410947e-07\n",
      " 1.08355448e-06 2.25673318e-07 6.07990529e-08 3.27736785e-07\n",
      " 1.94529434e-06 9.59728755e-11 5.89969943e-07 1.08286926e-07\n",
      " 1.94454401e-07 1.24209282e-06 1.04995294e-07 5.66023850e-08\n",
      " 1.14515535e-06 1.36065728e-08 2.72632889e-07 3.59859307e-08\n",
      " 5.33818763e-07 1.57667479e-09 5.11052427e-08 1.48550328e-06\n",
      " 4.26999918e-07 1.01064235e-09 3.82530828e-07 2.00063454e-07\n",
      " 4.11208134e-07 1.26318733e-08 4.82646840e-07 3.10044124e-09\n",
      " 5.98315797e-09 1.03841935e-10 1.34237879e-07 4.45546249e-08\n",
      " 1.72542258e-07 7.66385313e-07 3.46990618e-07 1.02063851e-07\n",
      " 6.19582219e-10 2.18679368e-08 3.72487733e-07 8.16514074e-08\n",
      " 3.40768679e-07 2.30120420e-07 5.57477222e-07 7.05837735e-08\n",
      " 6.82977230e-08 1.41614692e-06 2.80360144e-08 8.15473555e-09\n",
      " 4.74572985e-08 4.51768642e-07 3.74967868e-07 2.75215257e-07\n",
      " 1.16301759e-08 2.74032601e-07 3.30102466e-07 1.35862710e-09\n",
      " 6.27665372e-07 1.27161775e-06 3.03219736e-07 5.19690637e-08\n",
      " 9.68043992e-07 5.12651184e-07 5.00419105e-07 1.71368413e-06\n",
      " 7.17603484e-07 2.52153454e-11 1.42415484e-08 8.37795895e-08\n",
      " 8.51228378e-07 1.56349188e-07 4.15763338e-07 1.44480627e-09\n",
      " 1.65583373e-07 5.27230043e-07 7.30221359e-08 2.45289471e-07\n",
      " 1.58872346e-08 1.27945441e-07 2.57136392e-08 5.98666986e-07]\n",
      "tensor_name:  Spear/Attention_1/dense_3/kernel\n",
      "[[ 0.02109545  0.06662665 -0.09054234 ... -0.03365678  0.02617675\n",
      "  -0.01345237]\n",
      " [ 0.06879649  0.01337698 -0.05004449 ... -0.07981478 -0.02069179\n",
      "   0.00963027]\n",
      " [ 0.01438935 -0.09660324  0.02661251 ...  0.05497129  0.01977053\n",
      "   0.03466813]\n",
      " ...\n",
      " [-0.05235778  0.02967024 -0.07603732 ...  0.03613557  0.01387937\n",
      "   0.0287775 ]\n",
      " [-0.07732712  0.0644473   0.02300232 ... -0.06007732 -0.01699062\n",
      "   0.02233228]\n",
      " [ 0.03836596 -0.00114742  0.00216773 ...  0.02847776 -0.06994223\n",
      "   0.02749303]]\n",
      "tensor_name:  Spear/Attention_1/dense_3/kernel/Adam\n",
      "[[ 9.6579588e-06 -4.8600673e-06  1.7377950e-05 ... -7.6197243e-06\n",
      "  -9.4152865e-06  1.6226852e-06]\n",
      " [-2.1324686e-05 -9.6921340e-06 -1.0171579e-04 ...  6.1149680e-05\n",
      "   4.1818232e-05 -6.6255256e-05]\n",
      " [ 5.5075583e-05 -2.5358628e-05 -8.7824978e-05 ...  7.1215865e-05\n",
      "   5.5196206e-06 -1.2489976e-04]\n",
      " ...\n",
      " [-1.9964531e-05  1.8457544e-05  6.0639406e-05 ... -4.6974776e-05\n",
      "  -1.7252723e-05  6.9677903e-05]\n",
      " [ 7.6640590e-06 -1.6065787e-06  5.2121428e-05 ... -2.9641425e-05\n",
      "  -1.8230701e-05  3.2391683e-05]\n",
      " [-1.5057697e-06 -5.1283328e-06 -4.7218677e-05 ...  3.0489782e-05\n",
      "   1.7659144e-05 -3.7023921e-05]]\n",
      "tensor_name:  Spear/Attention_1/dense_3/kernel/Adam_1\n",
      "[[9.3274919e-12 2.3619938e-12 3.0198909e-11 ... 5.8059425e-12\n",
      "  8.8646434e-12 2.6330720e-13]\n",
      " [4.5473618e-11 9.3936196e-12 1.0345965e-09 ... 3.7392339e-10\n",
      "  1.7487410e-10 4.3896997e-10]\n",
      " [3.0332795e-10 6.4305138e-11 7.7131240e-10 ... 5.0716320e-10\n",
      "  3.0465803e-12 1.5599741e-09]\n",
      " ...\n",
      " [3.9857718e-11 3.4067641e-11 3.6770884e-10 ... 2.2066000e-10\n",
      "  2.9765253e-11 4.8549459e-10]\n",
      " [5.8737017e-12 2.5810606e-13 2.7166069e-10 ... 8.7860240e-11\n",
      "  3.3235404e-11 1.0492072e-10]\n",
      " [2.2673122e-13 2.6299444e-12 2.2295737e-10 ... 9.2961437e-11\n",
      "  3.1184118e-11 1.3707523e-10]]\n",
      "tensor_name:  Spear/dec0/lstm_cell/bias\n",
      "[-0.00099756  0.00098512  0.00098608 ... -0.00099891  0.00099784\n",
      " -0.0009994 ]\n",
      "tensor_name:  Spear/dec0/lstm_cell/bias/Adam\n",
      "[ 1.29329555e-05 -2.09321888e-06 -2.23996017e-06 ...  2.89142954e-05\n",
      " -1.46186712e-05  5.24562274e-05]\n",
      "tensor_name:  Spear/dec0/lstm_cell/bias/Adam_1\n",
      "[1.6725909e-11 4.3815071e-13 5.0173543e-13 ... 8.3602535e-11 2.1370270e-11\n",
      " 2.7516189e-10]\n",
      "tensor_name:  Spear/dec0/lstm_cell/kernel\n",
      "[[-0.04279811 -0.04181382 -0.00469817 ...  0.00420375 -0.02911714\n",
      "  -0.00196399]\n",
      " [-0.00552873 -0.00936515  0.05327071 ...  0.0514154  -0.04429282\n",
      "  -0.04965667]\n",
      " [ 0.01469923  0.02858216  0.01392812 ... -0.05911951  0.03360733\n",
      "   0.05175417]\n",
      " ...\n",
      " [ 0.00518511  0.04430522 -0.03706231 ...  0.03221409 -0.00050429\n",
      "  -0.01828546]\n",
      " [ 0.05442502 -0.05316323 -0.04991746 ... -0.01362158  0.04011815\n",
      "   0.04164203]\n",
      " [ 0.05837661  0.05623055 -0.02065328 ...  0.04226284  0.04796658\n",
      "   0.04699019]]\n",
      "tensor_name:  Spear/dec0/lstm_cell/kernel/Adam\n",
      "[[ 7.7952535e-08 -2.9589483e-08 -1.8476657e-08 ...  2.3196317e-07\n",
      "  -1.1938126e-07  4.3133400e-07]\n",
      " [-3.7369210e-07  2.6984422e-08  9.7817789e-08 ... -8.5655824e-07\n",
      "   4.3491065e-07 -1.5784714e-06]\n",
      " [ 2.7149102e-07 -4.5020773e-08 -5.8034733e-08 ...  6.3014943e-07\n",
      "  -3.2268917e-07  1.1707092e-06]\n",
      " ...\n",
      " [-2.5623777e-07  1.0492995e-07 -2.3667008e-08 ... -5.1776368e-07\n",
      "   2.0213811e-07 -8.4752349e-07]\n",
      " [ 4.6635381e-07 -1.5385368e-07  4.2108269e-08 ...  4.4864785e-07\n",
      "  -4.0130990e-07  9.0642044e-07]\n",
      " [ 1.2615646e-06 -5.1326140e-07  9.0252691e-08 ...  1.7136896e-06\n",
      "  -8.6857648e-07  3.3292597e-06]]\n",
      "tensor_name:  Spear/dec0/lstm_cell/kernel/Adam_1\n",
      "[[6.0765164e-16 8.7552575e-17 3.4138227e-17 ... 5.3806197e-15\n",
      "  1.4251695e-15 1.8604653e-14]\n",
      " [1.3964391e-14 7.2814929e-17 9.5681932e-16 ... 7.3368219e-14\n",
      "  1.8914474e-14 2.4915386e-13]\n",
      " [7.3706393e-15 2.0268429e-16 3.3679854e-16 ... 3.9708298e-14\n",
      "  1.0412690e-14 1.3705419e-13]\n",
      " ...\n",
      " [6.5656912e-15 1.1010148e-15 5.6011973e-17 ... 2.6807564e-14\n",
      "  4.0859273e-15 7.1828645e-14]\n",
      " [2.1748296e-14 2.3670636e-15 1.7730826e-16 ... 2.0128220e-14\n",
      "  1.6104749e-14 8.2158706e-14]\n",
      " [1.5915242e-13 2.6343377e-14 8.1454399e-16 ... 2.9366928e-13\n",
      "  7.5441498e-14 1.1083823e-12]]\n",
      "tensor_name:  Spear/dense_4/bias\n",
      "[-0.00099983  0.00099498  0.00099977 -0.00099993 -0.00099934 -0.0009999\n",
      " -0.00099956  0.00099994  0.00099976  0.00099991  0.00099889 -0.00099987\n",
      "  0.00099992  0.00099973  0.00099936  0.00099906 -0.00099936 -0.00099916\n",
      " -0.0009997   0.00099982  0.00099887  0.00099984 -0.00099953  0.00099977\n",
      " -0.00099985 -0.0009999  -0.0009996  -0.00099964 -0.00099995 -0.00099976\n",
      "  0.00099972 -0.00099994 -0.00099981 -0.0009998   0.00099896 -0.00099974\n",
      "  0.00099982 -0.00099962  0.00099954 -0.00099993 -0.00099982  0.00099865\n",
      " -0.00099967 -0.00099952 -0.00099982  0.00099982 -0.00099987  0.00099888\n",
      "  0.00099979  0.00099981 -0.00099993 -0.00099982  0.00099993 -0.0009997\n",
      " -0.00099976 -0.0009997   0.00099866 -0.00099985  0.00099984 -0.00099991\n",
      "  0.0009999   0.00099989  0.00099988  0.0009999  -0.00099717 -0.00099979\n",
      " -0.00099955 -0.00099988 -0.00099994 -0.00099973  0.0009999   0.00099967\n",
      "  0.00099965 -0.00099984  0.00099985  0.00099992  0.0009999   0.00099928\n",
      " -0.00099976  0.00099982  0.00099941 -0.00099974 -0.00099912 -0.00099976\n",
      " -0.0009997   0.00099955  0.00099988 -0.0009999  -0.00099772 -0.00099856\n",
      "  0.00099974 -0.00099988  0.00099921  0.00099985  0.00099655 -0.00099949\n",
      "  0.00099993  0.0009999  -0.00099986 -0.00099976  0.00099688  0.00099991\n",
      "  0.00099408  0.00099994  0.00099895  0.00099945 -0.00099982 -0.00099803\n",
      " -0.00099802 -0.00099968  0.00099989  0.00099978  0.00099797 -0.00099982\n",
      "  0.00099991  0.0009999  -0.00099989  0.00099836  0.00099977  0.00099975\n",
      " -0.00099874 -0.00099897  0.00099965 -0.00099888  0.00099987  0.00099991\n",
      " -0.00099965  0.00099992 -0.0009989   0.00099986 -0.0009995  -0.00099778\n",
      "  0.0009997   0.00099976 -0.00099994 -0.00099834 -0.00099985  0.00099992\n",
      " -0.00099995 -0.00099497 -0.00099907 -0.00099991  0.00099979 -0.00099985\n",
      "  0.00099623  0.00099977  0.00099959  0.00099991  0.00099983  0.00099971\n",
      " -0.00099891  0.00099992  0.00099956 -0.00099983 -0.00099995  0.0009998\n",
      " -0.00099966  0.00099821  0.00099988 -0.00099983  0.0009997   0.00099956\n",
      "  0.00093618  0.00099984 -0.00099994  0.00099992 -0.00099972  0.00099935\n",
      " -0.00099891 -0.00099972  0.0009998  -0.00099989 -0.00099983 -0.00099985\n",
      " -0.0009997  -0.00099987 -0.00099993  0.00099981 -0.0009999   0.00099596\n",
      " -0.00099986  0.0009996   0.00099909 -0.00099987 -0.00099987 -0.00099989\n",
      "  0.00099905 -0.00099993 -0.00099991  0.00099954  0.00099973  0.00099743\n",
      "  0.00099978  0.00099973  0.00099984  0.00099969 -0.00099973  0.00097202\n",
      "  0.00099922  0.00099967  0.00099987  0.0009994   0.00099961  0.00099977\n",
      "  0.00099975 -0.00099991  0.00099971  0.00099966  0.00099987  0.00099978\n",
      "  0.0009997   0.00099983  0.00099989 -0.00099938 -0.00099982 -0.00099971\n",
      " -0.00099978  0.00099975  0.00099882  0.00099987 -0.00099981 -0.00099965\n",
      "  0.00099991  0.00099734 -0.00099957  0.00099972  0.00099941  0.00099991\n",
      " -0.00099959  0.00099943 -0.0009997  -0.00099964 -0.00099986 -0.0009999\n",
      "  0.00099978 -0.00099986 -0.0009999  -0.00099952  0.00099982  0.00099693\n",
      "  0.00099994  0.00099992  0.0009999  -0.0009999  -0.00099888  0.00099975\n",
      " -0.00099989  0.00099968 -0.00099977  0.00099989  0.00099984 -0.00099941\n",
      " -0.0009999  -0.00098899 -0.00099974 -0.00099987 -0.00099763  0.00099883\n",
      "  0.00099976 -0.00077015 -0.00099901  0.00098245 -0.00099292  0.00099983\n",
      " -0.00099843  0.00099964  0.00099619  0.00099977  0.00099955  0.00099956\n",
      " -0.00099905  0.00099968  0.0009989   0.0009997  -0.00099968  0.00099849\n",
      "  0.00099962  0.00099949  0.00099954 -0.0009996  -0.00097431 -0.00099981\n",
      "  0.00099969  0.00099976 -0.00099978  0.00099926  0.00099978 -0.00099945\n",
      " -0.00099952  0.00099976 -0.00099956 -0.00099904  0.0009994   0.00099923\n",
      " -0.00099963 -0.00099866 -0.00099958 -0.00099986 -0.00099945 -0.00099844\n",
      "  0.00099975  0.00099948 -0.00099938 -0.00099826  0.00099964  0.00099906\n",
      " -0.00099459 -0.00099837  0.00099537 -0.00099981 -0.00099974  0.00099967\n",
      " -0.00099755 -0.00099977 -0.00099967 -0.00099768  0.00099977  0.00099985\n",
      " -0.00099657  0.00099974  0.00099958 -0.00099902 -0.0009997  -0.00098597\n",
      " -0.00099932 -0.00099986  0.00099941 -0.00099861  0.00099869  0.00099724\n",
      "  0.00099932  0.00099972  0.00099981  0.00099972 -0.0009996   0.00099896\n",
      " -0.00099947  0.00099841  0.00099973 -0.00099978 -0.0009995  -0.00099471\n",
      "  0.00099981 -0.00099945 -0.00099902 -0.00099874 -0.00095861  0.0009964\n",
      "  0.0009995  -0.00099862 -0.00099655 -0.0009953   0.00099307  0.00099958\n",
      "  0.00099947  0.00099889  0.00099777  0.00099922 -0.00099972  0.00099968\n",
      " -0.0009988   0.0009973   0.00099979  0.00099873  0.00099959  0.00099961\n",
      " -0.00099959 -0.00099917 -0.0009993  -0.00099955  0.00099978  0.00099949\n",
      " -0.00099869  0.00099966  0.00099925 -0.00099852  0.0009998  -0.0009994\n",
      " -0.00099908  0.00099976  0.0009993   0.00099931 -0.00099913 -0.00099927\n",
      "  0.00099617  0.00099938 -0.00099942  0.00099932 -0.00099965  0.00099789\n",
      " -0.00099976  0.00099892 -0.00099965 -0.00099927 -0.00099984  0.00099913\n",
      "  0.00099909 -0.0009997  -0.00099932  0.00099962  0.00099964 -0.00099976\n",
      "  0.00099893  0.00099879 -0.00099968 -0.00099933  0.00099939 -0.00099774\n",
      "  0.00099961 -0.00099856 -0.00099989  0.00099973 -0.00099982  0.00099946\n",
      " -0.00099842 -0.00099929  0.00099976  0.00099797 -0.00099897  0.00099896\n",
      " -0.00099928  0.00099887 -0.00098624  0.00098368 -0.00099955 -0.00099804\n",
      "  0.00099948 -0.00099855 -0.00099965  0.00099914  0.00099979 -0.00099969\n",
      " -0.00099972 -0.00099959 -0.00099982 -0.00099948 -0.00099937 -0.00099944\n",
      "  0.00099772 -0.00099973 -0.00099966 -0.00099963  0.00099929 -0.00099947\n",
      " -0.00099948 -0.00099971 -0.00099979 -0.00099872 -0.00099929  0.00099897\n",
      "  0.00099821  0.00099928  0.00099963  0.00099954 -0.00099969 -0.00099957\n",
      "  0.00099254 -0.00099973 -0.00099946 -0.00099962  0.00099979 -0.00099977\n",
      "  0.0009993  -0.00099979  0.00099949 -0.00099953  0.00099922 -0.00098554\n",
      "  0.0009943  -0.00099634 -0.00099957  0.00099972 -0.00099872 -0.00099923\n",
      "  0.00099963 -0.0009992  -0.00099808 -0.00099948  0.00099968  0.00099914\n",
      "  0.00099901  0.00099944  0.00099966  0.00099913 -0.00099965  0.00099875\n",
      "  0.00099973  0.00099931  0.00099935 -0.00099801  0.00099815 -0.0009995\n",
      " -0.00099981  0.00098858  0.00099931 -0.00099693  0.00099981  0.00099983\n",
      "  0.00099978 -0.0009998   0.00099931  0.00099962  0.0009992  -0.00099801\n",
      " -0.00099981 -0.00099956  0.00099938 -0.00099978 -0.00099954  0.00099963\n",
      "  0.00099949  0.00099908]\n",
      "tensor_name:  Spear/dense_4/bias/Adam\n",
      "[ 1.88857564e-04 -6.26422252e-06 -1.40432850e-04  4.41375538e-04\n",
      "  4.81736424e-05  3.29792761e-04  7.12780093e-05 -5.38851426e-04\n",
      " -1.33466048e-04 -3.40182101e-04 -2.84313355e-05  2.38504042e-04\n",
      " -4.08158230e-04 -1.16887109e-04 -4.90874445e-05 -3.37703714e-05\n",
      "  4.94722444e-05  3.77243559e-05  1.04089551e-04 -1.79948634e-04\n",
      " -2.78623193e-05 -2.00914830e-04  6.67601198e-05 -1.36130315e-04\n",
      "  2.05422781e-04  3.12109478e-04  7.90583363e-05  8.71718585e-05\n",
      "  7.00344797e-04  1.32729154e-04 -1.11764988e-04  4.86689416e-04\n",
      "  1.70271611e-04  1.56761307e-04 -3.02609769e-05  1.21014644e-04\n",
      " -1.77012364e-04  8.40900539e-05 -6.93191614e-05  4.42556338e-04\n",
      "  1.71851600e-04 -2.33977171e-05  9.67604938e-05  6.57276760e-05\n",
      "  1.79770752e-04 -1.76239890e-04  2.43847811e-04 -2.81244938e-05\n",
      " -1.47122977e-04 -1.67852544e-04  4.32969216e-04  1.78168862e-04\n",
      " -4.60375159e-04  1.06704247e-04  1.32477697e-04  1.04487597e-04\n",
      " -2.35200750e-05  2.17728535e-04 -1.96367153e-04  3.52201023e-04\n",
      " -3.14071047e-04 -2.88617390e-04 -2.72714708e-04 -3.13689263e-04\n",
      "  1.11370255e-05  1.47239218e-04  7.04783524e-05  2.57962820e-04\n",
      "  5.37727203e-04  1.15172057e-04 -3.15897603e-04 -9.72413109e-05\n",
      " -8.94188051e-05  1.94632710e-04 -2.05516801e-04 -3.74016498e-04\n",
      " -3.29222152e-04 -4.38567877e-05  1.33479320e-04 -1.72778207e-04\n",
      " -5.34510655e-05  1.22490397e-04  3.57106655e-05  1.30719534e-04\n",
      "  1.04975319e-04 -6.94659320e-05 -2.66396411e-04  3.08661605e-04\n",
      "  1.38684718e-05  2.19085850e-05 -1.19484510e-04  2.66686868e-04\n",
      " -3.99869168e-05 -2.08641795e-04 -9.13941585e-06  6.14697346e-05\n",
      " -4.55223548e-04 -3.05209804e-04  2.20145317e-04  1.33616864e-04\n",
      " -1.01030882e-05 -3.36905359e-04 -5.30857415e-06 -5.06293844e-04\n",
      " -3.00046195e-05 -5.73960024e-05  1.73980559e-04  1.60457912e-05\n",
      "  1.59695901e-05  9.81502017e-05 -2.81829416e-04 -1.42944336e-04\n",
      " -1.55263369e-05  1.78581060e-04 -3.44975997e-04 -3.11538199e-04\n",
      "  2.98718951e-04 -1.92590596e-05 -1.39943877e-04 -1.24371887e-04\n",
      "  2.50003013e-05  3.07230221e-05 -8.98447906e-05  2.82472629e-05\n",
      " -2.50231562e-04 -3.64953012e-04  9.12440810e-05 -3.98595177e-04\n",
      "  2.88371903e-05 -2.18358386e-04  6.30517461e-05  1.42108320e-05\n",
      " -1.05848500e-04 -1.33608803e-04  5.36228472e-04  1.90017126e-05\n",
      "  2.14405693e-04 -4.17139527e-04  6.76224125e-04  6.25948223e-06\n",
      "  3.39769904e-05  3.59362952e-04 -1.51402986e-04  2.10407539e-04\n",
      " -8.35273113e-06 -1.37813811e-04 -7.67398888e-05 -3.70113732e-04\n",
      " -1.88831546e-04 -1.09464861e-04  2.90456410e-05 -4.16340772e-04\n",
      " -7.10625245e-05  1.82994656e-04  5.76883845e-04 -1.55996968e-04\n",
      "  9.18504156e-05 -1.76759222e-05 -2.56762840e-04  1.82363889e-04\n",
      " -1.04415398e-04 -7.16275827e-05 -4.63873704e-07 -1.91714993e-04\n",
      "  5.04333759e-04 -3.87200154e-04  1.14479466e-04 -4.89537124e-05\n",
      "  2.88834544e-05  1.13128197e-04 -1.55234156e-04  2.88578973e-04\n",
      "  1.84946752e-04  2.13250285e-04  1.04115636e-04  2.47535383e-04\n",
      "  4.38079209e-04 -1.65448539e-04  3.14866367e-04 -7.79079619e-06\n",
      "  2.32711725e-04 -7.80632981e-05 -3.48044487e-05  2.38930137e-04\n",
      "  2.39531291e-04  2.79023574e-04 -3.32928103e-05  4.55637055e-04\n",
      "  3.41148203e-04 -6.91430832e-05 -1.19193035e-04 -1.22522833e-05\n",
      " -1.40718621e-04 -1.15677860e-04 -1.93601736e-04 -1.02926875e-04\n",
      "  1.15604773e-04 -1.09874577e-06 -4.03724334e-05 -9.49297537e-05\n",
      " -2.35152227e-04 -5.28693454e-05 -8.17672699e-05 -1.36482311e-04\n",
      " -1.26229905e-04  3.71839764e-04 -1.08891436e-04 -9.18945516e-05\n",
      " -2.47997814e-04 -1.43145109e-04 -1.06466403e-04 -1.89427781e-04\n",
      " -2.92830664e-04  5.13383093e-05  1.77337643e-04  1.09822737e-04\n",
      "  1.41619341e-04 -1.27341773e-04 -2.66599582e-05 -2.42112888e-04\n",
      "  1.64474754e-04  9.04910994e-05 -3.34800337e-04 -1.18521421e-05\n",
      "  7.39382376e-05 -1.12603993e-04 -5.32836821e-05 -3.51753377e-04\n",
      "  7.66796657e-05 -5.55487604e-05  1.06672545e-04  8.72072851e-05\n",
      "  2.18938803e-04  3.06311529e-04 -1.45367550e-04  2.28997465e-04\n",
      "  3.11329233e-04  6.53817769e-05 -1.80421950e-04 -1.02690301e-05\n",
      " -5.20625443e-04 -3.83551756e-04 -3.16916528e-04  3.07902606e-04\n",
      "  2.80862278e-05 -1.28199070e-04  2.77708081e-04 -1.00020028e-04\n",
      "  1.39087730e-04 -2.81035958e-04 -1.93267973e-04  5.33667007e-05\n",
      "  3.32385243e-04  2.83956297e-06  1.21901714e-04  2.41244110e-04\n",
      "  1.32906453e-05 -2.69877582e-05 -1.33763824e-04  1.05955706e-07\n",
      "  3.19039100e-05 -1.77072604e-06  4.43292856e-06 -1.83507291e-04\n",
      "  2.01566236e-05 -8.73981044e-05 -8.26560790e-06 -1.37817013e-04\n",
      " -6.97531286e-05 -7.17476578e-05  3.32376912e-05 -9.94067523e-05\n",
      " -2.88327610e-05 -1.04502637e-04  9.81061967e-05 -2.08459751e-05\n",
      " -8.36672189e-05 -6.23939341e-05 -6.91484893e-05  7.92237261e-05\n",
      "  1.19949743e-06  1.64871410e-04 -1.03285398e-04 -1.33965936e-04\n",
      "  1.46517050e-04 -4.26957340e-05 -1.43448633e-04  5.71707642e-05\n",
      "  6.56795164e-05 -1.29337437e-04  7.22509139e-05  3.27920934e-05\n",
      " -5.29717145e-05 -4.10087523e-05  8.45138566e-05  2.36090164e-05\n",
      "  7.45967991e-05  2.30309495e-04  5.79556363e-05  2.02018400e-05\n",
      " -1.26544561e-04 -6.08229384e-05  5.09999300e-05  1.80914394e-05\n",
      " -8.70396616e-05 -3.34917058e-05  5.81006088e-06  1.93820924e-05\n",
      " -6.79542791e-06  1.67754886e-04  1.21514167e-04 -9.48187517e-05\n",
      "  1.28508445e-05  1.37700292e-04  9.47124499e-05  1.36211565e-05\n",
      " -1.39547454e-04 -2.10325699e-04  9.17458692e-06 -1.20679339e-04\n",
      " -7.56590598e-05  3.22274900e-05  1.04752522e-04  2.22166591e-06\n",
      "  4.62507123e-05  2.26096497e-04 -5.36515399e-05  2.27457294e-05\n",
      " -2.40598220e-05 -1.14382647e-05 -4.62059033e-05 -1.12744819e-04\n",
      " -1.64052981e-04 -1.11652866e-04  7.98788315e-05 -3.03649540e-05\n",
      "  6.01124229e-05 -1.98959642e-05 -1.18241500e-04  1.46775652e-04\n",
      "  6.31835283e-05  5.94934545e-06 -1.65290432e-04  5.79455664e-05\n",
      "  3.23867462e-05  2.51599504e-05  7.32422166e-07 -8.75939622e-06\n",
      " -6.37194316e-05  2.29243633e-05  9.14292650e-06  6.69657538e-06\n",
      " -4.52866516e-06 -7.45903017e-05 -6.00136736e-05 -2.84593207e-05\n",
      " -1.41625615e-05 -4.07518492e-05  1.11309069e-04 -9.75449148e-05\n",
      "  2.64232258e-05 -1.16587753e-05 -1.47466591e-04 -2.48910656e-05\n",
      " -7.78033063e-05 -8.03856747e-05  7.75874869e-05  3.78750483e-05\n",
      "  4.48396931e-05  7.09265078e-05 -1.42351972e-04 -6.18502963e-05\n",
      "  2.41467769e-05 -9.42686238e-05 -4.20517354e-05  2.12630039e-05\n",
      " -1.58766576e-04  5.28575692e-05  3.43480897e-05 -1.33397552e-04\n",
      " -4.54417896e-05 -4.55912850e-05  3.62293140e-05  4.31555236e-05\n",
      " -8.22290440e-06 -5.09813071e-05  5.44732866e-05 -4.62878052e-05\n",
      "  8.98357175e-05 -1.49296911e-05  1.34372778e-04 -2.91783344e-05\n",
      "  9.10331801e-05  4.34260837e-05  2.01405346e-04 -3.63288636e-05\n",
      " -3.48353278e-05  1.04127270e-04  4.67403188e-05 -8.38178094e-05\n",
      " -8.75278420e-05  1.33395093e-04 -2.95353257e-05 -2.61640853e-05\n",
      "  9.86477025e-05  4.73996188e-05 -5.14869716e-05  1.39472804e-05\n",
      " -8.07913166e-05  2.19654103e-05  2.83172179e-04 -1.17233561e-04\n",
      "  1.74003566e-04 -5.85938724e-05  2.00225459e-05  4.42404562e-05\n",
      " -1.30543165e-04 -1.55733560e-05  3.06285947e-05 -3.03953020e-05\n",
      "  4.36065166e-05 -2.78866301e-05  2.26589873e-06 -1.90656715e-06\n",
      "  7.06345454e-05  1.60650598e-05 -6.09770395e-05  2.17732904e-05\n",
      "  9.10426243e-05 -3.66618442e-05 -1.48754392e-04  1.00440622e-04\n",
      "  1.13450857e-04  7.68910904e-05  1.77366383e-04  6.11463911e-05\n",
      "  4.99193484e-05  5.68936302e-05 -1.38660516e-05  1.18837874e-04\n",
      "  9.23774569e-05  8.45601899e-05 -4.43026111e-05  6.01076827e-05\n",
      "  6.10712159e-05  1.09165048e-04  1.48846681e-04  2.47227963e-05\n",
      "  4.45597070e-05 -3.07663940e-05 -1.76465801e-05 -4.41320444e-05\n",
      " -8.48684140e-05 -6.87520296e-05  1.02994338e-04  7.30573520e-05\n",
      " -4.20687684e-06  1.16109593e-04  5.83328147e-05  8.39166751e-05\n",
      " -1.50011852e-04  1.34979578e-04 -4.48448664e-05  1.47969797e-04\n",
      " -6.22303342e-05  6.71370726e-05 -4.04426501e-05  2.15566160e-06\n",
      " -5.51877019e-06  8.59862848e-06  7.34137211e-05 -1.13304384e-04\n",
      "  2.45865122e-05  4.09133936e-05 -8.47956180e-05  3.92991569e-05\n",
      "  1.64264748e-05  6.05551140e-05 -1.00231831e-04 -3.66828353e-05\n",
      " -3.18597304e-05 -5.60541012e-05 -9.36038268e-05 -3.62837745e-05\n",
      "  9.14747798e-05 -2.52586269e-05 -1.19272612e-04 -4.58774666e-05\n",
      " -4.89241211e-05  1.58699131e-05 -1.70449366e-05  6.26436231e-05\n",
      "  1.62918353e-04 -2.73818091e-06 -4.59060175e-05  1.02746844e-05\n",
      " -1.68333427e-04 -1.81773401e-04 -1.45165934e-04  1.56895039e-04\n",
      " -4.58030881e-05 -8.41326619e-05 -3.92730763e-05  1.58989933e-05\n",
      "  1.62254320e-04  7.19652307e-05 -5.05631397e-05  1.43947109e-04\n",
      "  6.80562662e-05 -8.65308029e-05 -6.21065192e-05 -3.44291875e-05]\n",
      "tensor_name:  Spear/dense_4/bias/Adam_1\n",
      "[3.56667051e-09 3.92399654e-12 1.97211225e-09 1.94809751e-08\n",
      " 2.32066893e-10 1.08761817e-08 5.08048659e-10 2.90357018e-08\n",
      " 1.78129500e-09 1.15722321e-08 8.08330000e-11 5.68834180e-09\n",
      " 1.66590901e-08 1.36624134e-09 2.40954534e-10 1.14042290e-10\n",
      " 2.44747056e-10 1.42310816e-10 1.08344889e-09 3.23810778e-09\n",
      " 7.76298539e-11 4.03662304e-09 4.45685433e-10 1.85312155e-09\n",
      " 4.21979562e-09 9.74110304e-09 6.25013763e-10 7.59883156e-10\n",
      " 4.90476246e-08 1.76167936e-09 1.24912458e-09 2.36863418e-08\n",
      " 2.89920332e-09 2.45737808e-09 9.15714518e-11 1.46443468e-09\n",
      " 3.13329562e-09 7.07104264e-10 4.80508244e-10 1.95853502e-08\n",
      " 2.95325764e-09 5.47445839e-11 9.36246858e-10 4.32006986e-10\n",
      " 3.23170912e-09 3.10600856e-09 5.94609562e-09 7.90976590e-11\n",
      " 2.16448837e-09 2.81740986e-09 1.87459843e-08 3.17437188e-09\n",
      " 2.11942464e-08 1.13856446e-09 1.75501069e-09 1.09175124e-09\n",
      " 5.53186559e-11 4.74050799e-09 3.85595467e-09 1.24043904e-08\n",
      " 9.86392923e-09 8.32988967e-09 7.43723216e-09 9.83996351e-09\n",
      " 1.24031688e-11 2.16791007e-09 4.96713171e-10 6.65439304e-09\n",
      " 2.89146698e-08 1.32644251e-09 9.97899541e-09 9.45574619e-10\n",
      " 7.99561584e-10 3.78813825e-09 4.22365920e-09 1.39886467e-08\n",
      " 1.08385771e-08 1.92339214e-10 1.78164916e-09 2.98519076e-09\n",
      " 2.85697827e-10 1.50036994e-09 1.27523450e-10 1.70873682e-09\n",
      " 1.10196696e-09 4.82545059e-10 7.09660997e-09 9.52707158e-09\n",
      " 1.92331932e-11 4.79979632e-11 1.42763579e-09 7.11209314e-09\n",
      " 1.59893224e-10 4.35308189e-09 8.35278114e-12 3.77847809e-10\n",
      " 2.07225721e-08 9.31517796e-09 4.84633089e-09 1.78532300e-09\n",
      " 1.02071034e-11 1.13503695e-08 2.81805829e-12 2.56330033e-08\n",
      " 9.00265071e-11 3.29425737e-10 3.02688341e-09 2.57463981e-11\n",
      " 2.55024387e-11 9.63333413e-10 7.94267496e-09 2.04328110e-09\n",
      " 2.41063922e-11 3.18907700e-09 1.19006849e-08 9.70547376e-09\n",
      " 8.92318130e-09 3.70906431e-11 1.95840300e-09 1.54681590e-09\n",
      " 6.25006713e-11 9.43891354e-11 8.07197864e-10 7.97897165e-11\n",
      " 6.26149976e-09 1.33188927e-08 8.32537095e-10 1.58875988e-08\n",
      " 8.31572519e-11 4.76797446e-09 3.97546940e-10 2.01945058e-11\n",
      " 1.12037557e-09 1.78510740e-09 2.87537141e-08 3.61060244e-11\n",
      " 4.59691840e-09 1.74003070e-08 4.57272940e-08 3.91805989e-12\n",
      " 1.15442038e-10 1.29139996e-08 2.29225594e-09 4.42707426e-09\n",
      " 6.97671781e-12 1.89923921e-09 5.88893156e-10 1.36982345e-08\n",
      " 3.56568752e-09 1.19823962e-09 8.43638007e-11 1.73337327e-08\n",
      " 5.04981501e-10 3.34866002e-09 3.32790506e-08 2.43347276e-09\n",
      " 8.43638714e-10 3.12434037e-11 6.59262733e-09 3.32561445e-09\n",
      " 1.09024301e-09 5.13044163e-10 2.15175930e-14 3.67541442e-09\n",
      " 2.54349128e-08 1.49921959e-08 1.31053735e-09 2.39643388e-10\n",
      " 8.34242814e-11 1.27978184e-09 2.40973197e-09 8.32767189e-09\n",
      " 3.42048434e-09 4.54750770e-09 1.08399212e-09 6.12729467e-09\n",
      " 1.91910825e-08 2.73728529e-09 9.91395055e-09 6.06956933e-12\n",
      " 5.41540235e-09 6.09379713e-10 1.21133353e-10 5.70868508e-09\n",
      " 5.73744696e-09 7.78531106e-09 1.10839643e-10 2.07602362e-08\n",
      " 1.16380532e-08 4.78070250e-10 1.42067891e-09 1.50116430e-11\n",
      " 1.98014627e-09 1.33811884e-09 3.74811338e-09 1.05938003e-09\n",
      " 1.33642852e-09 1.20722605e-13 1.62991162e-10 9.01153707e-10\n",
      " 5.52958346e-09 2.79513024e-10 6.68579747e-10 1.86271754e-09\n",
      " 1.59337776e-09 1.38262966e-08 1.18571863e-09 8.44449621e-10\n",
      " 6.15020923e-09 2.04902473e-09 1.13349430e-09 3.58824082e-09\n",
      " 8.57486526e-09 2.63558675e-10 3.14482218e-09 1.20608723e-09\n",
      " 2.00557704e-09 1.62157099e-09 7.10743894e-11 5.86178706e-09\n",
      " 2.70515854e-09 8.18852985e-10 1.12089760e-08 1.40471402e-11\n",
      " 5.46678980e-10 1.26794897e-09 2.83911311e-10 1.23728787e-08\n",
      " 5.87969284e-10 3.08562342e-10 1.13788801e-09 7.60500940e-10\n",
      " 4.79335593e-09 9.38255074e-09 2.11314433e-09 5.24391375e-09\n",
      " 9.69246017e-09 4.27471947e-10 3.25516458e-09 1.05451568e-11\n",
      " 2.71047202e-08 1.47109995e-08 1.00434745e-08 9.48027523e-09\n",
      " 7.88825671e-11 1.64347824e-09 7.71207542e-09 1.00038733e-09\n",
      " 1.93451366e-09 7.89801557e-09 3.73520104e-09 2.84796658e-10\n",
      " 1.10478471e-08 8.06301044e-13 1.48598300e-09 5.81979442e-09\n",
      " 1.76638894e-11 7.28329411e-11 1.78925208e-09 1.12264614e-15\n",
      " 1.01784595e-10 3.13542866e-13 1.96505919e-12 3.36744788e-09\n",
      " 4.06284069e-11 7.63832719e-10 6.83193606e-12 1.89932736e-09\n",
      " 4.86543472e-10 5.14765786e-10 1.10472936e-10 9.88157001e-10\n",
      " 8.31317029e-11 1.09206555e-09 9.62469660e-10 4.34548925e-11\n",
      " 7.00010994e-10 3.89295124e-10 4.78144913e-10 6.27631502e-10\n",
      " 1.43877477e-13 2.71822187e-09 1.06677311e-09 1.79466331e-09\n",
      " 2.14669593e-09 1.82290141e-10 2.05772377e-09 3.26845245e-10\n",
      " 4.31374103e-10 1.67279479e-09 5.22012433e-10 1.07530707e-10\n",
      " 2.80596518e-10 1.68169548e-10 7.14249604e-10 5.57378241e-11\n",
      " 5.56460877e-10 5.30417532e-09 3.35881073e-10 4.08108929e-11\n",
      " 1.60133140e-09 3.69938052e-10 2.60095806e-10 3.27295795e-11\n",
      " 7.57580110e-10 1.12167942e-10 3.37563571e-12 3.75660475e-11\n",
      " 4.61772287e-12 2.81413270e-09 1.47654966e-09 8.99047559e-10\n",
      " 1.65141997e-11 1.89611171e-09 8.97032837e-10 1.85533412e-11\n",
      " 1.94732319e-09 4.42363124e-09 8.41719142e-12 1.45633094e-09\n",
      " 5.72421666e-10 1.03859733e-10 1.09729437e-09 4.93573335e-13\n",
      " 2.13909987e-10 5.11189402e-09 2.87844915e-10 5.17361327e-11\n",
      " 5.78867301e-11 1.30832142e-11 2.13495707e-10 1.27112243e-09\n",
      " 2.69130229e-09 1.24661959e-09 6.38054221e-10 9.22018087e-11\n",
      " 3.61345481e-10 3.95844087e-11 1.39808654e-09 2.15428053e-09\n",
      " 3.99210498e-10 3.53942375e-12 2.73205636e-09 3.35764389e-10\n",
      " 1.04888744e-10 6.33014682e-11 5.36435046e-14 7.67260040e-12\n",
      " 4.06011225e-10 5.25519420e-11 8.35919962e-12 4.48435256e-12\n",
      " 2.05085341e-12 5.56363899e-10 3.60159264e-10 8.09922199e-11\n",
      " 2.00575476e-11 1.66069089e-10 1.23895438e-09 9.51488333e-10\n",
      " 6.98177557e-11 1.35925229e-11 2.17461071e-09 6.19556836e-11\n",
      " 6.05327399e-10 6.46177001e-10 6.01973860e-10 1.43450030e-10\n",
      " 2.01057129e-10 5.03050213e-10 2.02638128e-09 3.82540860e-10\n",
      " 5.83059087e-11 8.88645435e-10 1.76832479e-10 4.52109253e-11\n",
      " 2.52064902e-09 2.79388512e-10 1.17977544e-10 1.77946702e-09\n",
      " 2.06492851e-10 2.07853734e-10 1.31254549e-10 1.86237442e-10\n",
      " 6.76152537e-12 2.59905902e-10 2.96729918e-10 2.14253226e-10\n",
      " 8.07034828e-10 2.22892694e-11 1.80558024e-09 8.51363841e-11\n",
      " 8.28692892e-10 1.88579943e-10 4.05635747e-09 1.31976860e-10\n",
      " 1.21348390e-10 1.08423437e-09 2.18462817e-10 7.02533198e-10\n",
      " 7.66102071e-10 1.77940140e-09 8.72323810e-11 6.84550194e-11\n",
      " 9.73123915e-10 2.24669380e-10 2.65087258e-10 1.94524050e-11\n",
      " 6.52714993e-10 4.82472777e-11 8.01854050e-09 1.37435252e-09\n",
      " 3.02768366e-09 3.43319595e-10 4.00896989e-11 1.95719205e-10\n",
      " 1.70412906e-09 2.42526155e-11 9.38098141e-11 9.23862029e-11\n",
      " 1.90150298e-10 7.77653775e-11 5.13422855e-13 3.63494960e-13\n",
      " 4.98917241e-10 2.58082670e-11 3.71814940e-10 4.74069846e-11\n",
      " 8.28864866e-10 1.34407291e-10 2.21275731e-09 1.00881847e-09\n",
      " 1.28709243e-09 5.91216132e-10 3.14584137e-09 3.73883174e-10\n",
      " 2.49190835e-10 3.23684191e-10 1.92264833e-11 1.41222511e-09\n",
      " 8.53348114e-10 7.15032977e-10 1.96269528e-10 3.61288527e-10\n",
      " 3.72964326e-10 1.19168486e-09 2.21550378e-09 6.11208445e-11\n",
      " 1.98554090e-10 9.46558318e-11 3.11397609e-11 1.94761152e-10\n",
      " 7.20255189e-10 4.72677841e-10 1.06076914e-09 5.33730504e-10\n",
      " 1.76975752e-12 1.34812583e-09 3.40267203e-10 7.04191372e-10\n",
      " 2.25032548e-09 1.82192439e-09 2.01103509e-10 2.18947682e-09\n",
      " 3.87256227e-10 4.50732673e-10 1.63558625e-10 4.64681462e-13\n",
      " 3.04564208e-12 7.39354238e-12 5.38950218e-10 1.28377120e-09\n",
      " 6.04488543e-11 1.67388353e-10 7.19020066e-10 1.54440308e-10\n",
      " 2.69825481e-11 3.66687292e-10 1.00462871e-09 1.34561237e-10\n",
      " 1.01502896e-10 3.14202053e-10 8.76155870e-10 1.31649469e-10\n",
      " 8.36752390e-10 6.37989730e-11 1.42257661e-09 2.10471390e-10\n",
      " 2.39353759e-10 2.51850797e-11 2.90526006e-11 3.92417127e-10\n",
      " 2.65420375e-09 7.49753449e-13 2.10733431e-10 1.05567742e-11\n",
      " 2.83357648e-09 3.30411298e-09 2.10728679e-09 2.46157250e-09\n",
      " 2.09789505e-10 7.07821080e-10 1.54235402e-10 2.52774641e-11\n",
      " 2.63261146e-09 5.17892562e-10 2.55659688e-10 2.07204920e-09\n",
      " 4.63159400e-10 7.48747953e-10 3.85716792e-10 1.18535307e-10]\n",
      "tensor_name:  Spear/dense_4/kernel\n",
      "[[-0.05898841 -0.02894622  0.0122417  ...  0.02900831  0.04137269\n",
      "   0.01530162]\n",
      " [-0.05757773  0.03779107 -0.02534684 ... -0.0416358  -0.04566051\n",
      "  -0.04553201]\n",
      " [ 0.03869519  0.04691197  0.03149872 ...  0.05356413  0.00643662\n",
      "  -0.04646793]\n",
      " ...\n",
      " [-0.03023628 -0.01337879  0.01281395 ... -0.03169182  0.00391839\n",
      "   0.05278713]\n",
      " [-0.06080505  0.04167052 -0.03196853 ... -0.01596226  0.02517113\n",
      "   0.03208419]\n",
      " [ 0.06138061 -0.04992196 -0.04139034 ... -0.02216052 -0.04996465\n",
      "  -0.0222857 ]]\n",
      "tensor_name:  Spear/dense_4/kernel/Adam\n",
      "[[-2.43875616e-06  4.98815602e-08  1.81643566e-06 ...  1.11592124e-06\n",
      "   8.10904851e-07  4.50025766e-07]\n",
      " [-3.09767714e-08  3.18288933e-08 -4.18921386e-09 ...  6.60851462e-09\n",
      "   2.21387833e-08  6.15754869e-09]\n",
      " [ 2.62953927e-06 -4.42735981e-08 -1.97653117e-06 ... -1.20728032e-06\n",
      "  -8.69010194e-07 -4.89043259e-07]\n",
      " ...\n",
      " [ 5.99853081e-07 -1.93702316e-08 -4.47473269e-07 ... -2.72105950e-07\n",
      "  -1.95521508e-07 -1.12215055e-07]\n",
      " [-5.05514663e-07  9.68445857e-09  3.70911465e-07 ...  2.20793154e-07\n",
      "   1.61916219e-07  9.76436212e-08]\n",
      " [ 5.53563837e-07 -3.84623959e-08 -4.20842952e-07 ... -2.60615849e-07\n",
      "  -1.87075585e-07 -1.01364598e-07]]\n",
      "tensor_name:  Spear/dense_4/kernel/Adam_1\n",
      "[[5.9474528e-13 2.4881368e-16 3.2993944e-13 ... 1.2452637e-13\n",
      "  6.5755791e-14 2.0252050e-14]\n",
      " [9.5954758e-17 1.0130650e-16 1.7549278e-18 ... 4.3671881e-18\n",
      "  4.9011917e-17 3.7914895e-18]\n",
      " [6.9143839e-13 1.9601254e-16 3.9066230e-13 ... 1.4575061e-13\n",
      "  7.5516863e-14 2.3916009e-14]\n",
      " ...\n",
      " [3.5981888e-14 3.7520085e-17 2.0022964e-14 ... 7.4040666e-15\n",
      "  3.8228151e-15 1.2592049e-15]\n",
      " [2.5554166e-14 9.3787483e-18 1.3757348e-14 ... 4.8748961e-15\n",
      "  2.6216513e-15 9.5341510e-16]\n",
      " [3.0642884e-14 1.4793360e-16 1.7710641e-14 ... 6.7919710e-15\n",
      "  3.4996803e-15 1.0274643e-15]]\n",
      "tensor_name:  Spear/dense_5/bias\n",
      "[-0.00099656  0.001      -0.00099657 ... -0.00099652 -0.00099656\n",
      " -0.00099657]\n",
      "tensor_name:  Spear/dense_5/bias/Adam\n",
      "[ 9.174374e-06 -8.269165e-02  9.182255e-06 ...  9.055797e-06  9.163765e-06\n",
      "  9.179932e-06]\n",
      "tensor_name:  Spear/dense_5/bias/Adam_1\n",
      "[8.4168020e-12 6.8378169e-04 8.4312678e-12 ... 8.2006355e-12 8.3973471e-12\n",
      " 8.4270030e-12]\n",
      "tensor_name:  Spear/dense_5/kernel\n",
      "[[ 0.01410715  0.01859521  0.00412926 ...  0.00821465  0.02220949\n",
      "  -0.005128  ]\n",
      " [-0.01267442  0.01866025  0.00944885 ...  0.0192408  -0.01131843\n",
      "  -0.01863402]\n",
      " [-0.00576552  0.00887901  0.00379937 ...  0.01608261  0.0040151\n",
      "   0.0084624 ]\n",
      " ...\n",
      " [ 0.02252377  0.02049712 -0.00312342 ...  0.02144416  0.01218511\n",
      "  -0.0041487 ]\n",
      " [ 0.01445385  0.02395754 -0.0034908  ...  0.00670127 -0.0063907\n",
      "  -0.01141482]\n",
      " [-0.00984189 -0.0107931  -0.02354245 ... -0.00563991 -0.004728\n",
      "  -0.00483967]]\n",
      "tensor_name:  Spear/dense_5/kernel/Adam\n",
      "[[-3.21155369e-07  3.48195620e-03 -3.23315732e-07 ... -3.14859619e-07\n",
      "  -3.22184007e-07 -3.22697389e-07]\n",
      " [ 4.67334011e-08 -4.76726767e-04  4.74656545e-08 ...  4.56196325e-08\n",
      "   4.72453578e-08  4.72021142e-08]\n",
      " [ 6.52074883e-09 -9.94559159e-05  6.02448802e-09 ...  6.84892365e-09\n",
      "   6.12512085e-09  6.22349594e-09]\n",
      " ...\n",
      " [-1.07023155e-07  1.13632681e-03 -1.07378469e-07 ... -1.05185180e-07\n",
      "  -1.07013413e-07 -1.07302654e-07]\n",
      " [ 1.27865590e-07 -1.35282625e-03  1.28501370e-07 ...  1.25288310e-07\n",
      "   1.28077104e-07  1.28215660e-07]\n",
      " [ 4.12779087e-07 -4.44160635e-03  4.14608309e-07 ...  4.05437191e-07\n",
      "   4.13239150e-07  4.14120535e-07]]\n",
      "tensor_name:  Spear/dense_5/kernel/Adam_1\n",
      "[[1.03139390e-14 1.21238577e-06 1.04531667e-14 ... 9.91352538e-15\n",
      "  1.03801152e-14 1.04132215e-14]\n",
      " [2.18398168e-16 2.27265371e-08 2.25295809e-16 ... 2.08112329e-16\n",
      "  2.23209381e-16 2.22800979e-16]\n",
      " [4.25196023e-18 9.89134663e-10 3.62939721e-18 ... 4.69071337e-18\n",
      "  3.75166030e-18 3.87313880e-18]\n",
      " ...\n",
      " [1.14538019e-15 1.29122142e-07 1.15299819e-15 ... 1.10637739e-15\n",
      "  1.14517171e-15 1.15137061e-15]\n",
      " [1.63493887e-15 1.83011451e-07 1.65123811e-15 ... 1.56969520e-15\n",
      "  1.64035268e-15 1.64390365e-15]\n",
      " [1.70384303e-14 1.97276017e-06 1.71897764e-14 ... 1.64377128e-14\n",
      "  1.70764332e-14 1.71493543e-14]]\n",
      "tensor_name:  Spear/embedding_1/embeddings\n",
      "[[-0.04467436 -0.02400988 -0.04432923 ...  0.00681594  0.04553241\n",
      "   0.03759063]\n",
      " [ 0.01977843  0.02982054  0.02520482 ...  0.01893716  0.02924964\n",
      "  -0.03297545]\n",
      " [-0.01302441  0.01825335 -0.01907181 ...  0.01385263  0.02095794\n",
      "  -0.04859604]\n",
      " ...\n",
      " [ 0.029526    0.00841658 -0.0353816  ... -0.00365587 -0.02158752\n",
      "  -0.01148111]\n",
      " [ 0.01706529  0.00179927 -0.02684476 ...  0.0099903   0.01147996\n",
      "   0.02089662]\n",
      " [-0.0486888   0.04715612 -0.03776426 ... -0.01984856  0.02770456\n",
      "  -0.01583707]]\n",
      "tensor_name:  Spear/embedding_1/embeddings/Adam\n",
      "[[-1.5236231e-04 -4.7266192e-05  1.2463627e-04 ...  5.1507010e-05\n",
      "   9.6360032e-05 -2.9168550e-05]\n",
      " [ 2.5557811e-05 -6.5500382e-05  5.3460884e-05 ...  5.3496260e-05\n",
      "   9.4522547e-05  1.2835514e-04]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "tensor_name:  Spear/embedding_1/embeddings/Adam_1\n",
      "[[2.3213964e-09 2.2340631e-10 1.5533991e-09 ... 2.6529368e-10\n",
      "  9.2851316e-10 8.5079291e-11]\n",
      " [6.5319306e-11 4.2902423e-10 2.8580283e-10 ... 2.8618113e-10\n",
      "  8.9343927e-10 1.6474822e-09]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "tensor_name:  Spear/embedding_2/embeddings\n",
      "[[ 0.02345319  0.01809009  0.02596461 ...  0.00776425  0.02838442\n",
      "   0.00451776]\n",
      " [ 0.00920722 -0.02906482  0.02328834 ...  0.01616769  0.00977257\n",
      "   0.01098939]\n",
      " [-0.01202697 -0.04563269  0.02124287 ... -0.04112969  0.02151168\n",
      "   0.00380273]\n",
      " ...\n",
      " [ 0.00588303  0.04702334 -0.04788821 ...  0.00320004  0.0439221\n",
      "  -0.04203594]\n",
      " [-0.02743081 -0.01247464  0.01085684 ...  0.0480882  -0.01889975\n",
      "  -0.01842541]\n",
      " [ 0.00086443  0.04374708  0.01030334 ... -0.02222599 -0.03832223\n",
      "  -0.03869347]]\n",
      "tensor_name:  Spear/embedding_2/embeddings/Adam\n",
      "[[-3.4304805e-05 -1.7829363e-04  2.1157437e-05 ... -2.6954924e-05\n",
      "  -4.7408244e-06 -9.4321003e-05]\n",
      " [-3.0907578e-04 -2.7557984e-03 -6.9081114e-04 ... -9.1814046e-04\n",
      "  -6.2883127e-04 -1.8095049e-03]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "tensor_name:  Spear/embedding_2/embeddings/Adam_1\n",
      "[[1.1768038e-10 3.1788192e-09 4.4763120e-11 ... 7.2655819e-11\n",
      "  2.2475116e-12 8.8963331e-10]\n",
      " [9.5526573e-09 7.5943228e-07 4.7721368e-08 ... 8.4297071e-08\n",
      "  3.9542353e-08 3.2742642e-07]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/bias\n",
      "[ 9.6794823e-04 -6.1374845e-04 -9.1411744e-04 ...  7.4674022e-06\n",
      "  9.7961898e-04 -9.6192613e-04]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/bias/Adam\n",
      "[-9.5499752e-07  5.0248502e-08  3.3658898e-07 ... -2.3791819e-10\n",
      " -1.5199690e-06  7.9894562e-07]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/bias/Adam_1\n",
      "[9.1200803e-14 2.5248784e-16 1.1329062e-14 ... 5.6604309e-21 2.3102749e-13\n",
      " 6.3830560e-14]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/kernel\n",
      "[[ 0.04877724 -0.03331528  0.05854664 ... -0.06263816 -0.03378188\n",
      "   0.0620234 ]\n",
      " [ 0.06221524 -0.04151149 -0.04941664 ... -0.04807894  0.01210044\n",
      "  -0.06022522]\n",
      " [-0.03109218 -0.04153423  0.02667543 ...  0.03639242  0.04104504\n",
      "   0.00731127]\n",
      " ...\n",
      " [ 0.01713452  0.05061579 -0.05473443 ...  0.03872667  0.02008935\n",
      "  -0.04075112]\n",
      " [ 0.00742522  0.06308895  0.05756633 ...  0.00544388 -0.02170625\n",
      "  -0.04056479]\n",
      " [-0.04025136  0.00121659  0.02297199 ...  0.04125358  0.04388943\n",
      "   0.00644938]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/kernel/Adam\n",
      "[[-1.6311894e-08 -1.4473056e-09 -1.6244623e-08 ... -4.2099293e-09\n",
      "  -2.5820885e-08  2.2648625e-09]\n",
      " [ 5.0589428e-08 -2.7843738e-09 -2.5544034e-09 ... -9.0697778e-09\n",
      "  -1.0954519e-08 -2.4477242e-08]\n",
      " [ 2.1683453e-08 -5.8705563e-10 -9.7774766e-09 ... -3.9443742e-09\n",
      "  -2.1096607e-08 -2.0323624e-08]\n",
      " ...\n",
      " [ 3.4878533e-09 -1.1733386e-10  1.2754089e-10 ...  3.4216239e-09\n",
      "   3.6930854e-09 -4.2867498e-10]\n",
      " [-5.6211076e-09  5.5410655e-11 -2.4442853e-10 ... -2.5050789e-10\n",
      "  -1.1853409e-08  2.7369984e-09]\n",
      " [ 1.1076766e-09 -4.9218674e-11  7.4441980e-10 ...  4.2522671e-10\n",
      "  -8.8433244e-10  1.1098905e-08]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/bw/lstm_cell/kernel/Adam_1\n",
      "[[2.66074324e-17 2.09466572e-19 2.63884280e-17 ... 1.77232690e-18\n",
      "  6.66709228e-17 5.12953413e-19]\n",
      " [2.55925579e-16 7.75263449e-19 6.52488908e-19 ... 8.22597784e-18\n",
      "  1.19999886e-17 5.99127447e-17]\n",
      " [4.70165887e-17 3.44629680e-20 9.55977762e-18 ... 1.55578808e-18\n",
      "  4.45060889e-17 4.13044135e-17]\n",
      " ...\n",
      " [1.21649586e-18 1.37670511e-21 1.62664623e-21 ... 1.17073540e-18\n",
      "  1.36386977e-18 1.83759773e-20]\n",
      " [3.15964320e-18 3.07029991e-22 5.97445091e-21 ... 6.27533705e-21\n",
      "  1.40501425e-17 7.49105981e-19]\n",
      " [1.22693115e-19 2.42244568e-22 5.54153431e-20 ... 1.80815333e-20\n",
      "  7.82033482e-20 1.23184043e-17]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/bias\n",
      "[-0.00099303  0.00079224  0.00099057 ...  0.00093176  0.00097726\n",
      " -0.0009645 ]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/bias/Adam\n",
      "[ 4.5077841e-06 -1.2058824e-07 -3.3220974e-06 ... -4.3175379e-07\n",
      " -1.3592723e-06  8.5907362e-07]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/bias/Adam_1\n",
      "[2.0319848e-12 1.4541329e-15 1.1036184e-12 ... 1.8640884e-14 1.8475966e-13\n",
      " 7.3799758e-14]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/kernel\n",
      "[[-0.00185441 -0.05883053 -0.0600315  ... -0.057749   -0.03866227\n",
      "   0.04984944]\n",
      " [-0.0353009   0.04074535 -0.04055229 ...  0.00031644 -0.03160972\n",
      "  -0.02991091]\n",
      " [-0.01153293 -0.04960167 -0.01022156 ... -0.02140047  0.04299844\n",
      "   0.04705796]\n",
      " ...\n",
      " [ 0.00970904  0.06004722 -0.06184458 ... -0.00666265 -0.04932821\n",
      "   0.03926149]\n",
      " [ 0.05872828 -0.03285671 -0.03987806 ...  0.0359603  -0.0096067\n",
      "  -0.04985136]\n",
      " [-0.04365139  0.02288688 -0.05862705 ...  0.04125756 -0.03440792\n",
      "   0.01794022]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/kernel/Adam\n",
      "[[ 4.9441493e-08  8.7172802e-09  2.8189307e-08 ...  2.6220986e-08\n",
      "   1.6136060e-09 -6.9215207e-09]\n",
      " [ 3.2216601e-08  5.8085008e-09  5.8860259e-08 ...  3.5024712e-08\n",
      "   2.3652927e-08 -5.2238320e-09]\n",
      " [ 4.1277506e-08  1.4087201e-08  8.8844537e-08 ...  3.8277420e-08\n",
      "   2.8809451e-08 -1.1935667e-08]\n",
      " ...\n",
      " [-5.3632427e-09  8.9286335e-11  2.1874305e-09 ...  1.0263586e-08\n",
      "   4.1409960e-09 -5.0371973e-10]\n",
      " [-4.8576876e-09 -2.0042195e-11  2.9338068e-09 ...  7.8328471e-10\n",
      "   1.2361316e-08 -3.7107467e-10]\n",
      " [ 5.0982081e-09  2.1217945e-10 -3.3360401e-09 ...  9.8300357e-10\n",
      "  -2.3439775e-10  6.6524821e-09]]\n",
      "tensor_name:  Spear/enc0/bidirectional_rnn/fw/lstm_cell/kernel/Adam_1\n",
      "[[2.4444283e-16 7.5989956e-18 7.9462636e-17 ... 6.8753089e-17\n",
      "  2.6036895e-19 4.7906810e-18]\n",
      " [1.0378955e-16 3.3738230e-18 3.4644839e-16 ... 1.2267141e-16\n",
      "  5.5945355e-17 2.7288056e-18]\n",
      " [1.7038098e-16 1.9844657e-17 7.8932459e-16 ... 1.4651415e-16\n",
      "  8.2997337e-17 1.4245825e-17]\n",
      " ...\n",
      " [2.8763990e-18 7.9719431e-22 4.7847885e-19 ... 1.0533980e-17\n",
      "  1.7147620e-18 2.5373017e-20]\n",
      " [2.3596814e-18 4.0168422e-23 8.6071069e-19 ... 6.1352678e-20\n",
      "  1.5280009e-17 1.3769457e-20]\n",
      " [2.5991378e-18 4.5019519e-21 1.1129015e-18 ... 9.6628299e-20\n",
      "  5.4941575e-21 4.4254928e-18]]\n",
      "tensor_name:  beta1_power\n",
      "0.80999994\n",
      "tensor_name:  beta2_power\n",
      "0.99800104\n"
     ]
    }
   ],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\"/tmp/model1.ckpt\", tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
