{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "import pickle\n",
    "import rupo.api\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple, Optional, Tuple\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from keras import backend as K\n",
    "from utils import infer_length, infer_mask\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_ROOT = '/srv/hd7/data/aklyopova/models/model_translator_attn_reversed_amalgama_subtitles_09_03_2019/'\n",
    "#MODEL_PATH = MODEL_ROOT + 'model_r2_14_03_2019.pkl'\n",
    "MODEL_PATH = 'model_translator_attn_reversed_amalgama_subtitles_09_03_2019_420000_iters.pkl'\n",
    "\n",
    "RUPO_DATA_ROOT = '/srv/hd6/data/Poem2Poem/data/rupo/'\n",
    "RUPO_STRESS_MODEL_PATH   = RUPO_DATA_ROOT + 'stress_models/stress_ru_LSTM64_dropout0.2_acc99_wer8.h5'\n",
    "RUPO_ZALYZNIAK_DICT_PATH = RUPO_DATA_ROOT + 'dict/zaliznyak.txt'\n",
    "del RUPO_DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reversed(line):\n",
    "    return ''.join(reversed(line))\n",
    "\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations\n",
    "        given inp and reference out \"\"\"\n",
    "    translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "    # Note: if you experience out-of-memory error,\n",
    "    # split input lines into batches and translate separately\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100\n",
    "\n",
    "def compute_bleu_large(model, inp_lines, out_lines):\n",
    "    batch_size = 256\n",
    "    result = 0.0\n",
    "    for i in range(0, inp_lines.shape[0], batch_size):\n",
    "        current_bleu = compute_bleu(model,\n",
    "                                    inp_lines[i : i + batch_size],\n",
    "                                    out_lines[i : i + batch_size])\n",
    "        current_bleu *= min(i + batch_size, inp_lines.shape[0]) - i\n",
    "        result += current_bleu\n",
    "    result /= inp_lines.shape[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterType(Enum):\n",
    "    NOT_LETTER = 0\n",
    "    VOWEL = 1\n",
    "    CONSONANT = 2\n",
    "    SIGN = 3\n",
    "\n",
    "class RuAlphabetInfo(object):\n",
    "    \n",
    "    _ru_letters = set(''.join([chr(n) for n in range(ord('а'), ord('я') + 1)]) + 'ё')\n",
    "    assert len(_ru_letters) == 33\n",
    "    _ru_vowels = set('аеёийоуыэюя')\n",
    "    _ru_consonants = set('бвгджзклмнпрстфхцчшщ')\n",
    "    _ru_signs = set('ъь')\n",
    "\n",
    "    assert _ru_vowels & _ru_consonants == set()\n",
    "    assert _ru_vowels | _ru_consonants | _ru_signs == _ru_letters\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ru_letter_type(ch: str) -> LetterType:\n",
    "\n",
    "        if ch in RuAlphabetInfo._ru_vowels:\n",
    "            return LetterType.VOWEL\n",
    "        elif ch in RuAlphabetInfo._ru_consonants:\n",
    "            return LetterType.CONSONANT\n",
    "        elif ch in RuAlphabetInfo._ru_signs:\n",
    "            return LetterType.SIGN\n",
    "        else:\n",
    "            return LetterType.NOT_LETTER\n",
    "    \n",
    "    @staticmethod\n",
    "    def lower_and_strip_left_non_letters(line: str) -> str:\n",
    "        \n",
    "        line = line.lower()\n",
    "        \n",
    "        # Skip everything before first letter:\n",
    "        while len(line) > 0 and line[0] not in RuAlphabetInfo._ru_letters:\n",
    "            line = line[1:]\n",
    "        \n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RhymeInfo(NamedTuple):\n",
    "#    text: str\n",
    "#    finished: bool\n",
    "\n",
    "# Old-style for Python 3.5:\n",
    "RhymeInfo = NamedTuple('RhymeInfo',\n",
    "                       [('text', str), # text that will be used for testing whether rhyme is present\n",
    "                        ('finished', bool) # is text complete\n",
    "                       ])\n",
    "        \n",
    "class IRhymeTester(object):\n",
    "\n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        # Returns True - rhyme present, False - absent, None - don't know\n",
    "        raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedSuffixRhymeTester(IRhymeTester):\n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # input: line is REVERSED string\n",
    "        # We iterate all vowels from the beginning of line until first consonant\n",
    "        # Or all consonants until first vowel\n",
    "        # SIGNs are ignored but inserted to output\n",
    "\n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "            \n",
    "        finished = False\n",
    "        prefix_len = 0\n",
    "        prev_ch_type = None\n",
    "\n",
    "        for ch in line:\n",
    "            \n",
    "            ch_type = RuAlphabetInfo.get_ru_letter_type(ch)\n",
    "            \n",
    "            if ch_type == LetterType.NOT_LETTER:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            prefix_len += 1\n",
    "\n",
    "            if ch_type != LetterType.SIGN:\n",
    "                if prev_ch_type is None:\n",
    "                    prev_ch_type = ch_type\n",
    "                elif prev_ch_type != ch_type:\n",
    "                    finished = True\n",
    "                    break\n",
    "\n",
    "        return RhymeInfo(text = line[:prefix_len],\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            l = min(len(info1.text), len(info2.text))\n",
    "            if info1.text[:l] != info2.text[:l]:\n",
    "                return False\n",
    "            return None\n",
    "        \n",
    "        return info1.text == info2.text\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedSuffixRhymeTester()\n",
    "    for line, suffix, finished in [('пижама', 'ма', True),\n",
    "                                   ('обученный', 'ный', True),\n",
    "                                   ('Ихтиандр', 'андр', True),\n",
    "                                   ('КНДР', 'кндр', False),\n",
    "                                   ('махать', 'ать', True)]:\n",
    "        for l in (line, line + '!', ' ' + line + ', '):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == get_reversed(suffix)\n",
    "            assert info.finished == (finished or l[0] == ' ')\n",
    "            assert tester.is_rhyme(info, info) == (True if info.finished else None)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aklyopova/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1190: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/aklyopova/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/aklyopova/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "def make_rupo_engine():\n",
    "    rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "    rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                     zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "    return rupo_engine\n",
    "\n",
    "global_rupo_engine = make_rupo_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedWordRhymeTester(IRhymeTester):\n",
    "    \n",
    "    _WORD_INTERNAL_CHARS = set('-.')\n",
    "    \n",
    "    \n",
    "    def __init__(self, rupo_engine = None):\n",
    "        \n",
    "        if rupo_engine is None:\n",
    "            self._rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "            self._rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                                   zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "        else:\n",
    "            # Assume loaded engine:\n",
    "            assert rupo_engine.language == 'ru'\n",
    "            self._rupo_engine = rupo_engine\n",
    "    \n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # line is REVERSED string\n",
    "        # Here we skip everything until first letter\n",
    "        # return: it TRIES to return complete word (returns part in case if line contains only part)\n",
    "        \n",
    "        \n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "        \n",
    "        is_internal_char = lambda ch: ch in RuReversedWordRhymeTester._WORD_INTERNAL_CHARS\n",
    "            \n",
    "        finished = False\n",
    "        word_len = 0\n",
    "        \n",
    "        for ch in line:\n",
    "            \n",
    "            is_word_char = is_internal_char(ch) or \\\n",
    "                           RuAlphabetInfo.get_ru_letter_type(ch) != LetterType.NOT_LETTER\n",
    "            \n",
    "            if not is_word_char:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            word_len += 1\n",
    "        \n",
    "        while word_len > 0 and is_internal_char(line[word_len - 1]):\n",
    "            word_len -= 1\n",
    "\n",
    "        return RhymeInfo(text = get_reversed(line[:word_len]),\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            return None\n",
    "        \n",
    "        return self._rupo_engine.is_rhyme(info1.text, info2.text)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedWordRhymeTester(global_rupo_engine)\n",
    "    augment_line = lambda l: (l, l + '!', ' ' + l + ', ', 'Мы и ' + l)\n",
    "    \n",
    "    for line, word, finished in [('серая корова', 'корова', True),\n",
    "                                 ('молока, много', 'много', True),\n",
    "                                 ('Ихтиандр', 'ихтиандр', False),\n",
    "                                 ('КНДР', 'кндр', False),\n",
    "                                 ('аб-вг', 'аб-вг', False),\n",
    "                                 ('.-аб.вг.-', 'аб.вг', False)]:\n",
    "        for l in augment_line(line):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == word\n",
    "            assert info.finished == (finished or not l.startswith(line))\n",
    "            any_vowels = any([RuAlphabetInfo.get_ru_letter_type(ch) == LetterType.VOWEL for ch in word])\n",
    "            assert tester.is_rhyme(info, info) == (any_vowels if info.finished else None)\n",
    "    \n",
    "    for line1, line2, is_rhyme in [('серая корова', 'не очень здорова', True),\n",
    "                                   ('молока много', 'не мало', False),\n",
    "                                   ('и играть', 'и скакать', False),\n",
    "                                   ('и играть', 'не играть', True)]:\n",
    "        for l1 in augment_line(line1):\n",
    "            for l2 in augment_line(line2):\n",
    "                info1 = tester.extract_rhyme_info(get_reversed(l1))\n",
    "                info2 = tester.extract_rhyme_info(get_reversed(l2))\n",
    "                assert tester.is_rhyme(info1, info2) == is_rhyme\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHYME_DEBUG_PRINT = False # 1, 2 or 3 for more debug info\n",
    "\n",
    "class RhymeType(Enum):\n",
    "    SUFFIX = 1\n",
    "    WORD = 2\n",
    "\n",
    "    \n",
    "class ITranslationModel(object):\n",
    "    # Our model which we trained now is accessed via this interface\n",
    "\n",
    "    def make_initial_state(self, lines):\n",
    "        # one state per line (though format is model-depending)\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_output_vocabulary(self):\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "\n",
    "class RhymeTranslator(object):\n",
    "    \n",
    "    def __init__(self, model: ITranslationModel, rupo_engine = None):\n",
    "        \n",
    "        self._model = model\n",
    "        self._out_voc = model.get_output_vocabulary()\n",
    "        \n",
    "        self._suffix_rhyme_tester = RuReversedSuffixRhymeTester()\n",
    "        self._word_rhyme_tester = RuReversedWordRhymeTester(rupo_engine = rupo_engine)\n",
    "    \n",
    "    \n",
    "    def _lines_to_model_lines(self, lines):\n",
    "        return list(map(get_reversed, lines))\n",
    "    \n",
    "    def _merge_tokens(self, line):\n",
    "        return line.replace('@@ ', '')\n",
    "    \n",
    "    def _model_tokens_to_model_line(self, output, eos_as_space):\n",
    "        [line] = self._out_voc.to_lines([output])\n",
    "        if eos_as_space and output[-1] == self._out_voc.eos_ix:\n",
    "            line += ' '\n",
    "        return self._merge_tokens(line)\n",
    "    \n",
    "    def _model_tokens_to_lines(self, outputs):\n",
    "        lines = self._out_voc.to_lines(outputs)\n",
    "        return [get_reversed(self._merge_tokens(l)) for l in lines]\n",
    "    \n",
    "    def _apply_softmax(self, logits, temperature):\n",
    "        \n",
    "        if temperature != 1:\n",
    "            if temperature < 1:\n",
    "                # Convert to 64-bit float to avoid overflows:\n",
    "                # Note: There will still be overflows for T < 0.03\n",
    "                logits = logits.astype(np.float64)\n",
    "            \n",
    "            logits /= temperature\n",
    "\n",
    "        np.exp(logits, out = logits)\n",
    "        logits /= logits.sum(axis = -1)[..., np.newaxis]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def translate_lines(self,\n",
    "                        lines,\n",
    "                        sample_temperature = 0, \n",
    "                        max_len = 100):\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        state = self._model.make_initial_state(model_lines)\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "            \n",
    "            if sample_temperature:\n",
    "                # Sample from softmax with temperature:\n",
    "                logits = self._apply_softmax(logits, sample_temperature)\n",
    "                next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "            else:\n",
    "                next_tokens = np.argmax(logits, axis = -1)\n",
    "            \n",
    "            outputs[:, t + 1] = next_tokens\n",
    "            finished |= next_tokens == self._out_voc.eos_ix\n",
    "            \n",
    "            if finished.sum() == len(lines):\n",
    "                break # Early exis if all lines finished\n",
    "                \n",
    "        return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _translate_lines_in_rhyme(self,\n",
    "                                  lines,\n",
    "                                  rhyme_tester,\n",
    "                                  sample_temperature,\n",
    "                                  max_len,\n",
    "                                  rhyme_test_counts,\n",
    "                                  max_total_rhyme_tests):\n",
    "        # rhyme_test_counts = (2, 3, 4) means that first token is variated 2 variants\n",
    "        # second is variated 3 variants and third is variated 4 variants\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        initial_state = self._model.make_initial_state(model_lines)\n",
    "        initial_states = [[data[i: i+1] for data in initial_state] for i in range(len(lines))]\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "        \n",
    "        # Rhyme states:\n",
    "        # * True - rhyme found,\n",
    "        # * False - there is no rhyme,\n",
    "        # * None - not sure yet\n",
    "        rhyme_state = None\n",
    "        \n",
    "        GenState = namedtuple('GenState', ['state', 'toks', 'prob', 'next_states'])\n",
    "        \n",
    "        gen_states = [GenState(state,\n",
    "                               toks = [self._out_voc.bos_ix],\n",
    "                               prob = 1,\n",
    "                               next_states = [None] * rhyme_test_counts[0]) for state in initial_states]\n",
    "        \n",
    "        def fill_next_states(gen_state, t):\n",
    "            \n",
    "            last_state = t == len(rhyme_test_counts)\n",
    "            if last_state:\n",
    "                assert gen_state.next_states is None\n",
    "                line_last_gen_states.append(gen_state)\n",
    "                return\n",
    "                \n",
    "            test_count = rhyme_test_counts[t]\n",
    "            assert test_count == len(gen_state.next_states)\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(gen_state.state, [gen_state.toks])\n",
    "            [probs] = self._apply_softmax(logits, temperature = 1) # TODO set temperature\n",
    "            \n",
    "            best_line_tokens = np.argpartition(probs, kth = -test_count, axis = -1)[-test_count:]\n",
    "            best_line_token_probs = probs[best_line_tokens]\n",
    "            \n",
    "            for i in range(test_count):\n",
    "                next_gen_state = GenState(state,\n",
    "                                          toks = gen_state.toks + [best_line_tokens[i]],\n",
    "                                          prob = gen_state.prob * best_line_token_probs[i],\n",
    "                                          next_states = [None] * rhyme_test_counts[t + 1]\n",
    "                                                        if t + 1 < len(rhyme_test_counts)\n",
    "                                                        else None)\n",
    "                gen_state.next_states[i] = next_gen_state\n",
    "                fill_next_states(next_gen_state, t + 1)\n",
    "            \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating {} x {} states... ***'.format(len(lines), rhyme_test_counts)) # DEBUG\n",
    "        last_gen_states = []\n",
    "        for gen_state in gen_states:\n",
    "            line_last_gen_states = []\n",
    "            fill_next_states(gen_state, t = 0)\n",
    "            last_gen_states.append(line_last_gen_states)\n",
    "            \n",
    "        # by this moment we have tree-structure (stored in last_gen_states) for each line\n",
    "            \n",
    "        assert [len(line_last_gen_states) == np.prod(rhyme_test_counts) for line_last_gen_states in last_gen_states]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 3:\n",
    "            for i, line_last_gen_states in enumerate(last_gen_states):\n",
    "                print('*** DEBUG: Line {} suffixes: ***'.format(i + 1)) # DEBUG\n",
    "                for line_last_gen_state in line_last_gen_states:\n",
    "                    suffix = get_reversed(self._model_tokens_to_model_line(line_last_gen_state.toks,\n",
    "                                                                           eos_as_space = True))\n",
    "                    print('*** DEBUG:  line {}: \"{}\" ***'.format(i + 1, suffix)) # DEBUG\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating state pairs... ***') # DEBUG\n",
    "        assert len(lines) == 2\n",
    "        last_gen_state_pairs = []\n",
    "        for line_1_last_gen_state in last_gen_states[0]:\n",
    "            for line_2_last_gen_state in last_gen_states[1]:\n",
    "                \n",
    "                pair = (line_1_last_gen_state.prob * line_2_last_gen_state.prob,\n",
    "                        line_1_last_gen_state,\n",
    "                        line_2_last_gen_state)\n",
    "                last_gen_state_pairs.append(pair)\n",
    "        # pair is actually a triple: prob, state1, state2\n",
    "                \n",
    "        last_gen_state_pairs.sort(key = lambda t: t[0], reverse = True)\n",
    "        if max_total_rhyme_tests:\n",
    "            last_gen_state_pairs = last_gen_state_pairs[:max_total_rhyme_tests]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Testing state pairs... ***') # DEBUG\n",
    "        for _, line_1_last_gen_state, line_2_last_gen_state in last_gen_state_pairs:\n",
    "            \n",
    "            state = [np.concatenate((a, b), axis = 0)\n",
    "                     for a, b in zip(line_1_last_gen_state.state, line_2_last_gen_state.state)]\n",
    "            \n",
    "            outputs[0, :len(rhyme_test_counts) + 1] = line_1_last_gen_state.toks\n",
    "            outputs[1, :len(rhyme_test_counts) + 1] = line_2_last_gen_state.toks\n",
    "            \n",
    "            rhyme_state = None\n",
    "            \n",
    "            def update_rhyme_state(t):\n",
    "                # it will say whether we have a rhyme currently\n",
    "                nonlocal rhyme_state\n",
    "                \n",
    "                line_1 = self._model_tokens_to_model_line(outputs[0, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                line_2 = self._model_tokens_to_model_line(outputs[1, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                info_1 = rhyme_tester.extract_rhyme_info(line_1)\n",
    "                info_2 = rhyme_tester.extract_rhyme_info(line_2)\n",
    "                rhyme_state = rhyme_tester.is_rhyme(info_1, info_2)\n",
    "            \n",
    "            update_rhyme_state(len(rhyme_test_counts))\n",
    "            if rhyme_state == False:\n",
    "                continue\n",
    "                \n",
    "            finished.fill(False)\n",
    "\n",
    "            # And now the same generation function like in our model but with checking rhymes\n",
    "            for t in range(len(rhyme_test_counts), max_len):\n",
    "\n",
    "                state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "\n",
    "                if sample_temperature:\n",
    "                    # Sample from softmax with temperature:\n",
    "                    logits = self._apply_softmax(logits, sample_temperature)\n",
    "                    next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "                else:\n",
    "                    next_tokens = np.argmax(logits, axis = -1)\n",
    "\n",
    "                outputs[:, t + 1] = next_tokens\n",
    "                finished |= next_tokens == self._out_voc.eos_ix\n",
    "                \n",
    "                if rhyme_state is None:\n",
    "                    update_rhyme_state(t)\n",
    "                    if rhyme_state == False:\n",
    "                        break\n",
    "\n",
    "                if finished.sum() == len(lines):\n",
    "                    break # Early exis if all lines finished\n",
    "\n",
    "            if rhyme_state != True:\n",
    "                continue\n",
    "            \n",
    "            if RHYME_DEBUG_PRINT:\n",
    "                print('*** DEBUG: Rhyme found! ***') # DEBUG\n",
    "            return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "        if RHYME_DEBUG_PRINT:\n",
    "            print('*** DEBUG: Failed to find rhyme. ***') # DEBUG\n",
    "        return self.translate_lines(lines,\n",
    "                                    sample_temperature,\n",
    "                                    max_len)\n",
    "    \n",
    "    \n",
    "    def translate_lines_with_rhyme(self,\n",
    "                                   lines,\n",
    "                                   rhyme_type = RhymeType.WORD,\n",
    "                                   sample_temperature = 0,\n",
    "                                   max_len = 100,\n",
    "                                   rhyme_test_counts = (10, 10),\n",
    "                                   max_total_rhyme_tests = 1000):\n",
    "        \n",
    "        if rhyme_type == RhymeType.SUFFIX:\n",
    "            rhyme_tester = self._suffix_rhyme_tester\n",
    "        elif rhyme_type == RhymeType.WORD:\n",
    "            rhyme_tester = self._word_rhyme_tester\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        translated = []\n",
    "        for pair_idx in range(len(lines) // 2):\n",
    "            \n",
    "            pair_lines = lines[pair_idx * 2 : (pair_idx + 1) * 2]\n",
    "            \n",
    "            translated += self._translate_lines_in_rhyme(pair_lines,\n",
    "                                                         rhyme_tester,\n",
    "                                                         sample_temperature,\n",
    "                                                         max_len,\n",
    "                                                         rhyme_test_counts,\n",
    "                                                         max_total_rhyme_tests)\n",
    "        \n",
    "        if len(lines) % 2 == 1:\n",
    "            translated.append(self.translate_lines(lines[-1:])[0])\n",
    "            \n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    \n",
    "    def __init__(self, name, hid_size, activ=tf.tanh,):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        self.name = name\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # YOUR CODE - create layer variables\n",
    "            #<YOUR CODE>\n",
    "            self.linear_e = L.Dense(hid_size)\n",
    "            self.linear_d = L.Dense(hid_size)\n",
    "            self.linear_out = L.Dense(1)\n",
    "\n",
    "    def __call__(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # Compute logits\n",
    "            #<...>\n",
    "            logits_seq = self.linear_out(self.activ(self.linear_e(enc) + \\\n",
    "                                                    self.linear_d(dec)[:, tf.newaxis, :]))\n",
    "            logits_seq = tf.squeeze(logits_seq, axis = -1)\n",
    "            \n",
    "            # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "            # You may need tf.where\n",
    "            #<...>\n",
    "            \n",
    "            logits_seq = tf.where(inp_mask, logits_seq, tf.fill(tf.shape(logits_seq),\n",
    "                                                                -np.inf))\n",
    "            \n",
    "            # Compute attention probabilities (softmax)\n",
    "            probs = tf.nn.softmax(logits_seq) # <...>\n",
    "            \n",
    "            # Compute attention response using enc and probs\n",
    "            attn = tf.reduce_sum(probs[..., tf.newaxis] * enc, axis = 1) # <...>\n",
    "            \n",
    "            return attn, probs\n",
    "        \n",
    "class AttentiveModel(ITranslationModel):\n",
    "    \n",
    "    def __init__(self, filename, name = None, inp_voc = None, out_voc = None,\n",
    "                 emb_size = None, hid_size = None):\n",
    "        \n",
    "        if filename is None:\n",
    "            self.initialize(name, inp_voc, out_voc,\n",
    "                            emb_size, hid_size) #, attn_size)\n",
    "        else:\n",
    "            self.load(filename)\n",
    "    \n",
    "    \n",
    "    def initialize(self, name, inp_voc, out_voc,\n",
    "                   emb_size, hid_size): #, attn_size):\n",
    "        \n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        #self.attn_size = attn_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # YOUR CODE - define model layers\n",
    "            \n",
    "            # <...>\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc_lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            self.enc_lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            #self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.dense = L.Dense(hid_size)\n",
    "            self.activ = tf.tanh\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            self.attention = AttentionLayer(name = 'attention',\n",
    "                                            #enc_size = None, # FIXME: Unused\n",
    "                                            #dec_size = None, # FIXME: Unused\n",
    "                                            #hid_size = attn_size)\n",
    "                                            hid_size = 2 * self.hid_size)\n",
    "            \n",
    "            # END OF YOUR CODE\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "            self.next_softmax = tf.nn.softmax(self.next_logits)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        \n",
    "        # Call to 'K.get_session()' runs variable initializes for\n",
    "        # all variables including ones initialized using\n",
    "        # 'tf.global_variables_initializer()' (at least for Keras\n",
    "        # 2.0.5) thus it have to be called once here or model weights\n",
    "        # will be rewritten after training e.g. when 'get_weights' is\n",
    "        # called.\n",
    "        K.get_session()\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        # <YOUR CODE>\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_mask = infer_mask(inp, self.inp_voc.eos_ix, dtype = tf.bool)\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            #enc_seq, enc_last = tf.nn.dynamic_rnn(self.enc0,\n",
    "            #                                      inp_emb,\n",
    "            #                                      sequence_length = inp_lengths,\n",
    "            #                                      dtype = inp_emb.dtype)\n",
    "            ((enc_seq_fw,\n",
    "              enc_seq_bw),\n",
    "             (enc_last_fw,\n",
    "              enc_last_bw)) = tf.nn.bidirectional_dynamic_rnn(self.enc_lstm_fw_cell,\n",
    "                                                              self.enc_lstm_bw_cell,\n",
    "                                                              inp_emb,\n",
    "                                                              sequence_length = inp_lengths,\n",
    "                                                              dtype = inp_emb.dtype)\n",
    "        enc_seq = tf.concat((enc_seq_fw, enc_seq_bw), axis = -1)\n",
    "        dec_start = self.dec_start(enc_last_fw)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        #first_attn_probas = <...>\n",
    "        _, first_attn_probas = self.attention(enc_seq, dec_start, inp_mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        #first_state = [<...>, first_attn_probas]\n",
    "        first_state = [dec_start, enc_seq, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order\n",
    "        # that you've packed in encode\n",
    "        #[<...>, prev_attn_probas] = prev_state\n",
    "        [prev_dec, enc_seq, inp_mask, prev_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and attn probas given previous decoder state\n",
    "        # * use prev token embedding and attn response to update decoder states\n",
    "        # * (concatenate and feed into decoder cell)\n",
    "        # * predict logits\n",
    "        \n",
    "        # <APPLY_ATTENTION>\n",
    "        next_attn_response, next_attn_probas = self.attention(enc_seq, prev_dec, inp_mask)\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        dec_inputs = tf.concat([prev_emb, next_attn_response], axis = 1)\n",
    "        with tf.variable_scope('dec0'):\n",
    "            new_dec_out, new_dec_state = self.dec0(dec_inputs, prev_dec)\n",
    "        output_logits = self.logits(self.activ(self.dense(new_dec_out)))\n",
    "        #output_logits = self.logits(self.activ(new_dec_out))\n",
    "        \n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        #next_state = [<...>, next_attn_probas]\n",
    "        next_state = [new_dec_state, enc_seq, inp_mask, next_attn_probas]\n",
    "        return next_state, output_logits\n",
    "\n",
    "    \n",
    "    def compute_logits(self, inp, out, **flags):\n",
    "        \n",
    "        batch_size = tf.shape(inp)[0]\n",
    "\n",
    "        # Encode inp, get initial state\n",
    "        first_state = self.encode(inp) # <YOUR CODE HERE>\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        first_logits = tf.log(tf.one_hot(tf.fill([batch_size], self.out_voc.bos_ix),\n",
    "                                         len(self.out_voc)) + 1e-30)\n",
    "\n",
    "        # Decode step\n",
    "        def step(prev_state, y_prev):\n",
    "            # Given previous state, obtain next state and next token logits\n",
    "            # <YOUR CODE>\n",
    "            next_dec_state, next_logits = self.decode(prev_state, y_prev)\n",
    "            return next_dec_state, next_logits # <...>\n",
    "\n",
    "        # You can now use tf.scan to run step several times.\n",
    "        # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "        # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        out = tf.scan(lambda a, y: step(a[0], y),\n",
    "                      elems = tf.transpose(out)[:-1],\n",
    "                      initializer = (first_state, first_logits))\n",
    "\n",
    "\n",
    "        # FIXME remove?\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        logits_seq = out[1] # <YOUR CODE>\n",
    "\n",
    "        # prepend first_logits to logits_seq\n",
    "        logits_seq = tf.concat((first_logits[tf.newaxis], logits_seq), axis = 0) #<...>\n",
    "\n",
    "        # Make sure you convert logits_seq from\n",
    "        # [time, batch, voc_size] to [batch, time, voc_size]\n",
    "        logits_seq = tf.transpose(logits_seq, perm = [1, 0, 2]) #<...>\n",
    "\n",
    "        return logits_seq\n",
    "\n",
    "    def compute_loss(self, inp, out, **flags):\n",
    "        \n",
    "        mask = infer_mask(out, out_voc.eos_ix)    \n",
    "        logits_seq = self.compute_logits(inp, out, **flags)\n",
    "\n",
    "        # Compute loss as per instructions above\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        prob_seq = tf.nn.softmax(logits_seq)\n",
    "        out_one_hot = tf.one_hot(out, len(self.out_voc))\n",
    "\n",
    "        prob_seq_masked = tf.boolean_mask(prob_seq, mask)\n",
    "        out_one_hot_masked = tf.boolean_mask(out_one_hot, mask)\n",
    "        prob_seq_out = tf.boolean_mask(prob_seq_masked, out_one_hot_masked)\n",
    "        loss = tf.reduce_mean(-tf.log(prob_seq_out))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def make_initial_state(self, inp_lines):\n",
    "        return sess.run(self.initial_state, {self.inp: self.inp_voc.to_matrix(inp_lines)})\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        return sess.run([self.next_state, self.next_logits],\n",
    "                        {**dict(zip(self.prev_state, state)),\n",
    "                         self.prev_tokens: [out_i[-1] for out_i in outputs]})\n",
    "                         \n",
    "    def get_output_vocabulary(self):\n",
    "        return self.out_voc\n",
    "    \n",
    "    \n",
    "    def translate_lines(self, inp_lines, max_len=100):\n",
    "        \"\"\"\n",
    "        Translates a list of lines by greedily selecting most likely next token at each step\n",
    "        :returns: a list of output lines, a sequence of model states at each step\n",
    "        \"\"\"\n",
    "        state = self.make_initial_state(inp_lines)\n",
    "        outputs = [[self.out_voc.bos_ix] for _ in range(len(inp_lines))]\n",
    "        all_states = [state]\n",
    "        finished = [False] * len(inp_lines)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            state, logits = self.get_next_state_and_logits(state, outputs)\n",
    "            next_tokens = np.argmax(logits, axis=-1)\n",
    "            all_states.append(state)\n",
    "            for i in range(len(next_tokens)):\n",
    "                outputs[i].append(next_tokens[i])\n",
    "                finished[i] |= next_tokens[i] == self.out_voc.eos_ix\n",
    "        return self.out_voc.to_lines(outputs), all_states\n",
    "    \n",
    "    def dump(self, filename):\n",
    "        \n",
    "        values = {'name': self.name,\n",
    "                  'inp_voc': self.inp_voc,\n",
    "                  'out_voc': self.out_voc,\n",
    "                  'emb_size': self.emb_size,\n",
    "                  'hid_size': self.hid_size,\n",
    "                  #'attn_size': self.attn_size,\n",
    "                  'emb_inp_weights': self.emb_inp.get_weights(),\n",
    "                  'emb_out_weights': self.emb_out.get_weights(),\n",
    "                  #'enc0_weights': self.enc0.get_weights(),\n",
    "                  'enc_lstm_fw_cell_weights': self.enc_lstm_fw_cell.get_weights(),\n",
    "                  'enc_lstm_bw_cell_weights': self.enc_lstm_bw_cell.get_weights(),\n",
    "                  'dec0_weights': self.dec0.get_weights(),\n",
    "                  'dec_start_weights': self.dec_start.get_weights(),\n",
    "                  'dense_weights': self.dense.get_weights(),\n",
    "                  'logits_weights': self.logits.get_weights(),\n",
    "                  'attn__linear_e_weights': self.attention.linear_e.get_weights(),\n",
    "                  'attn__linear_d_weights': self.attention.linear_d.get_weights(),\n",
    "                  'attn__linear_out_weights': self.attention.linear_out.get_weights()}\n",
    "        pickle.dump(values, open(filename, 'wb'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            values = pickle.load(f)\n",
    "        self.initialize(values['name'], values['inp_voc'], values['out_voc'],\n",
    "                        values['emb_size'], values['hid_size']) #, values['attn_size'])\n",
    "        self.emb_inp.set_weights(values['emb_inp_weights'])\n",
    "        self.emb_out.set_weights(values['emb_out_weights'])\n",
    "        #self.enc0.set_weights(values['enc0_weights'])\n",
    "        self.enc_lstm_fw_cell.set_weights(values['enc_lstm_fw_cell_weights'])\n",
    "        self.enc_lstm_bw_cell.set_weights(values['enc_lstm_bw_cell_weights'])\n",
    "        self.dec0.set_weights(values['dec0_weights'])\n",
    "        self.dec_start.set_weights(values['dec_start_weights'])\n",
    "        self.dense.set_weights(values['dense_weights'])\n",
    "        self.logits.set_weights(values['logits_weights'])\n",
    "        self.attention.linear_e.set_weights(values['attn__linear_e_weights'])\n",
    "        self.attention.linear_d.set_weights(values['attn__linear_d_weights'])\n",
    "        self.attention.linear_out.set_weights(values['attn__linear_out_weights'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f93c3746630>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f93c3746630>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f93c3746550>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f93c3746550>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "if 'model_loaded' in globals():\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Need to also recreate Rupo as it uses TensorFlow:\n",
    "    global_rupo_engine = make_rupo_engine()\n",
    "\n",
    "model_loaded = AttentiveModel(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = RhymeTranslator(model_loaded, global_rupo_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Английский голос.\n",
      "прямо у меня за окном.\n",
      "Несмотря на то лето, как засчитывает все вещи,\n",
      "Горячий воздух.\n",
      "\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Английский голос.\n",
      "прямо у меня за окном.\n",
      "Несмотря на то лето, как засчитывает все вещи,\n",
      "ветер.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "assert global_rupo_engine.is_rhyme('вещи', 'ветер') # This is how Rupo thinks\n",
    "\n",
    "lines = '''Your voice\n",
    "Called me outside the window\n",
    "Of uncontested summer all things raise\n",
    "seamless air'''.split('\\n')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мы должны сделать свои драгоценные драгоценности в \"Прости\".\n",
      "- для свежей красоты для \"Пустяки\",\n",
      "Несмотря на их, и для птиц. Девочки из \"я\".\n",
      "Несмотря на себя, пожирающий для себя для себя - \"я\".\n",
      "Весна из Герцогина и королевская пехота: \"Ну-ка\".\n",
      "Ты, бл*, для меня, - это твоя \"волна!\".\n",
      "\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Мы должны сделать свои драгоценные драгоценности в \"Прости\".\n",
      "Чтобы раствориться об простыней для красоты для непристойности,\n",
      "Несмотря на их, и для птиц. Девочки из \"я\".\n",
      "Несмотря на себя, пожирающий для себя для себя - \"я\".\n",
      "Железные поляны из ресницы и красоты, урана.\n",
      "Ты, бл*, для меня, - это твоя \"волна!\".\n",
      "\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Я создана для того, чтобы попасть в райские драгоценности, чтобы разобраться с евреями.\n",
      "- для свежей красоты для известности...\n",
      "Они за их восхищённые и Милые цветы из плотины.\n",
      "- очередные плотины...\n",
      "Весна из Герцогина и королевская пехота: \"Ну-ка\".\n",
      "Ты, бл*, для меня, - это твоя \"волна!\".\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Torches are made to light, jewels to wear,\n",
    "Dainties to taste, fresh beauty for the use,\n",
    "Herbs for their smell, and sappy plants to bear;\n",
    "Things growing to themselves are growth’s abuse,\n",
    "Seeds spring from seeds, and beauty breedeth beauty;\n",
    "Thou wast begot; to get it is thy duty.'''.split('\\n')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он говорит о том, что этот ветер прорывается сквозь ночь,\n",
      "Ладно, мы уходим прочь.\n",
      "Крупное животное проходил через улицу,\n",
      "Нет, они так устали.\n"
     ]
    }
   ],
   "source": [
    "# Model internal translation function:\n",
    "lines = list(map(get_reversed,\n",
    "                 ['Let this wind blow through the night',\n",
    "                  'And we are going away',\n",
    "                  'The animal hasn\\'t crossed the street',\n",
    "                  'Because it was too tired']))\n",
    "translated = model_loaded.translate_lines(lines)[0]\n",
    "print('\\n'.join([get_reversed(line) for line in translated]).replace(' @@', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и был уверен.\n",
      "\"Человек\", я знаю. Грязное словечко.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Load up on guns and bring your friends\n",
    "It's fun to lose and to pretend\n",
    "She's over bored and self assured\n",
    "Oh no, I know a dirty word'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Ваш сын вернулся к отцу.\n",
      "Не спрашивай меня, и все крошки.\n",
      "- это хорошо.\n",
      "Это неправильно.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он в том, что с сыном пришёл его отец.\n",
      "и спросила у детей.\n",
      "- это плохо.\n",
      "Это не плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он вернулся с сыном с отцом.\n",
      "Попросила меня с ребёнком.\n",
      "- прекрасно.\n",
      "Это неправильно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Крошка сын к отцу пришел,\n",
    "и спросила кроха:\n",
    "— Что такое хорошо\n",
    "и что такое плохо?'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''Baby son to his father came\n",
    "and asked crumbs\n",
    "What is good\n",
    "what's wrong'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Ответ от этих отцов, вот это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет друзей,\n",
      "Ох, слушать, детей,\n",
      "Они смотрят на это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Послушай, мальчиков,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Он врывается в грязь и взволнован.\n",
      "Это эта коварная рубашка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Он врывается в грязь и взволнован.\n",
      "Это такая грязная футболка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Наверное, в грязи, и я была взволнована.\n",
      "Это кожаная рубашкана,\n",
      "Это то, что здесь сказано:\n",
      "Очевидно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Этот в грязь полез и рад.\n",
    "что грязна рубаха.\n",
    "Про такого говорят:\n",
    "он плохой, неряха.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''This in the dirt and got excited\n",
    "that dirty shirt\n",
    "About this say\n",
    "he's bad sloppy'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
