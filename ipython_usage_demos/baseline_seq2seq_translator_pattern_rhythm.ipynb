{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/isprovilkov/rupo\")\n",
    "import pickle\n",
    "import rupo.api\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple, Optional, Tuple\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from keras import backend as K\n",
    "from utils import infer_length, infer_mask\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_ROOT = '/srv/hd7/data/aklyopova/models/model_translator_attn_reversed_amalgama_subtitles_09_03_2019/'\n",
    "#MODEL_PATH = MODEL_ROOT + 'model_r2_14_03_2019.pkl'\n",
    "MODEL_PATH = '/home/isprovilkov/Poems/model_translator_attn_reversed_amalgama_subtitles_09_03_2019_550001_iters_fine_tuned_amalgama_300002_iters.pkl'\n",
    "\n",
    "RUPO_DATA_ROOT = '/home/isprovilkov/Poems/'\n",
    "RUPO_STRESS_MODEL_PATH   = '/home/isprovilkov/rupo/rupo/data/stress_models/stress_ru.h5'\n",
    "RUPO_ZALYZNIAK_DICT_PATH = RUPO_DATA_ROOT + 'zaliznyak.txt'\n",
    "del RUPO_DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reversed(line):\n",
    "    return ''.join(reversed(line))\n",
    "\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations\n",
    "        given inp and reference out \"\"\"\n",
    "    translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "    # Note: if you experience out-of-memory error,\n",
    "    # split input lines into batches and translate separately\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100\n",
    "\n",
    "def compute_bleu_large(model, inp_lines, out_lines):\n",
    "    batch_size = 256\n",
    "    result = 0.0\n",
    "    for i in range(0, inp_lines.shape[0], batch_size):\n",
    "        current_bleu = compute_bleu(model,\n",
    "                                    inp_lines[i : i + batch_size],\n",
    "                                    out_lines[i : i + batch_size])\n",
    "        current_bleu *= min(i + batch_size, inp_lines.shape[0]) - i\n",
    "        result += current_bleu\n",
    "    result /= inp_lines.shape[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterType(Enum):\n",
    "    NOT_LETTER = 0\n",
    "    VOWEL = 1\n",
    "    CONSONANT = 2\n",
    "    SIGN = 3\n",
    "\n",
    "class RuAlphabetInfo(object):\n",
    "    \n",
    "    _ru_letters = set(''.join([chr(n) for n in range(ord('а'), ord('я') + 1)]) + 'ё')\n",
    "    assert len(_ru_letters) == 33\n",
    "    _ru_vowels = set('аеёийоуыэюя')\n",
    "    _ru_consonants = set('бвгджзклмнпрстфхцчшщ')\n",
    "    _ru_signs = set('ъь')\n",
    "\n",
    "    assert _ru_vowels & _ru_consonants == set()\n",
    "    assert _ru_vowels | _ru_consonants | _ru_signs == _ru_letters\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ru_letter_type(ch: str) -> LetterType:\n",
    "\n",
    "        if ch in RuAlphabetInfo._ru_vowels:\n",
    "            return LetterType.VOWEL\n",
    "        elif ch in RuAlphabetInfo._ru_consonants:\n",
    "            return LetterType.CONSONANT\n",
    "        elif ch in RuAlphabetInfo._ru_signs:\n",
    "            return LetterType.SIGN\n",
    "        else:\n",
    "            return LetterType.NOT_LETTER\n",
    "    \n",
    "    @staticmethod\n",
    "    def lower_and_strip_left_non_letters(line: str) -> str:\n",
    "        \n",
    "        line = line.lower()\n",
    "        \n",
    "        # Skip everything before first letter:\n",
    "        while len(line) > 0 and line[0] not in RuAlphabetInfo._ru_letters:\n",
    "            line = line[1:]\n",
    "        \n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RhymeInfo(NamedTuple):\n",
    "#    text: str\n",
    "#    finished: bool\n",
    "\n",
    "# Old-style for Python 3.5:\n",
    "RhymeInfo = NamedTuple('RhymeInfo',\n",
    "                       [('text', str), # text that will be used for testing whether rhyme is present\n",
    "                        ('finished', bool) # is text complete\n",
    "                       ])\n",
    "        \n",
    "class IRhymeTester(object):\n",
    "\n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        # Returns True - rhyme present, False - absent, None - don't know\n",
    "        raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedSuffixRhymeTester(IRhymeTester):\n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # input: line is REVERSED string\n",
    "        # We iterate all vowels from the beginning of line until first consonant\n",
    "        # Or all consonants until first vowel\n",
    "        # SIGNs are ignored but inserted to output\n",
    "\n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "            \n",
    "        finished = False\n",
    "        prefix_len = 0\n",
    "        prev_ch_type = None\n",
    "\n",
    "        for ch in line:\n",
    "            \n",
    "            ch_type = RuAlphabetInfo.get_ru_letter_type(ch)\n",
    "            \n",
    "            if ch_type == LetterType.NOT_LETTER:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            prefix_len += 1\n",
    "\n",
    "            if ch_type != LetterType.SIGN:\n",
    "                if prev_ch_type is None:\n",
    "                    prev_ch_type = ch_type\n",
    "                elif prev_ch_type != ch_type:\n",
    "                    finished = True\n",
    "                    break\n",
    "\n",
    "        return RhymeInfo(text = line[:prefix_len],\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            l = min(len(info1.text), len(info2.text))\n",
    "            if info1.text[:l] != info2.text[:l]:\n",
    "                return False\n",
    "            return None\n",
    "        \n",
    "        return info1.text == info2.text\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedSuffixRhymeTester()\n",
    "    for line, suffix, finished in [('пижама', 'ма', True),\n",
    "                                   ('обученный', 'ный', True),\n",
    "                                   ('Ихтиандр', 'андр', True),\n",
    "                                   ('КНДР', 'кндр', False),\n",
    "                                   ('махать', 'ать', True)]:\n",
    "        for l in (line, line + '!', ' ' + line + ', '):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == get_reversed(suffix)\n",
    "            assert info.finished == (finished or l[0] == ' ')\n",
    "            assert tester.is_rhyme(info, info) == (True if info.finished else None)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isprovilkov/p2p/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "def make_rupo_engine():\n",
    "    rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "    rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                     zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "    return rupo_engine\n",
    "\n",
    "global_rupo_engine = make_rupo_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedWordRhymeTester(IRhymeTester):\n",
    "    \n",
    "    _WORD_INTERNAL_CHARS = set('-.')\n",
    "    \n",
    "    \n",
    "    def __init__(self, rupo_engine = None):\n",
    "        \n",
    "        if rupo_engine is None:\n",
    "            self._rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "            self._rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                                   zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "        else:\n",
    "            # Assume loaded engine:\n",
    "            assert rupo_engine.language == 'ru'\n",
    "            self._rupo_engine = rupo_engine\n",
    "    \n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # line is REVERSED string\n",
    "        # Here we skip everything until first letter\n",
    "        # return: it TRIES to return complete word (returns part in case if line contains only part)\n",
    "        \n",
    "        \n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "        \n",
    "        is_internal_char = lambda ch: ch in RuReversedWordRhymeTester._WORD_INTERNAL_CHARS\n",
    "            \n",
    "        finished = False\n",
    "        word_len = 0\n",
    "        \n",
    "        for ch in line:\n",
    "            \n",
    "            is_word_char = is_internal_char(ch) or \\\n",
    "                           RuAlphabetInfo.get_ru_letter_type(ch) != LetterType.NOT_LETTER\n",
    "            \n",
    "            if not is_word_char:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            word_len += 1\n",
    "        \n",
    "        while word_len > 0 and is_internal_char(line[word_len - 1]):\n",
    "            word_len -= 1\n",
    "\n",
    "        return RhymeInfo(text = get_reversed(line[:word_len]),\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            return None\n",
    "        \n",
    "        return self._rupo_engine.is_rhyme(info1.text, info2.text)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedWordRhymeTester(global_rupo_engine)\n",
    "    augment_line = lambda l: (l, l + '!', ' ' + l + ', ', 'Мы и ' + l)\n",
    "    \n",
    "    for line, word, finished in [('серая корова', 'корова', True),\n",
    "                                 ('молока, много', 'много', True),\n",
    "                                 ('Ихтиандр', 'ихтиандр', False),\n",
    "                                 ('КНДР', 'кндр', False),\n",
    "                                 ('аб-вг', 'аб-вг', False),\n",
    "                                 ('.-аб.вг.-', 'аб.вг', False)]:\n",
    "        for l in augment_line(line):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == word\n",
    "            assert info.finished == (finished or not l.startswith(line))\n",
    "            any_vowels = any([RuAlphabetInfo.get_ru_letter_type(ch) == LetterType.VOWEL for ch in word])\n",
    "            assert tester.is_rhyme(info, info) == (any_vowels if info.finished else None)\n",
    "    \n",
    "    for line1, line2, is_rhyme in [('серая корова', 'не очень здорова', True),\n",
    "                                   ('молока много', 'не мало', False),\n",
    "                                   ('и играть', 'и скакать', False),\n",
    "                                   ('и играть', 'не играть', True)]:\n",
    "        for l1 in augment_line(line1):\n",
    "            for l2 in augment_line(line2):\n",
    "                info1 = tester.extract_rhyme_info(get_reversed(l1))\n",
    "                info2 = tester.extract_rhyme_info(get_reversed(l2))\n",
    "                assert tester.is_rhyme(info1, info2) == is_rhyme\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHYME_DEBUG_PRINT = False # 1, 2 or 3 for more debug info\n",
    "\n",
    "class RhymeType(Enum):\n",
    "    SUFFIX = 1\n",
    "    WORD = 2\n",
    "\n",
    "    \n",
    "class ITranslationModel(object):\n",
    "    # Our model which we trained now is accessed via this interface\n",
    "\n",
    "    def make_initial_state(self, lines):\n",
    "        # one state per line (though format is model-depending)\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_output_vocabulary(self):\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "\n",
    "class RhymeTranslator(object):\n",
    "    \n",
    "    def __init__(self, model: ITranslationModel, rupo_engine = None):\n",
    "        \n",
    "        self._model = model\n",
    "        self._out_voc = model.get_output_vocabulary()\n",
    "        \n",
    "        self._suffix_rhyme_tester = RuReversedSuffixRhymeTester()\n",
    "        self._word_rhyme_tester = RuReversedWordRhymeTester(rupo_engine = rupo_engine)\n",
    "    \n",
    "    \n",
    "    def _lines_to_model_lines(self, lines):\n",
    "        return list(map(get_reversed, lines))\n",
    "    \n",
    "    def _merge_tokens(self, line):\n",
    "        return line.replace('@@ ', '')\n",
    "    \n",
    "    def _model_tokens_to_model_line(self, output, eos_as_space):\n",
    "        [line] = self._out_voc.to_lines([output])\n",
    "        if eos_as_space and output[-1] == self._out_voc.eos_ix:\n",
    "            line += ' '\n",
    "        return self._merge_tokens(line)\n",
    "    \n",
    "    def _model_tokens_to_lines(self, outputs):\n",
    "        lines = self._out_voc.to_lines(outputs)\n",
    "        return [get_reversed(self._merge_tokens(l)) for l in lines]\n",
    "    \n",
    "    def _apply_softmax(self, logits, temperature):\n",
    "        \n",
    "        if temperature != 1:\n",
    "            if temperature < 1:\n",
    "                # Convert to 64-bit float to avoid overflows:\n",
    "                # Note: There will still be overflows for T < 0.03\n",
    "                logits = logits.astype(np.float64)\n",
    "            \n",
    "            logits /= temperature\n",
    "\n",
    "        np.exp(logits, out = logits)\n",
    "        logits /= logits.sum(axis = -1)[..., np.newaxis]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def translate_lines(self,\n",
    "                        lines,\n",
    "                        sample_temperature = 0, \n",
    "                        max_len = 100):\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        state = self._model.make_initial_state(model_lines)\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "            \n",
    "            if sample_temperature:\n",
    "                # Sample from softmax with temperature:\n",
    "                logits = self._apply_softmax(logits, sample_temperature)\n",
    "                next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "            else:\n",
    "                next_tokens = np.argmax(logits, axis = -1)\n",
    "            \n",
    "            outputs[:, t + 1] = next_tokens\n",
    "            finished |= next_tokens == self._out_voc.eos_ix\n",
    "            \n",
    "            if finished.sum() == len(lines):\n",
    "                break # Early exis if all lines finished\n",
    "                \n",
    "        return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _translate_lines_in_rhyme(self,\n",
    "                                  lines,\n",
    "                                  rhyme_tester,\n",
    "                                  sample_temperature,\n",
    "                                  max_len,\n",
    "                                  rhyme_test_counts,\n",
    "                                  max_total_rhyme_tests):\n",
    "        # rhyme_test_counts = (2, 3, 4) means that first token is variated 2 variants\n",
    "        # second is variated 3 variants and third is variated 4 variants\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        initial_state = self._model.make_initial_state(model_lines)\n",
    "        initial_states = [[data[i: i+1] for data in initial_state] for i in range(len(lines))]\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "        \n",
    "        # Rhyme states:\n",
    "        # * True - rhyme found,\n",
    "        # * False - there is no rhyme,\n",
    "        # * None - not sure yet\n",
    "        rhyme_state = None\n",
    "        \n",
    "        GenState = namedtuple('GenState', ['state', 'toks', 'prob', 'next_states'])\n",
    "        \n",
    "        gen_states = [GenState(state,\n",
    "                               toks = [self._out_voc.bos_ix],\n",
    "                               prob = 1,\n",
    "                               next_states = [None] * rhyme_test_counts[0]) for state in initial_states]\n",
    "        \n",
    "        def fill_next_states(gen_state, t):\n",
    "            \n",
    "            last_state = t == len(rhyme_test_counts)\n",
    "            if last_state:\n",
    "                assert gen_state.next_states is None\n",
    "                line_last_gen_states.append(gen_state)\n",
    "                return\n",
    "                \n",
    "            test_count = rhyme_test_counts[t]\n",
    "            assert test_count == len(gen_state.next_states)\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(gen_state.state, [gen_state.toks])\n",
    "            [probs] = self._apply_softmax(logits, temperature = 1) # TODO set temperature\n",
    "            \n",
    "            best_line_tokens = np.argpartition(probs, kth = -test_count, axis = -1)[-test_count:]\n",
    "            best_line_token_probs = probs[best_line_tokens]\n",
    "            \n",
    "            for i in range(test_count):\n",
    "                next_gen_state = GenState(state,\n",
    "                                          toks = gen_state.toks + [best_line_tokens[i]],\n",
    "                                          prob = gen_state.prob * best_line_token_probs[i],\n",
    "                                          next_states = [None] * rhyme_test_counts[t + 1]\n",
    "                                                        if t + 1 < len(rhyme_test_counts)\n",
    "                                                        else None)\n",
    "                gen_state.next_states[i] = next_gen_state\n",
    "                fill_next_states(next_gen_state, t + 1)\n",
    "            \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating {} x {} states... ***'.format(len(lines), rhyme_test_counts)) # DEBUG\n",
    "        last_gen_states = []\n",
    "        for gen_state in gen_states:\n",
    "            line_last_gen_states = []\n",
    "            fill_next_states(gen_state, t = 0)\n",
    "            last_gen_states.append(line_last_gen_states)\n",
    "            \n",
    "        # by this moment we have tree-structure (stored in last_gen_states) for each line\n",
    "            \n",
    "        assert [len(line_last_gen_states) == np.prod(rhyme_test_counts) for line_last_gen_states in last_gen_states]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 3:\n",
    "            for i, line_last_gen_states in enumerate(last_gen_states):\n",
    "                print('*** DEBUG: Line {} suffixes: ***'.format(i + 1)) # DEBUG\n",
    "                for line_last_gen_state in line_last_gen_states:\n",
    "                    suffix = get_reversed(self._model_tokens_to_model_line(line_last_gen_state.toks,\n",
    "                                                                           eos_as_space = True))\n",
    "                    print('*** DEBUG:  line {}: \"{}\" ***'.format(i + 1, suffix)) # DEBUG\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating state pairs... ***') # DEBUG\n",
    "        assert len(lines) == 2\n",
    "        last_gen_state_pairs = []\n",
    "        for line_1_last_gen_state in last_gen_states[0]:\n",
    "            for line_2_last_gen_state in last_gen_states[1]:\n",
    "                \n",
    "                pair = (line_1_last_gen_state.prob * line_2_last_gen_state.prob,\n",
    "                        line_1_last_gen_state,\n",
    "                        line_2_last_gen_state)\n",
    "                last_gen_state_pairs.append(pair)\n",
    "        # pair is actually a triple: prob, state1, state2\n",
    "                \n",
    "        last_gen_state_pairs.sort(key = lambda t: t[0], reverse = True)\n",
    "        if max_total_rhyme_tests:\n",
    "            last_gen_state_pairs = last_gen_state_pairs[:max_total_rhyme_tests]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Testing state pairs... ***') # DEBUG\n",
    "        for _, line_1_last_gen_state, line_2_last_gen_state in last_gen_state_pairs:\n",
    "            \n",
    "            state = [np.concatenate((a, b), axis = 0)\n",
    "                     for a, b in zip(line_1_last_gen_state.state, line_2_last_gen_state.state)]\n",
    "            \n",
    "            outputs[0, :len(rhyme_test_counts) + 1] = line_1_last_gen_state.toks\n",
    "            outputs[1, :len(rhyme_test_counts) + 1] = line_2_last_gen_state.toks\n",
    "            \n",
    "            rhyme_state = None\n",
    "            \n",
    "            def update_rhyme_state(t):\n",
    "                # it will say whether we have a rhyme currently\n",
    "                nonlocal rhyme_state\n",
    "                \n",
    "                line_1 = self._model_tokens_to_model_line(outputs[0, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                line_2 = self._model_tokens_to_model_line(outputs[1, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                info_1 = rhyme_tester.extract_rhyme_info(line_1)\n",
    "                info_2 = rhyme_tester.extract_rhyme_info(line_2)\n",
    "                rhyme_state = rhyme_tester.is_rhyme(info_1, info_2)\n",
    "            \n",
    "            update_rhyme_state(len(rhyme_test_counts))\n",
    "            if rhyme_state == False:\n",
    "                continue\n",
    "                \n",
    "            finished.fill(False)\n",
    "\n",
    "            # And now the same generation function like in our model but with checking rhymes\n",
    "            for t in range(len(rhyme_test_counts), max_len):\n",
    "\n",
    "                state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "\n",
    "                if sample_temperature:\n",
    "                    # Sample from softmax with temperature:\n",
    "                    logits = self._apply_softmax(logits, sample_temperature)\n",
    "                    next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "                else:\n",
    "                    next_tokens = np.argmax(logits, axis = -1)\n",
    "\n",
    "                outputs[:, t + 1] = next_tokens\n",
    "                finished |= next_tokens == self._out_voc.eos_ix\n",
    "                \n",
    "                if rhyme_state is None:\n",
    "                    update_rhyme_state(t)\n",
    "                    if rhyme_state == False:\n",
    "                        break\n",
    "\n",
    "                if finished.sum() == len(lines):\n",
    "                    break # Early exis if all lines finished\n",
    "\n",
    "            if rhyme_state != True:\n",
    "                continue\n",
    "            \n",
    "            if RHYME_DEBUG_PRINT:\n",
    "                print('*** DEBUG: Rhyme found! ***') # DEBUG\n",
    "            return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "        if RHYME_DEBUG_PRINT:\n",
    "            print('*** DEBUG: Failed to find rhyme. ***') # DEBUG\n",
    "        return self.translate_lines(lines,\n",
    "                                    sample_temperature,\n",
    "                                    max_len)\n",
    "    \n",
    "    \n",
    "    def translate_lines_with_rhyme(self,\n",
    "                                   lines,\n",
    "                                   rhyme_type = RhymeType.WORD,\n",
    "                                   sample_temperature = 0,\n",
    "                                   max_len = 100,\n",
    "                                   rhyme_test_counts = (10, 10),\n",
    "                                   max_total_rhyme_tests = 1000):\n",
    "        \n",
    "        if rhyme_type == RhymeType.SUFFIX:\n",
    "            rhyme_tester = self._suffix_rhyme_tester\n",
    "        elif rhyme_type == RhymeType.WORD:\n",
    "            rhyme_tester = self._word_rhyme_tester\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        translated = []\n",
    "        for pair_idx in range(len(lines) // 2):\n",
    "            \n",
    "            pair_lines = lines[pair_idx * 2 : (pair_idx + 1) * 2]\n",
    "            \n",
    "            translated += self._translate_lines_in_rhyme(pair_lines,\n",
    "                                                         rhyme_tester,\n",
    "                                                         sample_temperature,\n",
    "                                                         max_len,\n",
    "                                                         rhyme_test_counts,\n",
    "                                                         max_total_rhyme_tests)\n",
    "        \n",
    "        if len(lines) % 2 == 1:\n",
    "            translated.append(self.translate_lines(lines[-1:])[0])\n",
    "            \n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    \n",
    "    def __init__(self, name, hid_size, activ=tf.tanh,):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        self.name = name\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # YOUR CODE - create layer variables\n",
    "            #<YOUR CODE>\n",
    "            self.linear_e = L.Dense(hid_size)\n",
    "            self.linear_d = L.Dense(hid_size)\n",
    "            self.linear_out = L.Dense(1)\n",
    "\n",
    "    def __call__(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # Compute logits\n",
    "            #<...>\n",
    "            logits_seq = self.linear_out(self.activ(self.linear_e(enc) + \\\n",
    "                                                    self.linear_d(dec)[:, tf.newaxis, :]))\n",
    "            logits_seq = tf.squeeze(logits_seq, axis = -1)\n",
    "            \n",
    "            # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "            # You may need tf.where\n",
    "            #<...>\n",
    "            \n",
    "            logits_seq = tf.where(inp_mask, logits_seq, tf.fill(tf.shape(logits_seq),\n",
    "                                                                -np.inf))\n",
    "            \n",
    "            # Compute attention probabilities (softmax)\n",
    "            probs = tf.nn.softmax(logits_seq) # <...>\n",
    "            \n",
    "            # Compute attention response using enc and probs\n",
    "            attn = tf.reduce_sum(probs[..., tf.newaxis] * enc, axis = 1) # <...>\n",
    "            \n",
    "            return attn, probs\n",
    "        \n",
    "class AttentiveModel(ITranslationModel):\n",
    "    \n",
    "    def __init__(self, filename, name = None, inp_voc = None, out_voc = None,\n",
    "                 emb_size = None, hid_size = None):\n",
    "        \n",
    "        if filename is None:\n",
    "            self.initialize(name, inp_voc, out_voc,\n",
    "                            emb_size, hid_size) #, attn_size)\n",
    "        else:\n",
    "            self.load(filename)\n",
    "    \n",
    "    \n",
    "    def initialize(self, name, inp_voc, out_voc,\n",
    "                   emb_size, hid_size): #, attn_size):\n",
    "        \n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        #self.attn_size = attn_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # YOUR CODE - define model layers\n",
    "            \n",
    "            # <...>\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc_lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            self.enc_lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            #self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.dense = L.Dense(hid_size)\n",
    "            self.activ = tf.tanh\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            self.attention = AttentionLayer(name = 'attention',\n",
    "                                            #enc_size = None, # FIXME: Unused\n",
    "                                            #dec_size = None, # FIXME: Unused\n",
    "                                            #hid_size = attn_size)\n",
    "                                            hid_size = 2 * self.hid_size)\n",
    "            \n",
    "            # END OF YOUR CODE\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "            self.next_softmax = tf.nn.softmax(self.next_logits)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        \n",
    "        # Call to 'K.get_session()' runs variable initializes for\n",
    "        # all variables including ones initialized using\n",
    "        # 'tf.global_variables_initializer()' (at least for Keras\n",
    "        # 2.0.5) thus it have to be called once here or model weights\n",
    "        # will be rewritten after training e.g. when 'get_weights' is\n",
    "        # called.\n",
    "        K.get_session()\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        # <YOUR CODE>\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_mask = infer_mask(inp, self.inp_voc.eos_ix, dtype = tf.bool)\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            #enc_seq, enc_last = tf.nn.dynamic_rnn(self.enc0,\n",
    "            #                                      inp_emb,\n",
    "            #                                      sequence_length = inp_lengths,\n",
    "            #                                      dtype = inp_emb.dtype)\n",
    "            ((enc_seq_fw,\n",
    "              enc_seq_bw),\n",
    "             (enc_last_fw,\n",
    "              enc_last_bw)) = tf.nn.bidirectional_dynamic_rnn(self.enc_lstm_fw_cell,\n",
    "                                                              self.enc_lstm_bw_cell,\n",
    "                                                              inp_emb,\n",
    "                                                              sequence_length = inp_lengths,\n",
    "                                                              dtype = inp_emb.dtype)\n",
    "        enc_seq = tf.concat((enc_seq_fw, enc_seq_bw), axis = -1)\n",
    "        dec_start = self.dec_start(enc_last_fw)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        #first_attn_probas = <...>\n",
    "        _, first_attn_probas = self.attention(enc_seq, dec_start, inp_mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        #first_state = [<...>, first_attn_probas]\n",
    "        first_state = [dec_start, enc_seq, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order\n",
    "        # that you've packed in encode\n",
    "        #[<...>, prev_attn_probas] = prev_state\n",
    "        [prev_dec, enc_seq, inp_mask, prev_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and attn probas given previous decoder state\n",
    "        # * use prev token embedding and attn response to update decoder states\n",
    "        # * (concatenate and feed into decoder cell)\n",
    "        # * predict logits\n",
    "        \n",
    "        # <APPLY_ATTENTION>\n",
    "        next_attn_response, next_attn_probas = self.attention(enc_seq, prev_dec, inp_mask)\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        dec_inputs = tf.concat([prev_emb, next_attn_response], axis = 1)\n",
    "        with tf.variable_scope('dec0'):\n",
    "            new_dec_out, new_dec_state = self.dec0(dec_inputs, prev_dec)\n",
    "        output_logits = self.logits(self.activ(self.dense(new_dec_out)))\n",
    "        #output_logits = self.logits(self.activ(new_dec_out))\n",
    "        \n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        #next_state = [<...>, next_attn_probas]\n",
    "        next_state = [new_dec_state, enc_seq, inp_mask, next_attn_probas]\n",
    "        return next_state, output_logits\n",
    "\n",
    "    \n",
    "    def compute_logits(self, inp, out, **flags):\n",
    "        \n",
    "        batch_size = tf.shape(inp)[0]\n",
    "\n",
    "        # Encode inp, get initial state\n",
    "        first_state = self.encode(inp) # <YOUR CODE HERE>\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        first_logits = tf.log(tf.one_hot(tf.fill([batch_size], self.out_voc.bos_ix),\n",
    "                                         len(self.out_voc)) + 1e-30)\n",
    "\n",
    "        # Decode step\n",
    "        def step(prev_state, y_prev):\n",
    "            # Given previous state, obtain next state and next token logits\n",
    "            # <YOUR CODE>\n",
    "            next_dec_state, next_logits = self.decode(prev_state, y_prev)\n",
    "            return next_dec_state, next_logits # <...>\n",
    "\n",
    "        # You can now use tf.scan to run step several times.\n",
    "        # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "        # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        out = tf.scan(lambda a, y: step(a[0], y),\n",
    "                      elems = tf.transpose(out)[:-1],\n",
    "                      initializer = (first_state, first_logits))\n",
    "\n",
    "\n",
    "        # FIXME remove?\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        logits_seq = out[1] # <YOUR CODE>\n",
    "\n",
    "        # prepend first_logits to logits_seq\n",
    "        logits_seq = tf.concat((first_logits[tf.newaxis], logits_seq), axis = 0) #<...>\n",
    "\n",
    "        # Make sure you convert logits_seq from\n",
    "        # [time, batch, voc_size] to [batch, time, voc_size]\n",
    "        logits_seq = tf.transpose(logits_seq, perm = [1, 0, 2]) #<...>\n",
    "\n",
    "        return logits_seq\n",
    "\n",
    "    def compute_loss(self, inp, out, **flags):\n",
    "        \n",
    "        mask = infer_mask(out, out_voc.eos_ix)    \n",
    "        logits_seq = self.compute_logits(inp, out, **flags)\n",
    "\n",
    "        # Compute loss as per instructions above\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        prob_seq = tf.nn.softmax(logits_seq)\n",
    "        out_one_hot = tf.one_hot(out, len(self.out_voc))\n",
    "\n",
    "        prob_seq_masked = tf.boolean_mask(prob_seq, mask)\n",
    "        out_one_hot_masked = tf.boolean_mask(out_one_hot, mask)\n",
    "        prob_seq_out = tf.boolean_mask(prob_seq_masked, out_one_hot_masked)\n",
    "        loss = tf.reduce_mean(-tf.log(prob_seq_out))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def make_initial_state(self, inp_lines):\n",
    "        return sess.run(self.initial_state, {self.inp: self.inp_voc.to_matrix(inp_lines)})\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        return sess.run([self.next_state, self.next_logits],\n",
    "                        {**dict(zip(self.prev_state, state)),\n",
    "                         self.prev_tokens: [out_i[-1] for out_i in outputs]})\n",
    "                         \n",
    "    def get_output_vocabulary(self):\n",
    "        return self.out_voc\n",
    "    \n",
    "    \n",
    "    def translate_lines(self, inp_lines, max_len=100):\n",
    "        \"\"\"\n",
    "        Translates a list of lines by greedily selecting most likely next token at each step\n",
    "        :returns: a list of output lines, a sequence of model states at each step\n",
    "        \"\"\"\n",
    "        state = self.make_initial_state(inp_lines)\n",
    "        outputs = [[self.out_voc.bos_ix] for _ in range(len(inp_lines))]\n",
    "        all_states = [state]\n",
    "        finished = [False] * len(inp_lines)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            state, logits = self.get_next_state_and_logits(state, outputs)\n",
    "            next_tokens = np.argmax(logits, axis=-1)\n",
    "            all_states.append(state)\n",
    "            for i in range(len(next_tokens)):\n",
    "                outputs[i].append(next_tokens[i])\n",
    "                finished[i] |= next_tokens[i] == self.out_voc.eos_ix\n",
    "        return self.out_voc.to_lines(outputs), all_states\n",
    "    \n",
    "    def dump(self, filename):\n",
    "        \n",
    "        values = {'name': self.name,\n",
    "                  'inp_voc': self.inp_voc,\n",
    "                  'out_voc': self.out_voc,\n",
    "                  'emb_size': self.emb_size,\n",
    "                  'hid_size': self.hid_size,\n",
    "                  #'attn_size': self.attn_size,\n",
    "                  'emb_inp_weights': self.emb_inp.get_weights(),\n",
    "                  'emb_out_weights': self.emb_out.get_weights(),\n",
    "                  #'enc0_weights': self.enc0.get_weights(),\n",
    "                  'enc_lstm_fw_cell_weights': self.enc_lstm_fw_cell.get_weights(),\n",
    "                  'enc_lstm_bw_cell_weights': self.enc_lstm_bw_cell.get_weights(),\n",
    "                  'dec0_weights': self.dec0.get_weights(),\n",
    "                  'dec_start_weights': self.dec_start.get_weights(),\n",
    "                  'dense_weights': self.dense.get_weights(),\n",
    "                  'logits_weights': self.logits.get_weights(),\n",
    "                  'attn__linear_e_weights': self.attention.linear_e.get_weights(),\n",
    "                  'attn__linear_d_weights': self.attention.linear_d.get_weights(),\n",
    "                  'attn__linear_out_weights': self.attention.linear_out.get_weights()}\n",
    "        pickle.dump(values, open(filename, 'wb'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            values = pickle.load(f)\n",
    "        self.initialize(values['name'], values['inp_voc'], values['out_voc'],\n",
    "                        values['emb_size'], values['hid_size']) #, values['attn_size'])\n",
    "        self.emb_inp.set_weights(values['emb_inp_weights'])\n",
    "        self.emb_out.set_weights(values['emb_out_weights'])\n",
    "        #self.enc0.set_weights(values['enc0_weights'])\n",
    "        self.enc_lstm_fw_cell.set_weights(values['enc_lstm_fw_cell_weights'])\n",
    "        self.enc_lstm_bw_cell.set_weights(values['enc_lstm_bw_cell_weights'])\n",
    "        self.dec0.set_weights(values['dec0_weights'])\n",
    "        self.dec_start.set_weights(values['dec_start_weights'])\n",
    "        self.dense.set_weights(values['dense_weights'])\n",
    "        self.logits.set_weights(values['logits_weights'])\n",
    "        self.attention.linear_e.set_weights(values['attn__linear_e_weights'])\n",
    "        self.attention.linear_d.set_weights(values['attn__linear_d_weights'])\n",
    "        self.attention.linear_out.set_weights(values['attn__linear_out_weights'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:80: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:80: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f6cdffd6588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f6cdffd6588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f6cdffd65f8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f6cdffd65f8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:87: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:87: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:142: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:142: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "if 'model_loaded' in globals():\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Need to also recreate Rupo as it uses TensorFlow:\n",
    "    global_rupo_engine = make_rupo_engine()\n",
    "\n",
    "model_loaded = AttentiveModel(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovoc = model_loaded.get_output_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok = {i:token for token, i in zip(ovoc.token_to_ix.keys(),ovoc.token_to_ix.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = [id2tok[i] for i in sorted(list(id2tok))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rupo_engine.get_stresses(\"ереп\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens_unbpe = [token.replace(\"@\", '') for token in sorted_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables_tokens = [global_rupo_engine.get_stresses(token) for token in sorted_tokens_unbpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"уёеэоаыяию\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vowels(word, idx):\n",
    "    before = 0\n",
    "    after = 0\n",
    "    for s in word[:idx]:\n",
    "        if s in VOWELS:\n",
    "            before += 1\n",
    "    for s in word[idx+1:]:\n",
    "        if s in VOWELS:\n",
    "            after += 1\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RhymeTranslator_F(object):\n",
    "    \n",
    "    def __init__(self, model: ITranslationModel, rupo_engine = None):\n",
    "        \n",
    "        self._model = model\n",
    "        self._out_voc = model.get_output_vocabulary()\n",
    "        \n",
    "        self._suffix_rhyme_tester = RuReversedSuffixRhymeTester()\n",
    "        self._word_rhyme_tester = RuReversedWordRhymeTester(rupo_engine = rupo_engine)\n",
    "    \n",
    "    \n",
    "    def _lines_to_model_lines(self, lines):\n",
    "        return list(map(get_reversed, lines))\n",
    "    \n",
    "    def _merge_tokens(self, line):\n",
    "        return line.replace('@@ ', '')\n",
    "    \n",
    "    def _model_tokens_to_model_line(self, output, eos_as_space):\n",
    "        [line] = self._out_voc.to_lines([output])\n",
    "        if eos_as_space and output[-1] == self._out_voc.eos_ix:\n",
    "            line += ' '\n",
    "        return self._merge_tokens(line)\n",
    "    \n",
    "    def _model_tokens_to_lines(self, outputs):\n",
    "        lines = self._out_voc.to_lines(outputs)\n",
    "        return [get_reversed(self._merge_tokens(l)) for l in lines]\n",
    "    \n",
    "    def _apply_softmax(self, logits, temperature):\n",
    "        \n",
    "        if temperature != 1:\n",
    "            if temperature < 1:\n",
    "                # Convert to 64-bit float to avoid overflows:\n",
    "                # Note: There will still be overflows for T < 0.03\n",
    "                logits = logits.astype(np.float64)\n",
    "            \n",
    "            logits /= temperature\n",
    "\n",
    "        np.exp(logits, out = logits)\n",
    "        logits /= logits.sum(axis = -1)[..., np.newaxis]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def translate_lines(self,\n",
    "                        lines,\n",
    "                        sample_temperature = 0, \n",
    "                        max_len = 100, \n",
    "                        rhythme_line=0):\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        # list len 4 with arrays\n",
    "        state = self._model.make_initial_state(model_lines)\n",
    "        eoses = np.array([self._out_voc.eos_ix for i in range(4)])\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 100), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "        \n",
    "        previous_state = state\n",
    "        last_state = state\n",
    "        word_count = max_len\n",
    "        start=True\n",
    "        idx=1\n",
    "        t=0\n",
    "        \n",
    "        while t < word_count:\n",
    "            next_word = False\n",
    "            counter = 0\n",
    "            buffer = []\n",
    "            tokens_to_add = []\n",
    "            while not next_word:\n",
    "                counter += 1\n",
    "                print(idx)\n",
    "                \n",
    "                state, logits = self._model.get_next_state_and_logits(state, outputs[:, :(idx+len(tokens_to_add))])\n",
    "                \n",
    "                if sample_temperature:\n",
    "                    # Sample from softmax with temperature:\n",
    "                    logits = self._apply_softmax(logits, sample_temperature)\n",
    "                    next_tokens = np.array([np.random.choice(len(probs), p=probs) for probs in logits])\n",
    "                else:\n",
    "                    next_tokens = np.argmax(logits, axis=-1)\n",
    "                \n",
    "                cur_token = sorted_tokens[next_tokens[rhythme_line]] # str\n",
    "                \n",
    "                condition = False # start new word\n",
    "                if len(buffer) > 0:\n",
    "                    condition = (cur_token[-1] != '@' and not start and buffer[-1][-1] != '@')\n",
    "                else:\n",
    "                    condition = (cur_token[-1] != '@' and not start)\n",
    "                if cur_token == \"_EOS_\":\n",
    "                    state = previous_state\n",
    "                    tokens_to_add = []\n",
    "                    continue\n",
    "                \n",
    "                if condition: \n",
    "                    print(\"cur\", cur_token)\n",
    "                    print(\"buf\", buffer)\n",
    "                    word = get_reversed(' '.join(buffer).replace('@@ ', ''))\n",
    "                    buffer = []\n",
    "                    stress = global_rupo_engine.get_stresses(word)\n",
    "                    #a = np.array(tokens_to_add + [np.array([self._out_voc.eos_ix] * 4)])  \n",
    "                    if len(stress) == 0:\n",
    "                        state = previous_state\n",
    "                        tokens_to_add = []\n",
    "                        continue\n",
    "                        \n",
    "                    stress = stress[0]\n",
    "                    before, after = count_vowels(word, stress)\n",
    "                    #if (before > 1) or (after > 0) or (before + after == 0):\n",
    "                    if len(word) > 3 and ((after > 0) or (before + after == 0)):\n",
    "                        state = previous_state\n",
    "                        tokens_to_add = []\n",
    "                    else:\n",
    "                        for i in range(len(tokens_to_add)):\n",
    "                            print(\"ID\", idx+i)\n",
    "                            outputs[:, idx + i] = tokens_to_add[i]\n",
    "                        print(word)\n",
    "                        idx += len(tokens_to_add)\n",
    "                        tokens_to_add = []\n",
    "                        #outputs[:, t + 1] = next_tokens\n",
    "                        next_word=True\n",
    "                        previous_state = last_state\n",
    "                        t += 1\n",
    "                        \n",
    "                        finished |= outputs[:, idx-1] == self._out_voc.eos_ix\n",
    "                    \n",
    "                        if finished.sum() == len(lines):\n",
    "                            break \n",
    "                else:\n",
    "                    buffer.append(cur_token)\n",
    "                    tokens_to_add.append(next_tokens)\n",
    "                    outputs[:, idx+len(tokens_to_add) - 1] = next_tokens\n",
    "                    last_state = state\n",
    "                    start = False\n",
    "                    \n",
    "                # Early exis if all lines finished\n",
    "        outputs[:, idx] = self._out_voc.eos_ix\n",
    "        print(outputs)\n",
    "        return self._model_tokens_to_lines(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = RhymeTranslator_F(model_loaded, global_rupo_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur асолог\n",
      "buf ['.@@', 'солог', 'йишйен@@', 'тсу', 'йе@@', 'от@@', 'Э']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur солог\n",
      "buf ['.@@', 'солог', 'й@@', 'ыдж@@', 'а@@', 'К', ',@@', 'мосолог', 'ьшеч@@', 'о@@', 'М', '.@@', 'солог', 'йищя@@', 'ма@@', 'З', '?@@', 'енс', 'о@@', 'В', '…@@', 'солог', 'йикс@@', 'йа@@', 'Т', ',@@', 'мосолог', 'еоннещ@@', 'ыс@@', 'ор@@', 'П', '...@@', 'асолог', 'еынч@@', 'е@@', 'В']\n",
      "1\n",
      "1\n",
      "1\n",
      "cur ьталедс\n",
      "buf ['.@@', 'салг']\n",
      "1\n",
      "cur солог\n",
      "buf []\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur ее\n",
      "buf ['.@@', 'ястеад@@', 'за@@', 'Р', '!@@', 'солог', 'йын@@', 'ьло@@', 'Б', '.@@', 'оволс', 'е@@', 'од@@', 'ж@@', 'а@@', 'Ш', '.@@', 'солог']\n",
      "1\n",
      "1\n",
      "1\n",
      "cur удуб\n",
      "buf ['.@@', 'ьтазакс']\n",
      "ID 1\n",
      "ID 2\n",
      "сказать.\n",
      "3\n",
      "cur ароп\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьтсонжомзов\n",
      "buf ['ьлов@@', 'зо@@', 'П']\n",
      "Позволь\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur имабуг\n",
      "buf ['оглод@@', 'е@@', 'Н']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur окобулг\n",
      "buf ['ум@@', 'о@@', 'К']\n",
      "3\n",
      "cur ьшил\n",
      "buf []\n",
      "3\n",
      "cur М\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['вив@@', 'атс@@', 'са@@', 'Р']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur онжом\n",
      "buf ['ьше@@', 'уз@@', 'аг@@', 'оп@@', 'С', 'неж@@', 'ло@@', 'Д']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur учох\n",
      "buf [',@@', 'еп@@', 'С', 'у@@', '-@@', 'у@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur онжом\n",
      "buf ['онж@@', 'яс@@', 'а@@', 'Н', 'о@@', 'д@@', 'О', 'ы@@', 'Т']\n",
      "3\n",
      "cur а\n",
      "buf []\n",
      "3\n",
      "cur хатанмок\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['онж@@', 'он@@', 'вен@@', 'г@@', 'ор@@', 'П', 'ум@@', 'зов@@', 'Е']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['сач@@', 'йе@@', 'С', 'ься@@', 'ем@@', 'С', 'ьсилич@@', 'у@@', 'ло@@', 'П']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur йыроток\n",
      "buf ['ещ@@', 'Е', 'тя@@', 'то@@', 'Х', 'а@@', 'Д', 'ещ@@', 'Е', 'ясме@@', 'арат@@', 'С', 'юун@@', 'мтир']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur Е\n",
      "buf ['ом@@', 'яз@@', 'ьле@@', 'Н', 'теуд@@', 'ел@@', 'С', 'онж@@', 'оротс@@', 'О', 'уго@@', 'М', 'от@@', '-@@', 'у@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur А\n",
      "buf ['о@@', 'И', 'уш@@', 'о@@', 'П']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur угом\n",
      "buf ['о@@', 'П']\n",
      "По\n",
      "3\n",
      "cur и\n",
      "buf []\n",
      "3\n",
      "cur етежомс\n",
      "buf []\n",
      "3\n",
      "cur угом\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur учох\n",
      "buf ['теуд@@', 'о@@', 'Д']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьсюаратс\n",
      "buf ['ьсишв@@', 'яз@@', 'В']\n",
      "3\n",
      "cur мок\n",
      "buf []\n",
      "3\n",
      "cur етитох\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur угом\n",
      "buf ['ьшеч@@', 'о@@', 'Х', 'неж@@', 'ло@@', 'Д']\n",
      "3\n",
      "cur охолпен\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur угом\n",
      "buf ['ьдуб@@', 'И']\n",
      "3\n",
      "cur хи\n",
      "buf []\n",
      "3\n",
      "cur ьшёнчан\n",
      "buf []\n",
      "3\n",
      "cur ебет\n",
      "buf []\n",
      "3\n",
      "cur агурд\n",
      "buf []\n",
      "3\n",
      "cur угом\n",
      "buf []\n",
      "3\n",
      "cur онжом\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['от@@', 'Э', 'ве@@', 'Л', 'от@@', 'Э', 'анж@@', 'ло@@', 'Д', 'от@@', 'Э']\n",
      "3\n",
      "cur И\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur яслилемсо\n",
      "buf ['ястю@@', 'ат', 'от@@', 'Ч', 'отс@@', 'ор@@', 'П', 'уч@@', 'о@@', 'Х', 'ься@@', 'арат@@', 'С', 'йывот@@', 'о@@', 'М']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur онжун\n",
      "buf ['е@@', 'Н']\n",
      "Не\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur олыб\n",
      "buf [',@@', 'яз@@', 'ьле@@', 'Н']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur угом\n",
      "buf ['е@@', 'Н']\n",
      "Не\n",
      "[[                   0                  121                49977\n",
      "                     1                 2145                47503\n",
      "                  2145                35513                 2151\n",
      "   6875977806491969312  7453005893910689138  8241432738698456681\n",
      "  -5561902485348714136  3181714867565475280 -9020228670688581856\n",
      "   2531068240223909328  8316213866978104352  8103473403919296544\n",
      "   8316862535872815215  7308895159697829468  6424146442465386611\n",
      "   7163379235450680687  7308335460314405989  8391735665606074468\n",
      "   7307218077897484659  6662916650857756448  8026380410238160750\n",
      "   2334102057711400046  6998719600886904179  7453010330690546796\n",
      "   6657854520580776051  8315171504050762606  2821267063918633075\n",
      "   2821633162687312686  2553079370590608476  2323048662788106604\n",
      "   8223695477882169127  8532461059567804783  7955925832296788581\n",
      "   8028911418774413600  6641793980238864503  8030798176250307438\n",
      "   8026294459353887346  2338616625292668021  7070770598005273204\n",
      "   7944670174561989743  7308901430199459940  7308327854430909472\n",
      "   2334956356649364594  6657568673646190665  8028827851413332846\n",
      "   8606207503214406511  8313962007119208558  2338319423521252896\n",
      "   6657565400796176457  7307221376599073646  7305437229900391794\n",
      "   7526676535825167470  7454131815275896933  7952296361846075250\n",
      "   7952352384102377251  6640320656114807408  7956012718558244444\n",
      "   7020112837082248232  7953764260234030964  7596551560816061555\n",
      "   7308895158524929390  7813865618798554227  8243118308127432549\n",
      "   3328509105524798561  7809092638721715250  7526113301533388389\n",
      "   7596551560699081849  2461543928214414702  8391722768137527852\n",
      "   7959390389040738153      137667446186612                 4817\n",
      "       140119607525800      140119607525800       94145990502720\n",
      "        94145990502720  4991219038288431186  5641130513407628610\n",
      "   6992412577556668500  7811247763204961139  8103508858688856687\n",
      "   7308895138223578991  7888451062753093934 -3335530482583000987\n",
      "  -3447751615711752055 -3335530688715304782  7517669013223712640\n",
      "   8027702015844971369  8367810636449718391  7785046396703238504\n",
      "   2819320576318205545  8511929144435091239  7809596940766898543\n",
      "   8007511656834033004  8367799589594690677  8026661835130299752\n",
      "   7164792839208897143  7234316415412760175  2338042677220963104]\n",
      " [                   0                25437                38790\n",
      "                     1                 2148                25682\n",
      "                  7909                36194                43209\n",
      "   6998715218727822695  2339461042908002592  4693606422430052215\n",
      "   8751182848932734062  7236287822542424352  7021802451765259808\n",
      "   7526763344484394358  7305437161266946826  7021802451513536288\n",
      "   7791276225190454646  8391086029169716847  7236793269984980847\n",
      "   2334102018990238752  7308901430351851364  2338038247817699616\n",
      "   8462091215169418081  8583983296923985004  2338609694373995880\n",
      "   2336920844765128034  7306085944835532916  4280799502135355250\n",
      "   8073276445291913994  7952274096735021426  8370061306587917863\n",
      "   8031151153929412978  7814711143229042290  7308895159446762593\n",
      "   3203015284880058483  6874019636378170144  8386109761144055156\n",
      "   3184658675522695797  7954883474117192992  8392854219956958525\n",
      "   7308895159446760808       11171900502077                 4241\n",
      "       140119607524216      140119607524216                    0\n",
      "                     0                    0                   22\n",
      "                    -1      140104991660336                    0\n",
      "       140105315774029                    0                    0\n",
      "                     0                    0                    0\n",
      "                     0                    0                    0\n",
      "                     0      140105315774029                    0\n",
      "                     0                    0                    0\n",
      "                     0                    0                    0\n",
      "                     0                    0      140105315774029\n",
      "                     0      140105315774029                    0\n",
      "                   576                    2      140105315774032\n",
      "        94145942818916                    3      140105315774031\n",
      "        94145942818680      140104991660464                    0\n",
      "                   840                    7      140105315774032\n",
      "        94145942818984                    0                   22\n",
      "                    -1      140104991660464      140101833195520\n",
      "                   912                    3      140105315774032\n",
      "        94145942818988                    1                    0\n",
      "                     0      140104991660336                    0]\n",
      " [                   0                   81                   26\n",
      "                     1                27649                44018\n",
      "                    14                40393                 9801\n",
      "                  3681      140119607524216      140119607524216\n",
      "                     0                    0                   23\n",
      "                    12      140104991660272      140101833195520\n",
      "                  1128                    3      140105315774032\n",
      "        94145942822968                    0                    0\n",
      "                     0                    0                    0\n",
      "                     0      140105315774029                    0\n",
      "                     0                    0                    0\n",
      "                     0                    0                    0\n",
      "                     0                    0      140105315774029\n",
      "                     0      140105315774029      140105315774032\n",
      "                     0                 1128                    2\n",
      "       140105315774032       94145942819152  2314885436716626736\n",
      "   4048795683447111680  3689354308106138932  3473174851193354032\n",
      "   3544672892056764416  3761123851488475447  3978987665372684320\n",
      "   4048791269180651574  2314861427799175223  3473743380628386592\n",
      "   3977582493807556153  3833443030492329520  3833746573011072308\n",
      "   3544948874629099574  3978987652419428401  3833185847901106745\n",
      "   2308710751096156978  3617856356896415776  3833180354621552435\n",
      "   3683979925216899127  3689352104636789046    14975585452765492\n",
      "                  3201      140119607524216      140119607524216\n",
      "                     0                    0  2314906524921836084\n",
      "   3906083447264063540  3977585783618090546  3979267958830674993\n",
      "   4121697664653211703  3546643204834210872                 2336\n",
      "                  2336                    0       94145837378656\n",
      "                  2274                   -1  2314885530818453732\n",
      "                     0  2314885530818453595  2314885530818453536\n",
      "   2314885599537930272  2314885530818453536  3539864629463228448\n",
      "   2314885530818457906  2314885530818453536   734708183334068256\n",
      "   2314885530818453536  2314885530818453536  2314908728206832672\n",
      "   2314885530818453536  3834005980125863968  2314885530819769143\n",
      "   2314885530818453536  3539864629463228448  2314885530818453514]\n",
      " [                   0                44616                24179\n",
      "                     1                 2139                    1\n",
      "                     1                49525                 5260\n",
      "   3834870303958245408  2314885530818453536  2314885530818453536\n",
      "   2314885599824523825  2314885530818453536  3833463920893370400\n",
      "   2314885530817016121  2314885530818453536  2314885530818453536\n",
      "   3978428039638818865  3978704008519037236                 2753\n",
      "       140119607524216      140119607524216                    0\n",
      "                     0  4123102853414860852  3904961945117144376\n",
      "   3832626166367401268  2321387046489634865  4048793450973704992\n",
      "   3545236950937581877  2314885436666164278  3761966068872525363\n",
      "   3978985483395152184  3760563001438123315  4121136922246068024\n",
      "   3904681574267040051  3906930062660870176  3690759496699819057\n",
      "   2314861427832665656  3689911790481716256  3977578078581175089\n",
      "   3473436535312954169  3473177132223705141  3761411910700250931\n",
      "   3905804179932717108  4049361911387075383  2308716244409856310\n",
      "   4050768203921694752  3906370424077170741  3611922305392261175\n",
      "   3976731471824106035  3616729365967418169  3905805270535649078\n",
      "   3616448986308227894   736118865242044467  3474865969130643488\n",
      "   3544676200040837687  2314907615877282105  3761410819677630772\n",
      "   4051328946245547312  3546077969672843828  4121416224069859640\n",
      "   3474018245683720501  3689065144716894218  3835149558505091121\n",
      "   2321380436551480881  3978147655652553504  3978144348426680627\n",
      "   3905500624513743161                 2353      140119607524216\n",
      "        94145960507104                    0                    0\n",
      "   3835153956518174752  3689065132118521142  3683979890873415222\n",
      "   3832616305072418873  3977020643365433911  3761385436133078835\n",
      "   3689913968114545207  4050764879583654704  3689347706655875120\n",
      "   3977858471125267512  2314913121974694705  3616730465428452146\n",
      "   3905240134746912819  3683979817171629616  3906925681878446390\n",
      "                  2193      140119607524216       94145960507104\n",
      "                     0                    0  3474303019228608561\n",
      "   2314885436683072051  3689072841568104500  3616727192780682544\n",
      "   3690453742004483638  3907211554850943286  3978983263048185650\n",
      "   3760559793366114336  3978423632953422389  2314861444978456114]]\n",
      "сказать.\n",
      "@@пает городок\n",
      "@@\",\n",
      "@@ный воздух\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "assert global_rupo_engine.is_rhyme('вещи', 'ветер') # This is how Rupo thinks\n",
    "\n",
    "lines = '''Your voice\n",
    "Called me outside the window\n",
    "Of uncontested summer all things raise\n",
    "seamless air'''.split('\\n')\n",
    "#lines = '''Two roads diverged in a yellow wood,\n",
    "#And sorry I could not travel both\n",
    "#And be one traveler, long I stood\n",
    "#And looked down one as far as I could\n",
    "#To where it bent in the undergrowth;\n",
    "#'''\n",
    "\n",
    "print('\\n'.join(translator.translate_lines(lines, sample_temperature=1.2, max_len=5, rhythme_line=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print()\n",
    "#print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "#                                                      rhyme_type = RhymeType.WORD,\n",
    "#                                                      sample_temperature=0.5,\n",
    "#                                                      rhyme_test_counts=(5, 5, 2),\n",
    "#                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лохматый голос,\n",
    "\n",
    "В сторону окна\n",
    "\n",
    "Коллекция жизнь растет\n",
    "\n",
    "скругляя воздух"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) родным криком....\n",
    "1) пробьет душа окном,\n",
    "2) лето, стад растём\n",
    "3) мигрой, отравить воздух\n",
    "стальных лесов, монет, иметь!\n",
    "типаж дождями гонётся окном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad: \"\"\"Сквозь голоса.\n",
    "Я отчётлыняюсь за окном\n",
    "растем что-нибудь быстрее...\n",
    "Вновь воздух\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ты молчишь?\n",
    "моих поцелуев опишется окна\n",
    "огонь, когда-нибудь вставали,\n",
    "Далеко быть\n",
    "\n",
    "Свой голос,\n",
    "на улице ждёт.\n",
    "стальных лесов, монет, иметь!\n",
    "Отпахлый воздух"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-d410dee9113a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mSeeds\u001b[0m \u001b[0mspring\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbeauty\u001b[0m \u001b[0mbreedeth\u001b[0m \u001b[0mbeauty\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m Thou wast begot; to get it is thy duty.'''.split('\\n')\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
      "\u001b[0;32m<ipython-input-110-ebd17b201fc1>\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, lines, sample_temperature, max_len)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mfinished\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mnext_tokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poem2poem/ipython_usage_demos/utils.py\u001b[0m in \u001b[0;36mto_lines\u001b[0;34m(self, matrix, crop)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poem2poem/ipython_usage_demos/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Torches are made to light, jewels to wear,\n",
    "Dainties to taste, fresh beauty for the use,\n",
    "Herbs for their smell, and sappy plants to bear;\n",
    "Things growing to themselves are growth’s abuse,\n",
    "Seeds spring from seeds, and beauty breedeth beauty;\n",
    "Thou wast begot; to get it is thy duty.'''.split('\\n')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он говорит о том, что этот ветер прорывается сквозь ночь,\n",
      "Ладно, мы уходим прочь.\n",
      "Крупное животное проходил через улицу,\n",
      "Нет, они так устали.\n"
     ]
    }
   ],
   "source": [
    "# Model internal translation function:\n",
    "lines = list(map(get_reversed,\n",
    "                 ['Let this wind blow through the night',\n",
    "                  'And we are going away',\n",
    "                  'The animal hasn\\'t crossed the street',\n",
    "                  'Because it was too tired']))\n",
    "translated = model_loaded.translate_lines(lines)[0]\n",
    "print('\\n'.join([get_reversed(line) for line in translated]).replace(' @@', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и был уверен.\n",
      "\"Человек\", я знаю. Грязное словечко.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Load up on guns and bring your friends\n",
    "It's fun to lose and to pretend\n",
    "She's over bored and self assured\n",
    "Oh no, I know a dirty word'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Ваш сын вернулся к отцу.\n",
      "Не спрашивай меня, и все крошки.\n",
      "- это хорошо.\n",
      "Это неправильно.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он в том, что с сыном пришёл его отец.\n",
      "и спросила у детей.\n",
      "- это плохо.\n",
      "Это не плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он вернулся с сыном с отцом.\n",
      "Попросила меня с ребёнком.\n",
      "- прекрасно.\n",
      "Это неправильно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Крошка сын к отцу пришел,\n",
    "и спросила кроха:\n",
    "— Что такое хорошо\n",
    "и что такое плохо?'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''Baby son to his father came\n",
    "and asked crumbs\n",
    "What is good\n",
    "what's wrong'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Ответ от этих отцов, вот это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет друзей,\n",
      "Ох, слушать, детей,\n",
      "Они смотрят на это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Послушай, мальчиков,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Он врывается в грязь и взволнован.\n",
      "Это эта коварная рубашка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Он врывается в грязь и взволнован.\n",
      "Это такая грязная футболка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Наверное, в грязи, и я была взволнована.\n",
      "Это кожаная рубашкана,\n",
      "Это то, что здесь сказано:\n",
      "Очевидно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Этот в грязь полез и рад.\n",
    "что грязна рубаха.\n",
    "Про такого говорят:\n",
    "он плохой, неряха.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''This in the dirt and got excited\n",
    "that dirty shirt\n",
    "About this say\n",
    "he's bad sloppy'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
