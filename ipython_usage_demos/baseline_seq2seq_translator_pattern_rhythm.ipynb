{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/isprovilkov/rupo\")\n",
    "import pickle\n",
    "import rupo.api\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple, Optional, Tuple\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from keras import backend as K\n",
    "from utils import infer_length, infer_mask\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_ROOT = '/srv/hd7/data/aklyopova/models/model_translator_attn_reversed_amalgama_subtitles_09_03_2019/'\n",
    "#MODEL_PATH = MODEL_ROOT + 'model_r2_14_03_2019.pkl'\n",
    "MODEL_PATH = '/home/isprovilkov/Poems/model_translator_attn_reversed_amalgama_subtitles_09_03_2019_550001_iters_fine_tuned_amalgama_300002_iters.pkl'\n",
    "\n",
    "RUPO_DATA_ROOT = '/home/isprovilkov/Poems/'\n",
    "RUPO_STRESS_MODEL_PATH   = '/home/isprovilkov/rupo/rupo/data/stress_models/stress_ru.h5'\n",
    "RUPO_ZALYZNIAK_DICT_PATH = RUPO_DATA_ROOT + 'zaliznyak.txt'\n",
    "del RUPO_DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reversed(line):\n",
    "    return ''.join(reversed(line))\n",
    "\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations\n",
    "        given inp and reference out \"\"\"\n",
    "    translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "    # Note: if you experience out-of-memory error,\n",
    "    # split input lines into batches and translate separately\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100\n",
    "\n",
    "def compute_bleu_large(model, inp_lines, out_lines):\n",
    "    batch_size = 256\n",
    "    result = 0.0\n",
    "    for i in range(0, inp_lines.shape[0], batch_size):\n",
    "        current_bleu = compute_bleu(model,\n",
    "                                    inp_lines[i : i + batch_size],\n",
    "                                    out_lines[i : i + batch_size])\n",
    "        current_bleu *= min(i + batch_size, inp_lines.shape[0]) - i\n",
    "        result += current_bleu\n",
    "    result /= inp_lines.shape[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterType(Enum):\n",
    "    NOT_LETTER = 0\n",
    "    VOWEL = 1\n",
    "    CONSONANT = 2\n",
    "    SIGN = 3\n",
    "\n",
    "class RuAlphabetInfo(object):\n",
    "    \n",
    "    _ru_letters = set(''.join([chr(n) for n in range(ord('а'), ord('я') + 1)]) + 'ё')\n",
    "    assert len(_ru_letters) == 33\n",
    "    _ru_vowels = set('аеёийоуыэюя')\n",
    "    _ru_consonants = set('бвгджзклмнпрстфхцчшщ')\n",
    "    _ru_signs = set('ъь')\n",
    "\n",
    "    assert _ru_vowels & _ru_consonants == set()\n",
    "    assert _ru_vowels | _ru_consonants | _ru_signs == _ru_letters\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ru_letter_type(ch: str) -> LetterType:\n",
    "\n",
    "        if ch in RuAlphabetInfo._ru_vowels:\n",
    "            return LetterType.VOWEL\n",
    "        elif ch in RuAlphabetInfo._ru_consonants:\n",
    "            return LetterType.CONSONANT\n",
    "        elif ch in RuAlphabetInfo._ru_signs:\n",
    "            return LetterType.SIGN\n",
    "        else:\n",
    "            return LetterType.NOT_LETTER\n",
    "    \n",
    "    @staticmethod\n",
    "    def lower_and_strip_left_non_letters(line: str) -> str:\n",
    "        \n",
    "        line = line.lower()\n",
    "        \n",
    "        # Skip everything before first letter:\n",
    "        while len(line) > 0 and line[0] not in RuAlphabetInfo._ru_letters:\n",
    "            line = line[1:]\n",
    "        \n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RhymeInfo(NamedTuple):\n",
    "#    text: str\n",
    "#    finished: bool\n",
    "\n",
    "# Old-style for Python 3.5:\n",
    "RhymeInfo = NamedTuple('RhymeInfo',\n",
    "                       [('text', str), # text that will be used for testing whether rhyme is present\n",
    "                        ('finished', bool) # is text complete\n",
    "                       ])\n",
    "        \n",
    "class IRhymeTester(object):\n",
    "\n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        # Returns True - rhyme present, False - absent, None - don't know\n",
    "        raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedSuffixRhymeTester(IRhymeTester):\n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # input: line is REVERSED string\n",
    "        # We iterate all vowels from the beginning of line until first consonant\n",
    "        # Or all consonants until first vowel\n",
    "        # SIGNs are ignored but inserted to output\n",
    "\n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "            \n",
    "        finished = False\n",
    "        prefix_len = 0\n",
    "        prev_ch_type = None\n",
    "\n",
    "        for ch in line:\n",
    "            \n",
    "            ch_type = RuAlphabetInfo.get_ru_letter_type(ch)\n",
    "            \n",
    "            if ch_type == LetterType.NOT_LETTER:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            prefix_len += 1\n",
    "\n",
    "            if ch_type != LetterType.SIGN:\n",
    "                if prev_ch_type is None:\n",
    "                    prev_ch_type = ch_type\n",
    "                elif prev_ch_type != ch_type:\n",
    "                    finished = True\n",
    "                    break\n",
    "\n",
    "        return RhymeInfo(text = line[:prefix_len],\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            l = min(len(info1.text), len(info2.text))\n",
    "            if info1.text[:l] != info2.text[:l]:\n",
    "                return False\n",
    "            return None\n",
    "        \n",
    "        return info1.text == info2.text\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedSuffixRhymeTester()\n",
    "    for line, suffix, finished in [('пижама', 'ма', True),\n",
    "                                   ('обученный', 'ный', True),\n",
    "                                   ('Ихтиандр', 'андр', True),\n",
    "                                   ('КНДР', 'кндр', False),\n",
    "                                   ('махать', 'ать', True)]:\n",
    "        for l in (line, line + '!', ' ' + line + ', '):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == get_reversed(suffix)\n",
    "            assert info.finished == (finished or l[0] == ' ')\n",
    "            assert tester.is_rhyme(info, info) == (True if info.finished else None)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isprovilkov/p2p/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "def make_rupo_engine():\n",
    "    rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "    rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                     zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "    return rupo_engine\n",
    "\n",
    "global_rupo_engine = make_rupo_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuReversedWordRhymeTester(IRhymeTester):\n",
    "    \n",
    "    _WORD_INTERNAL_CHARS = set('-.')\n",
    "    \n",
    "    \n",
    "    def __init__(self, rupo_engine = None):\n",
    "        \n",
    "        if rupo_engine is None:\n",
    "            self._rupo_engine = rupo.api.Engine(language = 'ru')\n",
    "            self._rupo_engine.load(stress_model_path = RUPO_STRESS_MODEL_PATH,\n",
    "                                   zalyzniak_dict = RUPO_ZALYZNIAK_DICT_PATH)\n",
    "        else:\n",
    "            # Assume loaded engine:\n",
    "            assert rupo_engine.language == 'ru'\n",
    "            self._rupo_engine = rupo_engine\n",
    "    \n",
    "    \n",
    "    def extract_rhyme_info(self, line: str) -> RhymeInfo:\n",
    "        # line is REVERSED string\n",
    "        # Here we skip everything until first letter\n",
    "        # return: it TRIES to return complete word (returns part in case if line contains only part)\n",
    "        \n",
    "        \n",
    "        line = RuAlphabetInfo.lower_and_strip_left_non_letters(line)\n",
    "        \n",
    "        is_internal_char = lambda ch: ch in RuReversedWordRhymeTester._WORD_INTERNAL_CHARS\n",
    "            \n",
    "        finished = False\n",
    "        word_len = 0\n",
    "        \n",
    "        for ch in line:\n",
    "            \n",
    "            is_word_char = is_internal_char(ch) or \\\n",
    "                           RuAlphabetInfo.get_ru_letter_type(ch) != LetterType.NOT_LETTER\n",
    "            \n",
    "            if not is_word_char:\n",
    "                finished = True\n",
    "                break\n",
    "            \n",
    "            word_len += 1\n",
    "        \n",
    "        while word_len > 0 and is_internal_char(line[word_len - 1]):\n",
    "            word_len -= 1\n",
    "\n",
    "        return RhymeInfo(text = get_reversed(line[:word_len]),\n",
    "                         finished = finished)\n",
    "    \n",
    "    \n",
    "    def is_rhyme(self, info1: RhymeInfo, info2: RhymeInfo) -> Optional[bool]:\n",
    "        \n",
    "        if not info1.finished or not info2.finished:\n",
    "            return None\n",
    "        \n",
    "        return self._rupo_engine.is_rhyme(info1.text, info2.text)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    \n",
    "    tester = RuReversedWordRhymeTester(global_rupo_engine)\n",
    "    augment_line = lambda l: (l, l + '!', ' ' + l + ', ', 'Мы и ' + l)\n",
    "    \n",
    "    for line, word, finished in [('серая корова', 'корова', True),\n",
    "                                 ('молока, много', 'много', True),\n",
    "                                 ('Ихтиандр', 'ихтиандр', False),\n",
    "                                 ('КНДР', 'кндр', False),\n",
    "                                 ('аб-вг', 'аб-вг', False),\n",
    "                                 ('.-аб.вг.-', 'аб.вг', False)]:\n",
    "        for l in augment_line(line):\n",
    "            info = tester.extract_rhyme_info(get_reversed(l))\n",
    "            assert info.text == word\n",
    "            assert info.finished == (finished or not l.startswith(line))\n",
    "            any_vowels = any([RuAlphabetInfo.get_ru_letter_type(ch) == LetterType.VOWEL for ch in word])\n",
    "            assert tester.is_rhyme(info, info) == (any_vowels if info.finished else None)\n",
    "    \n",
    "    for line1, line2, is_rhyme in [('серая корова', 'не очень здорова', True),\n",
    "                                   ('молока много', 'не мало', False),\n",
    "                                   ('и играть', 'и скакать', False),\n",
    "                                   ('и играть', 'не играть', True)]:\n",
    "        for l1 in augment_line(line1):\n",
    "            for l2 in augment_line(line2):\n",
    "                info1 = tester.extract_rhyme_info(get_reversed(l1))\n",
    "                info2 = tester.extract_rhyme_info(get_reversed(l2))\n",
    "                assert tester.is_rhyme(info1, info2) == is_rhyme\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHYME_DEBUG_PRINT = False # 1, 2 or 3 for more debug info\n",
    "\n",
    "class RhymeType(Enum):\n",
    "    SUFFIX = 1\n",
    "    WORD = 2\n",
    "\n",
    "    \n",
    "class ITranslationModel(object):\n",
    "    # Our model which we trained now is accessed via this interface\n",
    "\n",
    "    def make_initial_state(self, lines):\n",
    "        # one state per line (though format is model-depending)\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        raise Exception('Not implemented')\n",
    "    \n",
    "    def get_output_vocabulary(self):\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "\n",
    "class RhymeTranslator(object):\n",
    "    \n",
    "    def __init__(self, model: ITranslationModel, rupo_engine = None):\n",
    "        \n",
    "        self._model = model\n",
    "        self._out_voc = model.get_output_vocabulary()\n",
    "        \n",
    "        self._suffix_rhyme_tester = RuReversedSuffixRhymeTester()\n",
    "        self._word_rhyme_tester = RuReversedWordRhymeTester(rupo_engine = rupo_engine)\n",
    "    \n",
    "    \n",
    "    def _lines_to_model_lines(self, lines):\n",
    "        return list(map(get_reversed, lines))\n",
    "    \n",
    "    def _merge_tokens(self, line):\n",
    "        return line.replace('@@ ', '')\n",
    "    \n",
    "    def _model_tokens_to_model_line(self, output, eos_as_space):\n",
    "        [line] = self._out_voc.to_lines([output])\n",
    "        if eos_as_space and output[-1] == self._out_voc.eos_ix:\n",
    "            line += ' '\n",
    "        return self._merge_tokens(line)\n",
    "    \n",
    "    def _model_tokens_to_lines(self, outputs):\n",
    "        lines = self._out_voc.to_lines(outputs)\n",
    "        return [get_reversed(self._merge_tokens(l)) for l in lines]\n",
    "    \n",
    "    def _apply_softmax(self, logits, temperature):\n",
    "        \n",
    "        if temperature != 1:\n",
    "            if temperature < 1:\n",
    "                # Convert to 64-bit float to avoid overflows:\n",
    "                # Note: There will still be overflows for T < 0.03\n",
    "                logits = logits.astype(np.float64)\n",
    "            \n",
    "            logits /= temperature\n",
    "\n",
    "        np.exp(logits, out = logits)\n",
    "        logits /= logits.sum(axis = -1)[..., np.newaxis]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def translate_lines(self,\n",
    "                        lines,\n",
    "                        sample_temperature = 0, \n",
    "                        max_len = 100):\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        state = self._model.make_initial_state(model_lines)\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "            \n",
    "            if sample_temperature:\n",
    "                # Sample from softmax with temperature:\n",
    "                logits = self._apply_softmax(logits, sample_temperature)\n",
    "                next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "            else:\n",
    "                next_tokens = np.argmax(logits, axis = -1)\n",
    "            \n",
    "            outputs[:, t + 1] = next_tokens\n",
    "            finished |= next_tokens == self._out_voc.eos_ix\n",
    "            \n",
    "            if finished.sum() == len(lines):\n",
    "                break # Early exis if all lines finished\n",
    "                \n",
    "        return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _translate_lines_in_rhyme(self,\n",
    "                                  lines,\n",
    "                                  rhyme_tester,\n",
    "                                  sample_temperature,\n",
    "                                  max_len,\n",
    "                                  rhyme_test_counts,\n",
    "                                  max_total_rhyme_tests):\n",
    "        # rhyme_test_counts = (2, 3, 4) means that first token is variated 2 variants\n",
    "        # second is variated 3 variants and third is variated 4 variants\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        initial_state = self._model.make_initial_state(model_lines)\n",
    "        initial_states = [[data[i: i+1] for data in initial_state] for i in range(len(lines))]\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 1), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "        \n",
    "        # Rhyme states:\n",
    "        # * True - rhyme found,\n",
    "        # * False - there is no rhyme,\n",
    "        # * None - not sure yet\n",
    "        rhyme_state = None\n",
    "        \n",
    "        GenState = namedtuple('GenState', ['state', 'toks', 'prob', 'next_states'])\n",
    "        \n",
    "        gen_states = [GenState(state,\n",
    "                               toks = [self._out_voc.bos_ix],\n",
    "                               prob = 1,\n",
    "                               next_states = [None] * rhyme_test_counts[0]) for state in initial_states]\n",
    "        \n",
    "        def fill_next_states(gen_state, t):\n",
    "            \n",
    "            last_state = t == len(rhyme_test_counts)\n",
    "            if last_state:\n",
    "                assert gen_state.next_states is None\n",
    "                line_last_gen_states.append(gen_state)\n",
    "                return\n",
    "                \n",
    "            test_count = rhyme_test_counts[t]\n",
    "            assert test_count == len(gen_state.next_states)\n",
    "            \n",
    "            state, logits = self._model.get_next_state_and_logits(gen_state.state, [gen_state.toks])\n",
    "            [probs] = self._apply_softmax(logits, temperature = 1) # TODO set temperature\n",
    "            \n",
    "            best_line_tokens = np.argpartition(probs, kth = -test_count, axis = -1)[-test_count:]\n",
    "            best_line_token_probs = probs[best_line_tokens]\n",
    "            \n",
    "            for i in range(test_count):\n",
    "                next_gen_state = GenState(state,\n",
    "                                          toks = gen_state.toks + [best_line_tokens[i]],\n",
    "                                          prob = gen_state.prob * best_line_token_probs[i],\n",
    "                                          next_states = [None] * rhyme_test_counts[t + 1]\n",
    "                                                        if t + 1 < len(rhyme_test_counts)\n",
    "                                                        else None)\n",
    "                gen_state.next_states[i] = next_gen_state\n",
    "                fill_next_states(next_gen_state, t + 1)\n",
    "            \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating {} x {} states... ***'.format(len(lines), rhyme_test_counts)) # DEBUG\n",
    "        last_gen_states = []\n",
    "        for gen_state in gen_states:\n",
    "            line_last_gen_states = []\n",
    "            fill_next_states(gen_state, t = 0)\n",
    "            last_gen_states.append(line_last_gen_states)\n",
    "            \n",
    "        # by this moment we have tree-structure (stored in last_gen_states) for each line\n",
    "            \n",
    "        assert [len(line_last_gen_states) == np.prod(rhyme_test_counts) for line_last_gen_states in last_gen_states]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 3:\n",
    "            for i, line_last_gen_states in enumerate(last_gen_states):\n",
    "                print('*** DEBUG: Line {} suffixes: ***'.format(i + 1)) # DEBUG\n",
    "                for line_last_gen_state in line_last_gen_states:\n",
    "                    suffix = get_reversed(self._model_tokens_to_model_line(line_last_gen_state.toks,\n",
    "                                                                           eos_as_space = True))\n",
    "                    print('*** DEBUG:  line {}: \"{}\" ***'.format(i + 1, suffix)) # DEBUG\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Generating state pairs... ***') # DEBUG\n",
    "        assert len(lines) == 2\n",
    "        last_gen_state_pairs = []\n",
    "        for line_1_last_gen_state in last_gen_states[0]:\n",
    "            for line_2_last_gen_state in last_gen_states[1]:\n",
    "                \n",
    "                pair = (line_1_last_gen_state.prob * line_2_last_gen_state.prob,\n",
    "                        line_1_last_gen_state,\n",
    "                        line_2_last_gen_state)\n",
    "                last_gen_state_pairs.append(pair)\n",
    "        # pair is actually a triple: prob, state1, state2\n",
    "                \n",
    "        last_gen_state_pairs.sort(key = lambda t: t[0], reverse = True)\n",
    "        if max_total_rhyme_tests:\n",
    "            last_gen_state_pairs = last_gen_state_pairs[:max_total_rhyme_tests]\n",
    "        \n",
    "        if RHYME_DEBUG_PRINT >= 2:\n",
    "            print('*** DEBUG: Testing state pairs... ***') # DEBUG\n",
    "        for _, line_1_last_gen_state, line_2_last_gen_state in last_gen_state_pairs:\n",
    "            \n",
    "            state = [np.concatenate((a, b), axis = 0)\n",
    "                     for a, b in zip(line_1_last_gen_state.state, line_2_last_gen_state.state)]\n",
    "            \n",
    "            outputs[0, :len(rhyme_test_counts) + 1] = line_1_last_gen_state.toks\n",
    "            outputs[1, :len(rhyme_test_counts) + 1] = line_2_last_gen_state.toks\n",
    "            \n",
    "            rhyme_state = None\n",
    "            \n",
    "            def update_rhyme_state(t):\n",
    "                # it will say whether we have a rhyme currently\n",
    "                nonlocal rhyme_state\n",
    "                \n",
    "                line_1 = self._model_tokens_to_model_line(outputs[0, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                line_2 = self._model_tokens_to_model_line(outputs[1, :t + 1],\n",
    "                                                          eos_as_space = True)\n",
    "                info_1 = rhyme_tester.extract_rhyme_info(line_1)\n",
    "                info_2 = rhyme_tester.extract_rhyme_info(line_2)\n",
    "                rhyme_state = rhyme_tester.is_rhyme(info_1, info_2)\n",
    "            \n",
    "            update_rhyme_state(len(rhyme_test_counts))\n",
    "            if rhyme_state == False:\n",
    "                continue\n",
    "                \n",
    "            finished.fill(False)\n",
    "\n",
    "            # And now the same generation function like in our model but with checking rhymes\n",
    "            for t in range(len(rhyme_test_counts), max_len):\n",
    "\n",
    "                state, logits = self._model.get_next_state_and_logits(state, outputs[:, :t + 1])\n",
    "\n",
    "                if sample_temperature:\n",
    "                    # Sample from softmax with temperature:\n",
    "                    logits = self._apply_softmax(logits, sample_temperature)\n",
    "                    next_tokens = np.array([np.random.choice(len(probs), p = probs) for probs in logits])\n",
    "                else:\n",
    "                    next_tokens = np.argmax(logits, axis = -1)\n",
    "\n",
    "                outputs[:, t + 1] = next_tokens\n",
    "                finished |= next_tokens == self._out_voc.eos_ix\n",
    "                \n",
    "                if rhyme_state is None:\n",
    "                    update_rhyme_state(t)\n",
    "                    if rhyme_state == False:\n",
    "                        break\n",
    "\n",
    "                if finished.sum() == len(lines):\n",
    "                    break # Early exis if all lines finished\n",
    "\n",
    "            if rhyme_state != True:\n",
    "                continue\n",
    "            \n",
    "            if RHYME_DEBUG_PRINT:\n",
    "                print('*** DEBUG: Rhyme found! ***') # DEBUG\n",
    "            return self._model_tokens_to_lines(outputs)\n",
    "    \n",
    "        if RHYME_DEBUG_PRINT:\n",
    "            print('*** DEBUG: Failed to find rhyme. ***') # DEBUG\n",
    "        return self.translate_lines(lines,\n",
    "                                    sample_temperature,\n",
    "                                    max_len)\n",
    "    \n",
    "    \n",
    "    def translate_lines_with_rhyme(self,\n",
    "                                   lines,\n",
    "                                   rhyme_type = RhymeType.WORD,\n",
    "                                   sample_temperature = 0,\n",
    "                                   max_len = 100,\n",
    "                                   rhyme_test_counts = (10, 10),\n",
    "                                   max_total_rhyme_tests = 1000):\n",
    "        \n",
    "        if rhyme_type == RhymeType.SUFFIX:\n",
    "            rhyme_tester = self._suffix_rhyme_tester\n",
    "        elif rhyme_type == RhymeType.WORD:\n",
    "            rhyme_tester = self._word_rhyme_tester\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        translated = []\n",
    "        for pair_idx in range(len(lines) // 2):\n",
    "            \n",
    "            pair_lines = lines[pair_idx * 2 : (pair_idx + 1) * 2]\n",
    "            \n",
    "            translated += self._translate_lines_in_rhyme(pair_lines,\n",
    "                                                         rhyme_tester,\n",
    "                                                         sample_temperature,\n",
    "                                                         max_len,\n",
    "                                                         rhyme_test_counts,\n",
    "                                                         max_total_rhyme_tests)\n",
    "        \n",
    "        if len(lines) % 2 == 1:\n",
    "            translated.append(self.translate_lines(lines[-1:])[0])\n",
    "            \n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    \n",
    "    def __init__(self, name, hid_size, activ=tf.tanh,):\n",
    "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
    "        self.name = name\n",
    "        self.hid_size = hid_size # attention layer hidden units\n",
    "        self.activ = activ       # attention layer hidden nonlinearity\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # YOUR CODE - create layer variables\n",
    "            #<YOUR CODE>\n",
    "            self.linear_e = L.Dense(hid_size)\n",
    "            self.linear_d = L.Dense(hid_size)\n",
    "            self.linear_out = L.Dense(1)\n",
    "\n",
    "    def __call__(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Computes attention response and weights\n",
    "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
    "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
    "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "            - attn - attention response vector (weighted sum of enc)\n",
    "            - probs - attention weights after softmax\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # Compute logits\n",
    "            #<...>\n",
    "            logits_seq = self.linear_out(self.activ(self.linear_e(enc) + \\\n",
    "                                                    self.linear_d(dec)[:, tf.newaxis, :]))\n",
    "            logits_seq = tf.squeeze(logits_seq, axis = -1)\n",
    "            \n",
    "            # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
    "            # You may need tf.where\n",
    "            #<...>\n",
    "            \n",
    "            logits_seq = tf.where(inp_mask, logits_seq, tf.fill(tf.shape(logits_seq),\n",
    "                                                                -np.inf))\n",
    "            \n",
    "            # Compute attention probabilities (softmax)\n",
    "            probs = tf.nn.softmax(logits_seq) # <...>\n",
    "            \n",
    "            # Compute attention response using enc and probs\n",
    "            attn = tf.reduce_sum(probs[..., tf.newaxis] * enc, axis = 1) # <...>\n",
    "            \n",
    "            return attn, probs\n",
    "        \n",
    "class AttentiveModel(ITranslationModel):\n",
    "    \n",
    "    def __init__(self, filename, name = None, inp_voc = None, out_voc = None,\n",
    "                 emb_size = None, hid_size = None):\n",
    "        \n",
    "        if filename is None:\n",
    "            self.initialize(name, inp_voc, out_voc,\n",
    "                            emb_size, hid_size) #, attn_size)\n",
    "        else:\n",
    "            self.load(filename)\n",
    "    \n",
    "    \n",
    "    def initialize(self, name, inp_voc, out_voc,\n",
    "                   emb_size, hid_size): #, attn_size):\n",
    "        \n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        #self.attn_size = attn_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # YOUR CODE - define model layers\n",
    "            \n",
    "            # <...>\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            self.enc_lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            self.enc_lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hid_size,\n",
    "                                                                 forget_bias=1.0,\n",
    "                                                                 state_is_tuple = False)\n",
    "            #self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "\n",
    "            self.dec_start = L.Dense(hid_size)\n",
    "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
    "            self.dense = L.Dense(hid_size)\n",
    "            self.activ = tf.tanh\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            self.attention = AttentionLayer(name = 'attention',\n",
    "                                            #enc_size = None, # FIXME: Unused\n",
    "                                            #dec_size = None, # FIXME: Unused\n",
    "                                            #hid_size = attn_size)\n",
    "                                            hid_size = 2 * self.hid_size)\n",
    "            \n",
    "            # END OF YOUR CODE\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "            self.next_softmax = tf.nn.softmax(self.next_logits)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        \n",
    "        # Call to 'K.get_session()' runs variable initializes for\n",
    "        # all variables including ones initialized using\n",
    "        # 'tf.global_variables_initializer()' (at least for Keras\n",
    "        # 2.0.5) thus it have to be called once here or model weights\n",
    "        # will be rewritten after training e.g. when 'get_weights' is\n",
    "        # called.\n",
    "        K.get_session()\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        # <YOUR CODE>\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        inp_mask = infer_mask(inp, self.inp_voc.eos_ix, dtype = tf.bool)\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        with tf.variable_scope('enc0'):\n",
    "            #enc_seq, enc_last = tf.nn.dynamic_rnn(self.enc0,\n",
    "            #                                      inp_emb,\n",
    "            #                                      sequence_length = inp_lengths,\n",
    "            #                                      dtype = inp_emb.dtype)\n",
    "            ((enc_seq_fw,\n",
    "              enc_seq_bw),\n",
    "             (enc_last_fw,\n",
    "              enc_last_bw)) = tf.nn.bidirectional_dynamic_rnn(self.enc_lstm_fw_cell,\n",
    "                                                              self.enc_lstm_bw_cell,\n",
    "                                                              inp_emb,\n",
    "                                                              sequence_length = inp_lengths,\n",
    "                                                              dtype = inp_emb.dtype)\n",
    "        enc_seq = tf.concat((enc_seq_fw, enc_seq_bw), axis = -1)\n",
    "        dec_start = self.dec_start(enc_last_fw)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        #first_attn_probas = <...>\n",
    "        _, first_attn_probas = self.attention(enc_seq, dec_start, inp_mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        #first_state = [<...>, first_attn_probas]\n",
    "        first_state = [dec_start, enc_seq, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order\n",
    "        # that you've packed in encode\n",
    "        #[<...>, prev_attn_probas] = prev_state\n",
    "        [prev_dec, enc_seq, inp_mask, prev_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and attn probas given previous decoder state\n",
    "        # * use prev token embedding and attn response to update decoder states\n",
    "        # * (concatenate and feed into decoder cell)\n",
    "        # * predict logits\n",
    "        \n",
    "        # <APPLY_ATTENTION>\n",
    "        next_attn_response, next_attn_probas = self.attention(enc_seq, prev_dec, inp_mask)\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        dec_inputs = tf.concat([prev_emb, next_attn_response], axis = 1)\n",
    "        with tf.variable_scope('dec0'):\n",
    "            new_dec_out, new_dec_state = self.dec0(dec_inputs, prev_dec)\n",
    "        output_logits = self.logits(self.activ(self.dense(new_dec_out)))\n",
    "        #output_logits = self.logits(self.activ(new_dec_out))\n",
    "        \n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        #next_state = [<...>, next_attn_probas]\n",
    "        next_state = [new_dec_state, enc_seq, inp_mask, next_attn_probas]\n",
    "        return next_state, output_logits\n",
    "\n",
    "    \n",
    "    def compute_logits(self, inp, out, **flags):\n",
    "        \n",
    "        batch_size = tf.shape(inp)[0]\n",
    "\n",
    "        # Encode inp, get initial state\n",
    "        first_state = self.encode(inp) # <YOUR CODE HERE>\n",
    "\n",
    "        # initial logits: always predict BOS\n",
    "        first_logits = tf.log(tf.one_hot(tf.fill([batch_size], self.out_voc.bos_ix),\n",
    "                                         len(self.out_voc)) + 1e-30)\n",
    "\n",
    "        # Decode step\n",
    "        def step(prev_state, y_prev):\n",
    "            # Given previous state, obtain next state and next token logits\n",
    "            # <YOUR CODE>\n",
    "            next_dec_state, next_logits = self.decode(prev_state, y_prev)\n",
    "            return next_dec_state, next_logits # <...>\n",
    "\n",
    "        # You can now use tf.scan to run step several times.\n",
    "        # use tf.transpose(out) as elems (to process one time-step at a time)\n",
    "        # docs: https://www.tensorflow.org/api_docs/python/tf/scan\n",
    "\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        out = tf.scan(lambda a, y: step(a[0], y),\n",
    "                      elems = tf.transpose(out)[:-1],\n",
    "                      initializer = (first_state, first_logits))\n",
    "\n",
    "\n",
    "        # FIXME remove?\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        logits_seq = out[1] # <YOUR CODE>\n",
    "\n",
    "        # prepend first_logits to logits_seq\n",
    "        logits_seq = tf.concat((first_logits[tf.newaxis], logits_seq), axis = 0) #<...>\n",
    "\n",
    "        # Make sure you convert logits_seq from\n",
    "        # [time, batch, voc_size] to [batch, time, voc_size]\n",
    "        logits_seq = tf.transpose(logits_seq, perm = [1, 0, 2]) #<...>\n",
    "\n",
    "        return logits_seq\n",
    "\n",
    "    def compute_loss(self, inp, out, **flags):\n",
    "        \n",
    "        mask = infer_mask(out, out_voc.eos_ix)    \n",
    "        logits_seq = self.compute_logits(inp, out, **flags)\n",
    "\n",
    "        # Compute loss as per instructions above\n",
    "        # <YOUR CODE>\n",
    "\n",
    "        prob_seq = tf.nn.softmax(logits_seq)\n",
    "        out_one_hot = tf.one_hot(out, len(self.out_voc))\n",
    "\n",
    "        prob_seq_masked = tf.boolean_mask(prob_seq, mask)\n",
    "        out_one_hot_masked = tf.boolean_mask(out_one_hot, mask)\n",
    "        prob_seq_out = tf.boolean_mask(prob_seq_masked, out_one_hot_masked)\n",
    "        loss = tf.reduce_mean(-tf.log(prob_seq_out))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def make_initial_state(self, inp_lines):\n",
    "        return sess.run(self.initial_state, {self.inp: self.inp_voc.to_matrix(inp_lines)})\n",
    "    \n",
    "    def get_next_state_and_logits(self, state, outputs):\n",
    "        return sess.run([self.next_state, self.next_logits],\n",
    "                        {**dict(zip(self.prev_state, state)),\n",
    "                         self.prev_tokens: [out_i[-1] for out_i in outputs]})\n",
    "                         \n",
    "    def get_output_vocabulary(self):\n",
    "        return self.out_voc\n",
    "    \n",
    "    \n",
    "    def translate_lines(self, inp_lines, max_len=100):\n",
    "        \"\"\"\n",
    "        Translates a list of lines by greedily selecting most likely next token at each step\n",
    "        :returns: a list of output lines, a sequence of model states at each step\n",
    "        \"\"\"\n",
    "        state = self.make_initial_state(inp_lines)\n",
    "        outputs = [[self.out_voc.bos_ix] for _ in range(len(inp_lines))]\n",
    "        all_states = [state]\n",
    "        finished = [False] * len(inp_lines)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            state, logits = self.get_next_state_and_logits(state, outputs)\n",
    "            next_tokens = np.argmax(logits, axis=-1)\n",
    "            all_states.append(state)\n",
    "            for i in range(len(next_tokens)):\n",
    "                outputs[i].append(next_tokens[i])\n",
    "                finished[i] |= next_tokens[i] == self.out_voc.eos_ix\n",
    "        return self.out_voc.to_lines(outputs), all_states\n",
    "    \n",
    "    def dump(self, filename):\n",
    "        \n",
    "        values = {'name': self.name,\n",
    "                  'inp_voc': self.inp_voc,\n",
    "                  'out_voc': self.out_voc,\n",
    "                  'emb_size': self.emb_size,\n",
    "                  'hid_size': self.hid_size,\n",
    "                  #'attn_size': self.attn_size,\n",
    "                  'emb_inp_weights': self.emb_inp.get_weights(),\n",
    "                  'emb_out_weights': self.emb_out.get_weights(),\n",
    "                  #'enc0_weights': self.enc0.get_weights(),\n",
    "                  'enc_lstm_fw_cell_weights': self.enc_lstm_fw_cell.get_weights(),\n",
    "                  'enc_lstm_bw_cell_weights': self.enc_lstm_bw_cell.get_weights(),\n",
    "                  'dec0_weights': self.dec0.get_weights(),\n",
    "                  'dec_start_weights': self.dec_start.get_weights(),\n",
    "                  'dense_weights': self.dense.get_weights(),\n",
    "                  'logits_weights': self.logits.get_weights(),\n",
    "                  'attn__linear_e_weights': self.attention.linear_e.get_weights(),\n",
    "                  'attn__linear_d_weights': self.attention.linear_d.get_weights(),\n",
    "                  'attn__linear_out_weights': self.attention.linear_out.get_weights()}\n",
    "        pickle.dump(values, open(filename, 'wb'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            values = pickle.load(f)\n",
    "        self.initialize(values['name'], values['inp_voc'], values['out_voc'],\n",
    "                        values['emb_size'], values['hid_size']) #, values['attn_size'])\n",
    "        self.emb_inp.set_weights(values['emb_inp_weights'])\n",
    "        self.emb_out.set_weights(values['emb_out_weights'])\n",
    "        #self.enc0.set_weights(values['enc0_weights'])\n",
    "        self.enc_lstm_fw_cell.set_weights(values['enc_lstm_fw_cell_weights'])\n",
    "        self.enc_lstm_bw_cell.set_weights(values['enc_lstm_bw_cell_weights'])\n",
    "        self.dec0.set_weights(values['dec0_weights'])\n",
    "        self.dec_start.set_weights(values['dec_start_weights'])\n",
    "        self.dense.set_weights(values['dense_weights'])\n",
    "        self.logits.set_weights(values['logits_weights'])\n",
    "        self.attention.linear_e.set_weights(values['attn__linear_e_weights'])\n",
    "        self.attention.linear_d.set_weights(values['attn__linear_d_weights'])\n",
    "        self.attention.linear_out.set_weights(values['attn__linear_out_weights'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:80: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:80: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f9646a26160>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f9646a26160>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f9646a261d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f9646a261d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:87: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:87: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:142: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ee629692ad52>:142: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/isprovilkov/p2p/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "if 'model_loaded' in globals():\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Need to also recreate Rupo as it uses TensorFlow:\n",
    "    global_rupo_engine = make_rupo_engine()\n",
    "\n",
    "model_loaded = AttentiveModel(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovoc = model_loaded.get_output_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok = {i:token for token, i in zip(ovoc.token_to_ix.keys(),ovoc.token_to_ix.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = [id2tok[i] for i in sorted(list(id2tok))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rupo_engine.get_stresses(\"ереп\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens_unbpe = [token.replace(\"@\", '') for token in sorted_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables_tokens = [global_rupo_engine.get_stresses(token) for token in sorted_tokens_unbpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS = \"уёеэоаыяию\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vowels(word, idx):\n",
    "    before = 0\n",
    "    after = 0\n",
    "    for s in word[:idx]:\n",
    "        if s in VOWELS:\n",
    "            before += 1\n",
    "    for s in word[idx+1:]:\n",
    "        if s in VOWELS:\n",
    "            after += 1\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RhymeTranslator_F(object):\n",
    "    \n",
    "    def __init__(self, model: ITranslationModel, rupo_engine = None):\n",
    "        \n",
    "        self._model = model\n",
    "        self._out_voc = model.get_output_vocabulary()\n",
    "        \n",
    "        self._suffix_rhyme_tester = RuReversedSuffixRhymeTester()\n",
    "        self._word_rhyme_tester = RuReversedWordRhymeTester(rupo_engine = rupo_engine)\n",
    "    \n",
    "    \n",
    "    def _lines_to_model_lines(self, lines):\n",
    "        return list(map(get_reversed, lines))\n",
    "    \n",
    "    def _merge_tokens(self, line):\n",
    "        return line.replace('@@ ', '')\n",
    "    \n",
    "    def _model_tokens_to_model_line(self, output, eos_as_space):\n",
    "        [line] = self._out_voc.to_lines([output])\n",
    "        if eos_as_space and output[-1] == self._out_voc.eos_ix:\n",
    "            line += ' '\n",
    "        return self._merge_tokens(line)\n",
    "    \n",
    "    def _model_tokens_to_lines(self, outputs):\n",
    "        lines = self._out_voc.to_lines(outputs)\n",
    "        return [get_reversed(self._merge_tokens(l)) for l in lines]\n",
    "    \n",
    "    def _apply_softmax(self, logits, temperature):\n",
    "        \n",
    "        if temperature != 1:\n",
    "            if temperature < 1:\n",
    "                # Convert to 64-bit float to avoid overflows:\n",
    "                # Note: There will still be overflows for T < 0.03\n",
    "                logits = logits.astype(np.float64)\n",
    "            \n",
    "            logits /= temperature\n",
    "\n",
    "        np.exp(logits, out = logits)\n",
    "        logits /= logits.sum(axis = -1)[..., np.newaxis]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def translate_lines(self,\n",
    "                        lines,\n",
    "                        sample_temperature = 0, \n",
    "                        max_len = 100, \n",
    "                        rhythme_line=0):\n",
    "        \n",
    "        model_lines = self._lines_to_model_lines(lines)\n",
    "        # list len 4 with arrays\n",
    "        state = self._model.make_initial_state(model_lines)\n",
    "        eoses = np.array([self._out_voc.eos_ix for i in range(4)])\n",
    "        \n",
    "        outputs = np.empty((len(lines), max_len + 100), dtype = np.int64)\n",
    "        outputs[:, 0] = self._out_voc.bos_ix\n",
    "        finished = np.zeros((len(lines),), dtype = bool)\n",
    "        \n",
    "        previous_state = state\n",
    "        last_state = state\n",
    "        word_count = max_len\n",
    "        start=True\n",
    "        idx=1\n",
    "        t=0\n",
    "        \n",
    "        while t < word_count:\n",
    "            next_word = False\n",
    "            counter = 0\n",
    "            buffer = []\n",
    "            tokens_to_add = []\n",
    "            while not next_word:\n",
    "                counter += 1\n",
    "                print(idx)\n",
    "                \n",
    "                state, logits = self._model.get_next_state_and_logits(state, outputs[:, :(idx+len(tokens_to_add))])\n",
    "                \n",
    "                if sample_temperature:\n",
    "                    # Sample from softmax with temperature:\n",
    "                    logits = self._apply_softmax(logits, sample_temperature)\n",
    "                    next_tokens = np.array([np.random.choice(len(probs), p=probs) for probs in logits])\n",
    "                else:\n",
    "                    next_tokens = np.argmax(logits, axis=-1)\n",
    "                \n",
    "                cur_token = sorted_tokens[next_tokens[rhythme_line]] # str\n",
    "                \n",
    "                condition = False# start new word\n",
    "                if len(buffer) > 0:\n",
    "                    condition = (cur_token[-1] != '@' and not start and buffer[-1][-1] != '@')\n",
    "                else:\n",
    "                    condition = (cur_token[-1] != '@' and not start)\n",
    "                if cur_token == \"_EOS_\":\n",
    "                    state = previous_state\n",
    "                    tokens_to_add = []\n",
    "                    continue\n",
    "                \n",
    "                if condition: \n",
    "                    print(\"cur\", cur_token)\n",
    "                    print(\"buf\", buffer)\n",
    "                    word = get_reversed(' '.join(buffer).replace('@@ ', ''))\n",
    "                    buffer = []\n",
    "                    stress = global_rupo_engine.get_stresses(word)\n",
    "                    #a = np.array(tokens_to_add + [np.array([self._out_voc.eos_ix] * 4)])  \n",
    "                    if len(stress) == 0:\n",
    "                        state = previous_state\n",
    "                        tokens_to_add = []\n",
    "                        continue\n",
    "                        \n",
    "                    stress = stress[0]\n",
    "                    before, after = count_vowels(word, stress)\n",
    "                    #if (before > 1) or (after > 0) or (before + after == 0):\n",
    "                    if (after > 0) or (before + after == 0):\n",
    "                        state = previous_state\n",
    "                        tokens_to_add = []\n",
    "                    else:\n",
    "                        for i in range(len(tokens_to_add)):\n",
    "                            print(\"ID\", idx+i)\n",
    "                            outputs[:, idx + i] = tokens_to_add[i]\n",
    "                        print(word)\n",
    "                        idx += len(tokens_to_add)\n",
    "                        tokens_to_add = []\n",
    "                        #outputs[:, t + 1] = next_tokens\n",
    "                        next_word=True\n",
    "                        previous_state = last_state\n",
    "                        t += 1\n",
    "                        \n",
    "                        finished |= outputs[:, idx-1] == self._out_voc.eos_ix\n",
    "                    \n",
    "                        if finished.sum() == len(lines):\n",
    "                            break \n",
    "                else:\n",
    "                    buffer.append(cur_token)\n",
    "                    tokens_to_add.append(next_tokens)\n",
    "                    outputs[:, idx+len(tokens_to_add) - 1] = next_tokens\n",
    "                    last_state = state\n",
    "                    start = False\n",
    "                    \n",
    "                # Early exis if all lines finished\n",
    "        outputs[:, idx] = self._out_voc.eos_ix\n",
    "        print(outputs)\n",
    "        return self._model_tokens_to_lines(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = RhymeTranslator_F(model_loaded, global_rupo_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur худзов\n",
      "buf ['худзов', 'яах@@', 'ут@@', 'У', ',@@', 'ухудзов', 'о@@', 'П', '!@@', 'аз@@', '-@@', 'у@@', '-@@', 'сп@@', 'С', '?@@', 'т@@', 'авон@@', 'ри@@', 'М']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur мохудзов\n",
      "buf ['!@@', 'худзов', 'йыт@@', 'ле@@', 'П']\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur худзов\n",
      "buf ['.@@', 'худзов', 'яащ@@', 'олг@@', 'о@@', 'П', ',@@', 'худзов', 'то@@', 'В', '!@@', 'худзов', ',@@', 'б@@', 'ы@@', 'Д']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur ухудзов\n",
      "buf [',@@', 'худзов', 'алин@@', 'лоп@@', 'а@@', 'Н', ',@@', 'худзов', 'ьс@@', 'яач@@', 'ь@@', 'ка@@', 'Б', '...@@', 'имазалг', 'нав@@', 'и@@', 'Ж']\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur худзов\n",
      "buf ['!@@', 'коретев', 'йыв@@', 'инрап', '.@@', 'худзов', 'ю@@', 'имузеб', 'н@@', 'О']\n",
      "1\n",
      "cur –\n",
      "buf []\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur -\n",
      "buf ['…@@', 'ыв@@', 'ыр@@', 'кс@@', 'В', '...@@', 'титс@@', 'и@@', 'Ч']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur В\n",
      "buf ['!@@', 'еинещущо', 'еон@@', 'ра@@', 'укнитрак', 'удюс@@', 'во@@', 'П', '...@@', 'ехудзов']\n",
      "1\n",
      "cur худзов\n",
      "buf []\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur В\n",
      "buf ['.@@', 'худзов', 'йынж@@', 'он@@', 'е@@', 'Щ', '...@@', 'худзов', 'йынж@@', 'отч@@', 'и@@', 'Н', ',@@', 'худзов', 'йынн@@', 'яв@@', 'хулс']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "cur худзов\n",
      "buf [',@@', 'худзов', 'йонв@@', 'ыр@@', 'п@@', 'В']\n",
      "1\n",
      "1\n",
      "1\n",
      "cur йынженед\n",
      "buf [',@@', 'худзов']\n",
      "ID 1\n",
      "ID 2\n",
      "воздух,\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ан\n",
      "buf ['йин@@', 'евс@@', 'О', 'йы@@', 'ле@@', 'ту@@', 'О', 'юат@@', 'яд@@', 'у@@', 'Г', 'йон@@', 'ьли@@', 'са@@', 'З', 'йич@@', 'уго@@', 'М', 'йы@@', 'ма@@', 'С', 'юат@@', 'а@@', 'М', 'йынч@@', 'ыб@@', 'О']\n",
      "3\n",
      "cur И\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['яатс@@', 'ич@@', 'О', 'йын@@', 'ьли@@', 'са@@', 'З', 'ме@@', 'Ч', 'йок@@', 'п@@', 'арт@@', 'С', 'йынн@@', 'юл@@', 'ли@@', 'Ф', 'йыл@@', 'и@@', 'М', 'ок@@', 'Э', 'мед@@', 'Ж', 'ьшеав@@', 'иг@@', 'ин@@', 'до@@', 'П', 'йыл@@', 'хяр@@', 'Д', 'йы@@', 'ма@@', 'С', 'яащ@@', 'сед@@', 'ес@@', 'он@@', 'и@@', 'С']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur А\n",
      "buf ['йик@@', 'ела@@', 'Д', 'ш@@', 'а@@', 'Н', 'йикс@@', 'в@@', 'ти@@', 'ре@@', 'Г', 'йик@@', 'отс@@', 'е@@', 'Ж', 'ьше@@', 'яле@@', 'п@@', 'У', 'йиж@@', 'ев@@', 'С', 'йикс@@', 'ровт@@', 'ап@@', 'са@@', 'Р', 'еом@@', 'илодоерпен', 'ьсед@@', 'З', 'еищ@@', 'я@@', 'бе@@', 'Д', 'я@@', 'не@@', 'М', 'йы@@', 'лер@@', 'огаз', 'лап@@', 'са@@', 'З', 'йын@@', 'ма@@', 'зе@@', 'Н', 'яан@@', 'тол@@', 'П', 'ьт@@', 'ырк@@', 'т@@', 'О', 'йын@@', 'ме@@', 'Т', 'йон@@', 'ме@@', 'Т', 'йын@@', 'ьли@@', 'С', 'йищюян@@', 'яь@@', 'п@@', 'О', 'акит@@', 'ав@@', 'ра@@', 'Г', 'йын@@', 'ре@@', 'Ч', 'йы@@', 'ма@@', 'С', 'ло@@', 'П', 'йыб@@', 'ал@@', 'С', 'юущ@@', 'я@@', 'П', 'ьчеж@@', 'а@@', 'З', 'йокс@@', 'уребаз']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur С\n",
      "buf ['ы@@', 'Т', 'йы@@', 'ле@@', 'м@@', 'С', 'йищя@@', 'от@@', 'са@@', 'Н', 'йыл@@', 'хял@@', 'С', 'я@@', 'рето@@', 'П', 'йынч@@', 'ясти@@', 'о@@', 'С', 'йикс@@', 'ле@@', 'Б', 'йыл@@', 'от@@', 'умеч', ',@@', 'нос', 'ясьт@@', 'от@@', 'елб@@', 'О', 'йыл@@', 'з@@', 'и']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur хиксром\n",
      "buf ['еынч@@', 'осе@@', 'П', 'йиш@@', 'ьли@@', 'С', 'удюс@@', 'во@@', 'П', 'йытс@@', 'и@@', 'Ч', 'яащ@@', 'он@@', 'ок@@', 'О', 'яащ@@', 'я@@', 'дор@@', 'б@@', 'О', '\"@@', 'ьтетел@@', 'ан', 'ил@@', 'одан', 'ьсед@@', 'З']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur йишсыв\n",
      "buf ['юу@@', 'ка@@', 'Т', 'йин@@', 'ма@@', 'С', 'йон@@', 'муш', 'до@@', 'П', 'ы@@', 'Т', 'еынч@@', 'ыб@@', 'О', 'йы@@', 'ме@@', 'Т', 'йыннё@@', 'лбюл@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['йын@@', 'рев@@', 'о@@', 'С', 'йыл@@', 'п@@', 'са@@', 'Л', 'йещюав@@', 'илоп@@', 'т@@', 'О', 'ы@@', 'Т', 'йон@@', 'яь@@', 'Д', 'ьвон@@', 'В', 'йынн@@', 'иля@@', 'ув@@', 'З', 'йин@@', 'о@@', 'ил@@', 'и@@', 'М', 'яан@@', 'ражоп']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur –\n",
      "buf ['яак@@', 'ы@@', 'Д', 'ю@@', 'ор@@', 'зо@@', 'В', 'йыннеж@@', 'ил@@', 'О', 'ьвон@@', 'В', 'йиш@@', 'п@@', 'илир@@', 'П', 'ьтетел@@', 'з@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['йич@@', 'у@@', 'удоп', ',@@', 'гю', 'т@@', 'от@@', 'Э', 'удюс@@', 'во@@', 'П', 'яам@@', 'у@@', 'Л', 'йын@@', 'ьли@@', 'С', 'лит@@', 'пот@@', 'В', 'йынч@@', 'он@@', 'ор@@', 'ха@@', 'К']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['йич@@', 'илзар', 'окьл@@', 'от@@', 'С', 'я@@', 'ьвон@@', 'ы@@', 'йищюян@@', 'яь@@', 'п@@', 'О', 'то@@', 'В', 'е@@', 'ок@@', 'к@@', 'Э', 'ьш@@', 'етс@@', 'я@@', '\"', 'от@@', 'Ч', 'мед@@', 'ил@@', 'уб@@', 'О', 'еыньл@@', 'ы@@', 'Д', 'йыб@@', 'ал@@', 'С', 'йынн@@', 'ор@@', 'ох@@', 'ол@@', 'З', 'йы@@', 'ма@@', 'С', 'йынч@@', 'ор@@', 'У', 'йын@@', 'ьли@@', 'С', 'йын@@', 'ло@@', 'П', 'йыл@@', 'ксут', 'з@@', 'И', 'йынч@@', 'И', 'т@@', 'О', 'ьвон@@', 'В', 'яав@@', 'ыб@@', 'ы@@', 'В', 'йищюян@@', 'яь@@', 'рав@@', 'Ш', 'йон@@', 'ра@@', 'йон@@', 'б@@', 'а@@', 'Б', 'яан@@', 'дох@@', 'сов@@', 'ёсв']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur А\n",
      "buf ['йищя@@', 'ро@@', 'Г', 'у@@', '-@@', 'у@@', 'Ф', 'ра@@', 'уг@@', 'и@@', 'Д', 'яан@@', 'ш@@', 'ара@@', 'К', 'еовок@@', 'тс@@', 'орд@@', 'ру@@', 'Д', 'йаз@@', 'ре@@', 'Т', 'ка@@', '-@@', 'о@@', '-@@', 'О']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur -\n",
      "buf ['ущ@@', 'икш@@', 'ып@@', 'С', 'яан@@', 'нелв@@', 'алсор@@', 'П', 'йынч@@', 'кес@@', 'от@@', 'У', 'ущ@@', 'И', 'ы@@', 'Т']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur тедж\n",
      "buf ['йынневтс@@', 'ещ@@', 'уж@@', 'ив@@', 'ен@@', 'ти@@', 'С', 'яащ@@', 'ьд@@', 'я@@', 'С', 'йон@@', 'ьли@@', 'С', 'ыт@@', 'ак@@', 'ра@@', 'Н']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur мибюл\n",
      "buf ['йын@@', 'а@@', 'З', 'йыннёдж@@', 'о@@', 'М', 'е@@', 'од@@', 'оло@@', 'М', 'йы@@', 'ма@@', 'С', 'йищюадж@@', 'рев@@', 'ере@@', 'П']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur йикьнелам\n",
      "buf ['итс@@', 'о@@', 'П']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ...\n",
      "buf ['ы@@', 'Т', 'йыл@@', 'им@@', 'С', 'яак@@', 'те@@', 'ал@@', 'П', 'йыв@@', 'оз@@', 'У', 'йын@@', 'сед@@', 'у@@', 'Ч']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['от@@', 'Э', 'йонч@@', 'ил@@', 'од@@', 'ж@@', 'ене@@', 'онтсолажзеб']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur онволс\n",
      "buf ['лахе@@', 'ъ@@', 'р@@', 'У', 'йыв@@', 'яс@@', 'ло@@', 'Х', 'йыннёщ@@', 'олг@@', 'о@@', 'П', 'ы@@', 'Т', 'йыл@@', 'п@@', 'е@@', 'Т', 'йынт@@', 'аг@@', 'о@@', 'Б', 'илад@@', 'арт@@', 'С']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur йиктуж\n",
      "buf ['еон@@', 'сед@@', 'у@@', 'Ч', 'я@@', 'ле@@', 'Ж', 'уч@@', 'ал@@', 'П', 'ы@@', 'Т']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['йиш@@', 'в@@', 'елор@@', 'о@@', 'К', 'йыл@@', 'и@@', 'М', 'йиг@@', 'оне@@', 'н@@', 'Г']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur худзов\n",
      "buf ['мед@@', 'ео@@', 'П', 'ка@@', 'К', 'ясйишв@@', 'ид@@', 'ео@@', 'П', 'йыт@@', 'аж@@', 'т@@', 'О', 'йыб@@', 'ал@@', 'С', 'яан@@', 'и@@', 'П', 'йикс@@', 'арв@@', 'з@@', 'н@@', 'ил@@', 'Г', 'т@@', 'ув@@', 'ро@@', 'С', 'йы@@', 'тох@@', 'я@@', 'В', 'йынч@@', 'оле@@', 'М', 'яатс@@', 'ите@@', 'М']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur йыннещявс\n",
      "buf ['йынн@@', 'яьл@@', 'их@@', 'о@@', 'О', 'йик@@', 'ле@@', 'Ж', 'йыл@@', 'ёж@@', 'юур@@', 'юьтремс']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ан\n",
      "buf ['яан@@', 'те@@', 'ме@@', 'Г', 'йищю@@', 'ати@@', 'зо@@', 'П', 'йош@@', 'ьло@@', 'Б', ',@@', '-@@', 'ле@@', 'Х', 'ясьтамин@@', 'С', 'йищ@@', 'арв@@', 'зе@@', 'Б', 'йыл@@', 'х@@', 'о@@', 'Д', 'йынн@@', 'ярд', 'а@@', 'Н', 'йынн@@', 'лет@@', 'кич@@', 'з@@', 'опаз']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['йы@@', 'ле@@', 'Т', 'лахе@@', 'У', 'йын@@', 'ьли@@', 'С', 'йищя@@', 'ту@@', 'Т', 'ьсетиж@@', 'ре@@', 'Д', 'йик@@', 'об@@', 'ул@@', 'Г', 'йищя@@', 'п@@', 'И', 'йыньлети@@', 'п@@', 'елс@@', 'О', 'ьт@@', 'я@@', 'то@@', 'Д', 'йон@@', 'те@@', 'Л', 'ьт@@', 'са@@', 'Ч', 'ш@@', 'а@@', 'Н', 'йы@@', 'ле@@', 'м@@', 'уз@@', 'а@@', 'Р', 'яат@@', 'ыб@@', 'о@@', 'Д', 'еын@@', 'кип', 'е@@', 'Т', 'йынч@@', 'у@@', 'ло@@', 'ми@@', 'п@@', 'и@@', 'В', 'йын@@', 'ьли@@', 'С', 'икит@@', 'авон@@', '-@@', 'етс@@', 'ар']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьшевоз\n",
      "buf ['йикс@@', 'ме@@', 'уж', 'йаш@@', 'ул@@', 'С', 'ьшеав@@', 'ырк@@', 'С', 'ок@@', 'ела@@', 'Д', 'е@@', 'кар@@', 'з@@', 'ир@@', 'П', 'ясте@@', 'ах', 'ы@@', 'Т', 'йонч@@', 'об@@', 'а@@', 'З', 'яатс@@', 'ы@@', 'Р', 'йищюян@@', 'яь@@', 'п@@', 'О', 'йын@@', 'сед@@', 'у@@', 'Ч', 'икс@@', 'з@@', 'ы@@', 'Б', 'йын@@', 'са@@', 'Ч', 'теч@@', 'ы@@', 'Б']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur кинзов\n",
      "buf ['йынч@@', 'ыб@@', 'О', 'йом@@', 'рут@@', 'Ш']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur Я\n",
      "buf ['ти@@', 'те@@', 'Л', 'ту@@', 'Т', 'е@@', 'Н']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur отсорп\n",
      "buf ['йыт@@', 'лё@@', 'Ж', 'яан@@', 'мё@@', 'Т', 'йов@@', 'т@@', 'О', 'ы@@', 'Т', 'яан@@', 'п@@', 'ур@@', 'К', 'йонч@@', 'ыб@@', 'О', 'з@@', 'ере@@', 'Ч', 'йатс@@', 'уктешер', 'а@@', 'З', 'йич@@', 'илзар', 'мяр@@', 'П', 'ееш@@', 'ор@@', 'о@@', 'Х', 'йон@@', 'а@@', 'кинд@@', 'зар@@', 'П']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur янем\n",
      "buf ['еынн@@', 'яр@@', 'У']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['ьт@@', 'ту@@', 'умен', 'ареч@@', 'В', 'йынч@@', 'е@@', 'В', 'ю@@', 'ом@@', 'ру@@', 'Б', 'йыл@@', 'сор@@', 'ы@@', 'З']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur О\n",
      "buf ['яат@@', 'те@@', 'лер@@', 'зо@@', 'П', 'йынч@@', 'амор@@', 'Г', 'яав@@', 'ысарб@@', 'т@@', 'О', 'йин@@', 'но@@', 'С', 'ме@@', 'И']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['ьт@@', 'ариб@@', 'о@@', 'С', 'яах@@', 'етйувтс@@', 'а@@', 'Р', 'теав@@', 'овтснешревос', 'ьж@@', 'ет@@', 'мот', 'л@@', 'ырк@@', 'са@@', 'Р', 'йик@@', 'ела@@', 'Д', ',@@', 'о', ',@@', 'ман', 'ьти@@', 'Ж', 'йыл@@', 'иж', 'ьт@@', 'ырк@@', 'са@@', 'Р', 'ю@@', 'ов@@', 'С', 'йынж@@', 'ивд@@', 'а@@', 'З']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur акмудыв\n",
      "buf ['йоме@@', 'Н', 'еищ@@', 'я@@', 'лешов', 'яувтс@@', 'ву@@', 'Ч', 'йынн@@', 'утс@@', 'И', 'йищ@@', 'я@@', 'от@@', 'са@@', 'Н', 'мын@@', 'мё@@', 'яин@@', 'и@@', 'Д', 'ьшит@@', 'ущ@@', 'мич@@', 'у@@', 'М']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur еноф\n",
      "buf ['йытс@@', 'ал@@', 'П', 'юу@@', 'ле@@', 'З', 'атя@@', 'ьшеажзеу', 'ы@@', 'Т', 'йон@@', 'тел@@', 'п@@', 'С', 'йом@@', 'зо@@', 'Х', 'ка@@', 'К']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur еметсис\n",
      "buf ['яат@@', 'ле@@', 'м@@', 'И', 'йик@@', 'р@@', 'яр@@', 'ве@@', 'Д', 'йонч@@', 'о@@', 'ев@@', 'У', 'от@@', 'ду@@', 'Б']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur литавхдоп\n",
      "buf ['йик@@', 'иле@@', 'В', 'яащ@@', 'ят@@', 'ик@@', 'ул@@', 'Б', 'йищ@@', 'о@@', 'од@@', 'аг@@', 'а@@', 'З', 'йищ@@', 'юял@@', 'п@@', 'ор@@', 'П', 'йов@@', 'ут@@', 'раз@@', 'А', 'йынн@@', 'ул@@', 'яз@@', 'о@@', 'Х']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur луник\n",
      "buf ['йыньлети@@', 'сапс', 'ы@@', 'Т', 'йич@@', 'об@@', 'а@@', 'Р', 'йы@@', 'п@@', 'орот@@', 'С']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьтаксыто\n",
      "buf ['йыннё@@', 'лбюл@@', 'В', 'яатс@@', 'е@@', 'М', 'яах@@', 'ыд@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur текап\n",
      "buf ['йын@@', 'ьли@@', 'С', 'еыр@@', 'ат@@', 'С', 'мён@@', 'ьл@@', 'уб@@', 'С', 'йонч@@', 'ю@@', 'ро@@', 'Г']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur О\n",
      "buf ['йон@@', 'те@@', 'М', 'им@@', 'д@@', 'Е', 'йы@@', 'ле@@', 'неж@@', 'ор@@', 'еинеп', 'еонч@@', 'о@@', 'К']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur -\n",
      "buf ['ы@@', 'Т', 'йонч@@', 'ора@@', 'П', 'йон@@', 'ве@@', 'а@@', 'Б', 'йищюян@@', 'яь@@', 'п@@', 'О']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur В\n",
      "buf ['атя@@', 'бе@@', 'п@@', 'са@@', 'Р', ',@@', 'тев@@', 'теелажос', 'кил@@', 'ы@@', 'огешв@@', 'т@@', 'О', 'йынч@@', 'арз@@', 'ор@@', 'П', 'ы@@', 'М', 'яан@@', 'б@@', 'ет@@', 'У', 'яан@@', 'ним@@', 'ал@@', 'Б', 'яак@@', 'ела@@', 'Д', 'йын@@', 'ве@@', 'Л']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ивбюл\n",
      "buf ['ьшеав@@', 'их@@', 'ап@@', 'а@@', 'З', 'йо@@', 'ма@@', 'С', 'йынн@@', 'алп@@', 'я@@', 'М', 'ьт@@', 'удзар', 'от@@', 'Э', 'йищя@@', 'Я', 'ьвон@@', 'В']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur илису\n",
      "buf ['яах@@', 'ыд@@', 'В', 'йыннел@@', 'пук', 'яак@@', 'яс@@', 'ы@@', 'Т', 'е@@', 'уш@@', 'ил@@', 'дан']\n",
      "3\n",
      "cur йижевс\n",
      "buf []\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur И\n",
      "buf ['то@@', 'З', 'йынн@@', 'липутс@@', 'С', 'йынч@@', 'ыб@@', 'О', 'их@@', 'ам@@', 'б@@', 'О', 'йынш@@', 'арт@@', 'С', 'йиш@@', 'я@@', 'не@@', 'М', 'йы@@', 'ма@@', 'С', 'то@@', 'В', 'еине@@', 'да@@', 'М', 'йищ@@', 'юувтс@@', 'ву@@', 'Ч', 'яах@@', 'ыдз@@', 'В', 'яаш@@', 'е@@', 'В', 'йынт@@', 'алп@@', 'ы@@', 'М', 'йынч@@', 'ил@@', 'ш@@', 'ор@@', 'о@@', 'Х']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur и\n",
      "buf ['ы@@', 'Т', 'йоме@@', 'ялгирп', 'то@@', 'В', 'йыннё@@', 'лбюл@@', 'В', 'то@@', 'ьтетел@@', 'б@@', 'О', 'ап@@', 'и@@', 'Т', 'йыннё@@', 'тян@@', 'ес@@', 'са@@', 'Р', 'мё@@', 'те@@', 'Л', 'ы@@', 'Т', 'йынн@@', 'ыс']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьшёанзу\n",
      "buf ['етиш@@', 'и@@', 'М', 'йик@@', 'ела@@', 'Д', 'яан@@', 'мё@@', 'а@@', 'З', 'а@@', 'З', 'еытс@@', 'сор@@', 'Б', 'ьтиле@@', 'Д', 'яам@@', 'иротвопен']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur как\n",
      "buf ['еик@@', 'тор@@', 'ере@@', 'К', 'йы@@', 'ма@@', 'С']\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "cur янем\n",
      "buf ['йен@@', 'до@@', 'П', 'йон@@', 'о@@', 'л@@', 'М', 'йон@@', 'м@@', 'У']\n",
      "3\n",
      "3\n",
      "3\n",
      "cur ьтуп\n",
      "buf ['ьт@@', 'иварто']\n",
      "ID 3\n",
      "ID 4\n",
      "отравить\n",
      "5\n",
      "5\n",
      "5\n",
      "cur ан\n",
      "buf [',@@', 'гю']\n",
      "5\n",
      "cur лидох\n",
      "buf []\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur ен\n",
      "buf ['йаж@@', 'орг@@', 'У', 'ясьтиш@@', 'им', 'хе@@', 'муб']\n",
      "5\n",
      "cur тичанз\n",
      "buf []\n",
      "5\n",
      "cur онжун\n",
      "buf []\n",
      "5\n",
      "cur и\n",
      "buf []\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur С\n",
      "buf ['х@@', 'И', ',@@', 'ала@@', 'Я', 'йаж@@', 'ом@@', 'е@@', 'Н', ',@@', 'ьтярет', ',@@', 'меинеж@@', 'олив@@', 'херт']\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur метаз\n",
      "buf ['тен@@', 'ат@@', 'С', 'анж@@', 'ло@@', 'Д', ',@@', 'ясьт@@', 'янд@@', 'о@@', 'П', 'туд@@', 'е@@', 'В', 'йаж@@', 'лод@@', 'ор@@', '-@@', 'ед@@', 'ув@@', 'Д']\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur енсеп\n",
      "buf [',@@', 'ьтя@@', 'ем@@', 'ы@@', 'В', 'ешьло@@', 'Б', 'ыб@@', 'от@@', 'Ч', ',@@', 'ьтете@@', 'Л', 'етйав@@', 'а@@', 'Д', 'теч@@', 'о@@', 'Х', 'ьдуб@@', 'ин@@', '-@@', 'йаж@@', 'лод@@', 'ор@@', 'П', 'йаж@@', 'ж@@', 'об@@', 'О', ',@@', 'еер@@', 'п@@', 'еч@@', 'О', 'ьтетел@@', 'орп']\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur енм\n",
      "buf [',@@', 'ьтя@@', 'п@@', 'а@@', 'З']\n",
      "Запять,\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "cur тежомоп\n",
      "buf [',@@', 'йор@@', 'гим']\n",
      "ID 5\n",
      "ID 6\n",
      "ID 7\n",
      "мигрой,\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur в\n",
      "buf ['йынд@@', 'юн@@', 'ох@@', 'и@@', 'В', 'еыннеш@@', 'ото']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur с\n",
      "buf ['до@@', 'П']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur йотэ\n",
      "buf ['от@@', 'Э']\n",
      "8\n",
      "cur йотэ\n",
      "buf []\n",
      "8\n",
      "cur йотэ\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur йынпелокилев\n",
      "buf ['йончин@@', 'тяп', 'а@@', 'З', 'йо@@', 'М']\n",
      "8\n",
      "cur аз\n",
      "buf []\n",
      "8\n",
      "cur епдоп\n",
      "buf []\n",
      "8\n",
      "cur имак\n",
      "buf []\n",
      "8\n",
      "cur сан\n",
      "buf []\n",
      "8\n",
      "cur аз\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur илажеб\n",
      "buf ['тев@@', 'С']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur Б\n",
      "buf ['йон@@', 'нёдж@@', 'о@@', 'Р', 'мынд@@', 'ус']\n",
      "8\n",
      "cur дан\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur яатсорп\n",
      "buf ['да@@', 'Н']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur о\n",
      "buf ['да@@', 'С', ',@@', 'ред@@', 'и@@', 'Л', 'илавовтс@@', 'еищ@@', 'отс@@', 'мобулог']\n",
      "8\n",
      "cur в\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur аз\n",
      "buf ['мыл@@', 'ш@@', 'ор@@', 'П', 'а@@', 'З']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur ад\n",
      "buf ['яаннеч@@', 'уналп', 'тело@@', 'ка@@', 'о@@', 'Г', 'уш@@', 'а@@', 'М', 'й@@', 'ут@@', 'нак@@', 'тун@@', 'атичирп']\n",
      "8\n",
      "cur гаш\n",
      "buf []\n",
      "8\n",
      "cur йомас\n",
      "buf []\n",
      "8\n",
      "cur алыб\n",
      "buf []\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur ел\n",
      "buf ['а@@', 'З', 'а@@', 'З']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur лыб\n",
      "buf ['а@@', 'З']\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur еьталп\n",
      "buf ['йыв@@', 'ре@@', 'Ж']\n",
      "8\n",
      "cur йынжав\n",
      "buf []\n",
      "8\n",
      "cur аз\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur ущи\n",
      "buf ['йыв@@', 'ибюл@@', '-@@', 'юьп', 'от@@', '-@@', 'отч']\n",
      "8\n",
      "cur с\n",
      "buf []\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "cur И\n",
      "buf [',@@', 'йош@@', 'ьло@@', 'Б']\n",
      "Большой,\n",
      "[[              0              12           30066           35893\n",
      "             2193               1               1               1\n",
      "                1               1            9241           21680\n",
      "             2112               1               1              95\n",
      "  140282874319616     25769803792 140282221821977  94730914536328\n",
      "  140297419205912 140282221821953     25769803792               1\n",
      "   94730914536760 140282221821980 140296133939568 140282221821953\n",
      "  140282671039280     68719476752              25  94730914536424\n",
      "  140297262202288  94729798680577     68719476752               1\n",
      "   94730914536520 140282221821980 140297418844400  94729798680577\n",
      "  140282671064544    184683593744               3  94730914536632\n",
      "   94730914536688  94730914536744 140282221821969 140282804981312\n",
      "  140282671065440 140282671065552    296352743440 140292087259472\n",
      "   94730914536592 140282221821969  94730782587712 140282671066224\n",
      "  140282671039568    352187318288 140297357426736  94730914536648\n",
      "  140282221821969  94730782587648 140282671040048 140282671040144\n",
      "     420906795024 140285717657904  94730914536704 140282221821968\n",
      "   94730914536464  94730914536504  94730914536560     68719476752\n",
      "                0               0 140282221821968  94730914536368\n",
      "   94730914536408  94730914536800     25769803792               0\n",
      "                0 140282221821968  94730914536272  94730914536312\n",
      "   94730914536856              16 140282221821974  94730914536872\n",
      "  140282870712600 140282870712720 140282870712840 140282671149104\n",
      "               16 140282221821954  94730914536256 140282671149328\n",
      "  140282671149440 140282671149552 140282870713200 140282870713320\n",
      "  140282870713440 140282671149664 140282870713560 140282870713680\n",
      "  140282671149776]\n",
      " [              0           26871           35809           54060\n",
      "            53765              81           15792           27298\n",
      "                1               1               1           39459\n",
      "                1           40402           23861           46663\n",
      "  140282871894904 140282871895024 140282671097000 140282671150448\n",
      "  140282671150560 140282671150672 140282671150784 140282671150896\n",
      "  140282671151008 140282671151120 140282671151232 140282671151344\n",
      "  140282671151456 140282671151568 140282671151680 140282671151792\n",
      "  140282671151904 140282671152016 140282671152128 140282671152240\n",
      "  140282671041296 140282671097104 140282671097208 140282671097312\n",
      "  140282671097416 140282671097520 140282671097624 140282671097728\n",
      "  140282671097832 140282671097936 140282671098040 140282671098144\n",
      "  140282671098248 140282671098352 140282671098456 140282671098560\n",
      "  140282671098664 140282671098768 140282671098872 140282671098976\n",
      "  140282671099080 140282671099184 140282671099288 140282671099392\n",
      "  140282671099496 140282671099600 140282671099704 140282871906352\n",
      "  140282871906456 140282871906560 140282871906664 140282871906768\n",
      "  140282871906872 140282871906976 140282871907080 140282871907184\n",
      "  140282871907288 140282871907392 140282671152352 140282671152464\n",
      "  140282671152576 140282671152688 140282671152800 140282671152912\n",
      "  140282671153024 140282871783472 140282871783584 140282671041392\n",
      "  140282871907496 140282871907600 140282871907704 140282871907808\n",
      "  140282871907912 140282871908016 140282871908120 140282871908224\n",
      "  140282871908328 140282871908432 140282871908536 140282871908640\n",
      "  140282871908744 140282871908848 140282871908952 140282871909056\n",
      "  140282871909160 140282871909264 140282871909368 140282871909472\n",
      "  140282871909576]\n",
      " [              0           13160            2204              59\n",
      "            14952           48207            9801            2112\n",
      "                1           11473           40904            2139\n",
      "            53838            2157               1           36886\n",
      "  140282871813080 140282871813184 140282871813288 140282871813392\n",
      "  140282871813496 140282871813600 140282871813704 140282871813808\n",
      "  140282871813912 140282871814016 140282871814120 140282871814224\n",
      "  140282871814328 140282871814432 140282871814536 140282871814640\n",
      "  140282871814744 140282871814848 140282871814952 140282871815056\n",
      "  140282871815160 140282871815264 140282871815368 140282871815472\n",
      "  140282871815576 140282871815680 140282871815784 140282871815888\n",
      "  140282871815992 140282871734320 140282871734424 140282871734528\n",
      "  140282871734632 140282871734736 140282871734840 140282871734944\n",
      "  140282871735048 140282871735152 140282871735256 140282871735360\n",
      "  140282871735464 140282871735568 140282871735672 140282871735776\n",
      "  140282871735880 140282871735984 140282871736088 140282871736192\n",
      "  140282871736296 140282871736400 140282871736504 140282871736608\n",
      "  140282871736712 140282871736816 140282871736920 140282871737024\n",
      "  140282871737128 140282871783696 140282871737232 140282871737336\n",
      "  140282871737440 140282871737544 140282871737648 140282871737752\n",
      "  140282671041584 140282871737856 140282871737960 140282871738064\n",
      "  140282871738168 140282871758896 140282871759000 140282871759104\n",
      "  140282871759208 140282871759312 140282671041680 140282671041776\n",
      "  140282671041872 140282871759416 140282671041968 140282671042064\n",
      "  140282671042160 140282671042256 140282671042352 140282671042448\n",
      "  140282871840816 140282871759520 140282871759624 140282871759728\n",
      "  140282871759832]\n",
      " [              0              81           44616           49404\n",
      "            15658              81           23842            9075\n",
      "                1           23979           47862            2109\n",
      "            35893              95           36071           29088\n",
      "  140282871841872 140282871841968 140282871842064 140282871842160\n",
      "  140282871842256 140282871842352 140282871842448 140282871842544\n",
      "  140282871842640 140282871760456 140282871783808 140282871783920\n",
      "  140282871784032 140282871784144 140282871784256 140282871784368\n",
      "  140282871784480 140282871784592 140282871784704 140282871842736\n",
      "  140282871760560 140282871760664 140282871760768 140282871760872\n",
      "  140282871760976 140282871761080 140282871761184 140282871761288\n",
      "  140282871761392 140282871761496 140282871761600 140282871761704\n",
      "  140282871761808 140282871761912 140282871762016 140282871762120\n",
      "  140282871762224 140282871762328 140282871762432 140282871842832\n",
      "  140282871762536 140282871762640 140282871762744 140282871930928\n",
      "  140282871931032 140282871931136 140282871931240 140282871931344\n",
      "  140282871931448 140282871931552 140282871931656 140282871784816\n",
      "  140282871784928 140282871785040 140282871785152 140282871785264\n",
      "  140282871785376 140282871785488 140282871785600 140282871785712\n",
      "  140282871785824 140282871785936 140282871786048 140282871786160\n",
      "  140282871786272 140282871786384 140282871786496 140282871931760\n",
      "  140282871931864 140282871786608 140282871786720 140282871786832\n",
      "  140282871931968 140282871786944 140282871787056 140282871932072\n",
      "  140282871932176 140282871932280 140282871932384 140282871932488\n",
      "  140282871932592 140282871932696 140282871932800 140282871932904\n",
      "  140282871933008 140282871933112 140282871933216 140282871787168\n",
      "  140282871787280]]\n",
      "Это шум!\n",
      "@@олги, которую я со бродил\n",
      "Весь ж**а все\n",
      "мигрой, отравить воздух,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "assert global_rupo_engine.is_rhyme('вещи', 'ветер') # This is how Rupo thinks\n",
    "\n",
    "lines = '''Your voice\n",
    "Called me outside the window\n",
    "Of uncontested summer all things raise\n",
    "seamless air'''.split('\\n')\n",
    "#lines = '''Two roads diverged in a yellow wood,\n",
    "#And sorry I could not travel both\n",
    "#And be one traveler, long I stood\n",
    "#And looked down one as far as I could\n",
    "#To where it bent in the undergrowth;\n",
    "#'''\n",
    "\n",
    "print('\\n'.join(translator.translate_lines(lines, sample_temperature=1.2, max_len=5, rhythme_line=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print()\n",
    "#print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "#                                                      rhyme_type = RhymeType.WORD,\n",
    "#                                                      sample_temperature=0.5,\n",
    "#                                                      rhyme_test_counts=(5, 5, 2),\n",
    "#                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) родным криком....\n",
    "1) пробьет душа окном,\n",
    "2) лето, стад растём\n",
    "3) мигрой, отравить воздух\n",
    "стальных лесов, монет, иметь!\n",
    "типаж дождями гонётся окном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad: \"\"\"Сквозь голоса.\n",
    "Я отчётлыняюсь за окном\n",
    "растем что-нибудь быстрее...\n",
    "Вновь воздух\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ты молчишь?\n",
    "моих поцелуев опишется окна\n",
    "огонь, когда-нибудь вставали,\n",
    "Далеко быть\n",
    "\n",
    "Свой голос,\n",
    "на улице ждёт.\n",
    "стальных лесов, монет, иметь!\n",
    "Отпахлый воздух"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-d410dee9113a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mSeeds\u001b[0m \u001b[0mspring\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbeauty\u001b[0m \u001b[0mbreedeth\u001b[0m \u001b[0mbeauty\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m Thou wast begot; to get it is thy duty.'''.split('\\n')\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
      "\u001b[0;32m<ipython-input-110-ebd17b201fc1>\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, lines, sample_temperature, max_len)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mfinished\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mnext_tokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poem2poem/ipython_usage_demos/utils.py\u001b[0m in \u001b[0;36mto_lines\u001b[0;34m(self, matrix, crop)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Poem2poem/ipython_usage_demos/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Torches are made to light, jewels to wear,\n",
    "Dainties to taste, fresh beauty for the use,\n",
    "Herbs for their smell, and sappy plants to bear;\n",
    "Things growing to themselves are growth’s abuse,\n",
    "Seeds spring from seeds, and beauty breedeth beauty;\n",
    "Thou wast begot; to get it is thy duty.'''.split('\\n')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))\n",
    "print()\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(5, 5, 2),\n",
    "                                                      max_total_rhyme_tests = 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он говорит о том, что этот ветер прорывается сквозь ночь,\n",
      "Ладно, мы уходим прочь.\n",
      "Крупное животное проходил через улицу,\n",
      "Нет, они так устали.\n"
     ]
    }
   ],
   "source": [
    "# Model internal translation function:\n",
    "lines = list(map(get_reversed,\n",
    "                 ['Let this wind blow through the night',\n",
    "                  'And we are going away',\n",
    "                  'The animal hasn\\'t crossed the street',\n",
    "                  'Because it was too tired']))\n",
    "translated = model_loaded.translate_lines(lines)[0]\n",
    "print('\\n'.join([get_reversed(line) for line in translated]).replace(' @@', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и был уверен.\n",
      "\"Человек\", я знаю. Грязное словечко.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Приведи своих друзей\n",
      "Это не так, весело, чтобы проиграть и притворяться.\n",
      "Мне стало скучно и быть уверенным в себе.\n",
      "\"Ну-ка\", я знаю непристойное слово,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines = '''Load up on guns and bring your friends\n",
    "It's fun to lose and to pretend\n",
    "She's over bored and self assured\n",
    "Oh no, I know a dirty word'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      rhyme_test_counts=(3, 10, 3),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Ваш сын вернулся к отцу.\n",
      "Не спрашивай меня, и все крошки.\n",
      "- это хорошо.\n",
      "Это неправильно.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он в том, что с сыном пришёл его отец.\n",
      "и спросила у детей.\n",
      "- это плохо.\n",
      "Это не плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Он вернулся с сыном с отцом.\n",
      "Попросила меня с ребёнком.\n",
      "- прекрасно.\n",
      "Это неправильно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Крошка сын к отцу пришел,\n",
    "и спросила кроха:\n",
    "— Что такое хорошо\n",
    "и что такое плохо?'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''Baby son to his father came\n",
    "and asked crumbs\n",
    "What is good\n",
    "what's wrong'''.split('\\n')\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 2500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Ответ от этих отцов, вот это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 5 variants of token at last position\n",
    "                                                      # 5 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 5, 3, 5),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "У меня нет секретов,\n",
      "Ох, слушать, детей,\n",
      "Приказ от отцов на этот ответ.\n",
      "Положила её в книге,\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет друзей,\n",
      "Ох, слушать, детей,\n",
      "Они смотрят на это.\n",
      "Я внесла всю книгу в этом,\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "У меня нет секретов,\n",
      "Послушай, мальчиков,\n",
      "Редента с этим ответом.\n",
      "Я внесла всю книгу в этом,\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''У меня секретов нет,\n",
    "слушайте, детишки,\n",
    "папы этого ответ\n",
    "помещаю в книжке.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''I have no secrets\n",
    "listen kids\n",
    "dads this answer\n",
    "I put it in the book'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ARGMAX----\n",
      "Он врывается в грязь и взволнован.\n",
      "Это эта коварная рубашка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_WORD/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Failed to find rhyme. ***\n",
      "Он врывается в грязь и взволнован.\n",
      "Это такая грязная футболка,\n",
      "так говорят:\n",
      "Ладно, это плохо.\n",
      "\n",
      "----RHYME_SUFFIX/ARGMAX----\n",
      "*** DEBUG: Rhyme found! ***\n",
      "*** DEBUG: Rhyme found! ***\n",
      "Наверное, в грязи, и я была взволнована.\n",
      "Это кожаная рубашкана,\n",
      "Это то, что здесь сказано:\n",
      "Очевидно.\n"
     ]
    }
   ],
   "source": [
    "RHYME_DEBUG_PRINT = 1\n",
    "\n",
    "lines_ru = '''Этот в грязь полез и рад.\n",
    "что грязна рубаха.\n",
    "Про такого говорят:\n",
    "он плохой, неряха.'''\n",
    "\n",
    "# Translated to English with YandexTranslate\n",
    "# And punctuation manually removed\n",
    "\n",
    "lines = '''This in the dirt and got excited\n",
    "that dirty shirt\n",
    "About this say\n",
    "he's bad sloppy'''.split('\\n')\n",
    "\n",
    "print('----ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines(lines)))\n",
    "print()\n",
    "print('----RHYME_WORD/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.WORD,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))\n",
    "print()\n",
    "print('----RHYME_SUFFIX/ARGMAX----')\n",
    "print('\\n'.join(translator.translate_lines_with_rhyme(lines,\n",
    "                                                      rhyme_type = RhymeType.SUFFIX,\n",
    "                                                      # 1 variants of token at last position\n",
    "                                                      # 20 variants at 'last minus 1' position\n",
    "                                                      # 2 variants at 'last minus 2' position\n",
    "                                                      rhyme_test_counts=(1, 20, 2),\n",
    "                                                      max_total_rhyme_tests = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
