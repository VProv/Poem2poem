{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import opustools_pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opus_reader = opustools_pkg.OpusRead([\"-d\", \"OpenSubtitles2018\", \"-s\", \"en\", \"-t\", \"ru\"\n",
    "#                                      ,\"-rd\", \"/srv/hd5/data/vprov/Poems/data/\"])\n",
    "                                      #,\"-wm\", \"moses\", \"-w\", \n",
    "                                     #\"/srv/hd5/data/vprov/Poems/data/c.clean.en,/srv/hd5/data/vprov/Poems/data/c.clean.ru\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT START HERE (BPE learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.001195669174194336\n",
      "100005 0.9053459167480469\n",
      "200010 1.8064923286437988\n",
      "300015 2.7129876613616943\n",
      "400020 3.621779203414917\n",
      "500025 4.536507606506348\n",
      "600030 5.449934482574463\n",
      "700035 6.354427814483643\n",
      "800040 7.272990703582764\n",
      "900045 8.191219568252563\n",
      "1000050 9.093032121658325\n",
      "1100055 9.999032497406006\n",
      "1200060 10.907984018325806\n",
      "1300065 11.814679861068726\n",
      "1400070 12.711447954177856\n",
      "1500075 13.613584041595459\n",
      "1600080 14.511685132980347\n",
      "1700085 15.410392999649048\n",
      "1800090 16.301660776138306\n",
      "1900095 17.19749903678894\n",
      "2000100 18.099979162216187\n",
      "2100105 18.993664026260376\n",
      "2200110 19.889615535736084\n",
      "2300115 20.777644634246826\n",
      "2400120 21.669100999832153\n",
      "2500125 22.565782070159912\n",
      "2600130 23.463003396987915\n",
      "2700135 24.356353998184204\n",
      "2800140 25.258469820022583\n",
      "2900145 26.148648023605347\n",
      "3000150 27.06009006500244\n",
      "3100155 27.965386867523193\n",
      "3200160 28.860894203186035\n",
      "3300165 29.76190757751465\n",
      "3400170 30.657077074050903\n",
      "3500175 31.54589557647705\n",
      "3600180 32.43953275680542\n",
      "3700185 33.33024525642395\n",
      "3800190 34.2247633934021\n",
      "3900195 35.10517144203186\n",
      "4000200 36.0043740272522\n",
      "4100205 36.895916223526\n",
      "4200210 37.80675506591797\n",
      "4300215 38.715675354003906\n",
      "4400220 39.649725914001465\n",
      "4500225 40.543015241622925\n",
      "4600230 41.45064353942871\n",
      "4700235 42.33576822280884\n",
      "4800240 43.241822481155396\n",
      "4900245 44.140838623046875\n",
      "5000250 45.03716564178467\n",
      "5100255 45.945656299591064\n",
      "5200260 46.83538508415222\n",
      "5300265 47.750202655792236\n",
      "5400270 48.655959129333496\n",
      "5500275 49.58573508262634\n",
      "5600280 50.51603817939758\n",
      "5700285 51.416555643081665\n",
      "5800290 52.32608485221863\n",
      "5900295 53.24762463569641\n",
      "6000300 54.17292070388794\n",
      "6100305 55.08116102218628\n",
      "6200310 55.98121237754822\n",
      "6300315 56.88816976547241\n",
      "6400320 57.808300733566284\n",
      "6500325 58.7203483581543\n",
      "6600330 59.626967430114746\n",
      "6700335 60.5335328578949\n",
      "6800340 61.44820857048035\n",
      "6900345 62.368130922317505\n",
      "7000350 63.27041029930115\n",
      "7100355 64.17006206512451\n",
      "7200360 65.07470870018005\n",
      "7300365 65.9945387840271\n",
      "7400370 66.9185893535614\n",
      "7500375 67.83062243461609\n",
      "7600380 68.73686146736145\n",
      "7700385 69.6355311870575\n",
      "7800390 70.55088496208191\n",
      "7900395 71.46460103988647\n",
      "8000400 72.37955284118652\n",
      "8100405 73.28943920135498\n",
      "8200410 74.19582343101501\n",
      "8300415 75.0874514579773\n",
      "8400420 75.9922366142273\n",
      "8500425 76.91027164459229\n",
      "8600430 77.83162212371826\n",
      "8700435 78.74720239639282\n",
      "8800440 79.6490364074707\n",
      "8900445 80.54999423027039\n",
      "9000450 81.44330859184265\n",
      "9100455 82.34695887565613\n",
      "9200460 83.26191115379333\n",
      "9300465 84.17744398117065\n",
      "9400470 85.07666635513306\n",
      "9500475 85.97803521156311\n",
      "9600480 86.88253736495972\n",
      "9700485 87.77089643478394\n",
      "9800490 88.68217015266418\n",
      "9900495 89.5968565940857\n",
      "10000500 90.54875564575195\n",
      "10100505 91.46301317214966\n",
      "10200510 92.35350751876831\n",
      "10300515 93.24637508392334\n",
      "10400520 94.14915561676025\n",
      "10500525 95.05345249176025\n",
      "10600530 95.9762601852417\n",
      "10700535 96.89323997497559\n",
      "10800540 97.81444096565247\n",
      "10900545 98.73713779449463\n",
      "11000550 99.63646745681763\n",
      "11100555 100.53747749328613\n",
      "11200560 101.43326091766357\n",
      "11300565 102.33705949783325\n",
      "11400570 103.25067114830017\n",
      "11500575 104.16932988166809\n",
      "11600580 105.08718681335449\n",
      "11700585 106.00440669059753\n",
      "11800590 106.9273886680603\n",
      "11900595 107.84834313392639\n",
      "12000600 108.75585174560547\n",
      "12100605 109.67516613006592\n",
      "12200610 110.5885751247406\n",
      "12300615 111.50983715057373\n",
      "12400620 112.42973613739014\n",
      "12500625 113.31211686134338\n",
      "12600630 114.21070766448975\n",
      "12700635 115.1117913722992\n",
      "12800640 116.02637600898743\n",
      "12900645 116.9397885799408\n",
      "13000650 117.85123491287231\n",
      "13100655 118.7671685218811\n",
      "13200660 119.66629314422607\n",
      "13300665 120.57497334480286\n",
      "13400670 121.49215817451477\n",
      "13500675 122.40596628189087\n",
      "13600680 123.3195972442627\n",
      "13700685 124.2325131893158\n",
      "13800690 125.1561279296875\n",
      "13900695 126.07779145240784\n",
      "14000700 126.99667263031006\n",
      "14100705 127.92600512504578\n",
      "14200710 128.84546566009521\n",
      "14300715 129.75527477264404\n",
      "14400720 130.66055250167847\n",
      "14500725 131.56946349143982\n",
      "14600730 132.47524857521057\n",
      "14700735 133.37937116622925\n",
      "14800740 134.2861180305481\n",
      "14900745 135.20185136795044\n",
      "15000750 136.13581705093384\n",
      "15100755 137.05772042274475\n",
      "15200760 137.96904826164246\n",
      "15300765 138.88546633720398\n",
      "15400770 139.81157541275024\n",
      "15500775 140.73317432403564\n",
      "15600780 141.65201592445374\n",
      "15700785 142.5741319656372\n",
      "15800790 143.49449133872986\n",
      "15900795 144.41467356681824\n",
      "16000800 145.33675956726074\n",
      "16100805 146.25801849365234\n",
      "16200810 147.1788341999054\n",
      "16300815 148.11642122268677\n",
      "16400820 149.0637571811676\n",
      "16500825 149.96982169151306\n",
      "16600830 150.87717151641846\n",
      "16700835 151.7812144756317\n",
      "16800840 152.67389059066772\n",
      "16900845 153.5651819705963\n",
      "17000850 154.46992945671082\n",
      "17100855 155.37789058685303\n",
      "17200860 156.28327775001526\n",
      "17300865 157.18243217468262\n",
      "17400870 158.0843915939331\n",
      "17500875 159.00218152999878\n",
      "17600880 159.91021871566772\n",
      "17700885 160.82259368896484\n",
      "17800890 161.74300622940063\n",
      "17900895 162.65759801864624\n",
      "18000900 163.58628487586975\n",
      "18100905 164.50215864181519\n",
      "18200910 165.43695449829102\n",
      "18300915 166.35659432411194\n",
      "18400920 167.27139568328857\n",
      "18500925 168.18312239646912\n",
      "18600930 169.0983600616455\n",
      "18700935 170.01746463775635\n",
      "18800940 170.93451595306396\n",
      "18900945 171.85295224189758\n",
      "19000950 172.76600289344788\n",
      "19100955 173.6746723651886\n",
      "19200960 174.58688044548035\n",
      "19300965 175.51242446899414\n",
      "19400970 176.41450333595276\n",
      "19500975 177.30437588691711\n",
      "19600980 178.1995973587036\n",
      "19700985 179.0897719860077\n",
      "19800990 179.99032640457153\n",
      "19900995 180.89856004714966\n",
      "20001000 181.82374715805054\n",
      "20101005 182.74270677566528\n",
      "20201010 183.65568709373474\n",
      "20301015 184.5571358203888\n",
      "20401020 185.47152256965637\n",
      "20501025 186.39147996902466\n",
      "20601030 187.3042984008789\n",
      "20701035 188.21443128585815\n",
      "20801040 189.13287353515625\n",
      "20901045 190.0403356552124\n",
      "21001050 190.95522451400757\n",
      "21101055 191.872150182724\n",
      "21201060 192.7847626209259\n",
      "21301065 193.70335054397583\n",
      "21401070 194.62259769439697\n",
      "21501075 195.5386004447937\n",
      "21601080 196.4705970287323\n",
      "21701085 197.39600706100464\n",
      "21801090 198.31698846817017\n",
      "21901095 199.2352147102356\n",
      "22001100 200.15967535972595\n",
      "22101105 201.08355832099915\n",
      "22201110 202.0031292438507\n",
      "22301115 202.9395477771759\n",
      "22401120 203.86420488357544\n",
      "22501125 204.7813379764557\n",
      "22601130 205.67817449569702\n",
      "22701135 206.57272171974182\n",
      "22801140 207.4717276096344\n",
      "22901145 208.36892175674438\n",
      "23001150 209.2771372795105\n",
      "23101155 210.1893789768219\n",
      "23201160 211.11645889282227\n",
      "23301165 212.02393078804016\n",
      "23401170 212.94508528709412\n",
      "23501175 213.86117243766785\n",
      "23601180 214.78933691978455\n",
      "23701185 215.71495985984802\n",
      "23801190 216.63634514808655\n",
      "23901195 217.5530710220337\n",
      "24001200 218.47388672828674\n",
      "24101205 219.39103293418884\n",
      "24201210 220.31635284423828\n",
      "24301215 221.24494457244873\n",
      "24401220 222.164142370224\n",
      "24501225 223.08753943443298\n",
      "24601230 224.01765823364258\n",
      "24701235 224.93067002296448\n",
      "24801240 225.84960460662842\n",
      "24901245 226.77126049995422\n",
      "25001250 227.70619869232178\n",
      "25101255 228.63377022743225\n",
      "25201260 229.56542587280273\n",
      "25301265 230.48749661445618\n",
      "25401270 231.4161515235901\n",
      "25501275 232.3427848815918\n",
      "25601280 233.26124167442322\n",
      "25701285 234.15911746025085\n",
      "25801290 235.05255103111267\n",
      "25901295 235.95210480690002\n",
      "26001300 236.8576693534851\n",
      "26101305 237.76648378372192\n",
      "26201310 238.67937088012695\n",
      "26301315 239.5909662246704\n",
      "26401320 240.49494338035583\n",
      "26501325 241.41392397880554\n",
      "26601330 242.33439326286316\n",
      "26701335 243.26791143417358\n",
      "26801340 244.1907444000244\n",
      "26901345 245.11447429656982\n",
      "27001350 246.04341077804565\n",
      "27101355 246.962890625\n",
      "27201360 247.887291431427\n",
      "27301365 248.80690121650696\n",
      "27401370 249.72820806503296\n",
      "27501375 250.64787101745605\n",
      "27601380 251.572003364563\n",
      "27701385 252.50392985343933\n",
      "27801390 253.42407941818237\n",
      "27901395 254.3529417514801\n",
      "28001400 255.27521300315857\n",
      "28101405 256.19479632377625\n",
      "28201410 257.1216928958893\n",
      "28301415 258.0426971912384\n",
      "28401420 258.96202540397644\n",
      "28501425 259.8819284439087\n",
      "28601430 260.80993723869324\n",
      "28701435 261.74232268333435\n",
      "28801440 262.6671657562256\n",
      "28901445 263.583283662796\n",
      "29001450 264.50337624549866\n",
      "29101455 265.4180037975311\n",
      "29201460 266.3370318412781\n",
      "29301465 267.26044249534607\n",
      "29401470 268.17902398109436\n",
      "29501475 269.102646112442\n",
      "29601480 270.01490354537964\n",
      "29701485 270.9063766002655\n",
      "29801490 271.79356384277344\n",
      "29901495 272.68535113334656\n",
      "30001500 273.58625864982605\n",
      "30101505 274.50395154953003\n",
      "30201510 275.4114625453949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30301515 276.3225815296173\n",
      "30401520 277.2324984073639\n",
      "30501525 278.1497528553009\n",
      "30601530 279.0662055015564\n",
      "30701535 279.9833381175995\n",
      "30801540 280.9035665988922\n",
      "30901545 281.8225166797638\n",
      "31001550 282.7413582801819\n",
      "31101555 283.66345834732056\n",
      "31201560 284.57537817955017\n",
      "31301565 285.49353551864624\n",
      "31401570 286.4119303226471\n",
      "31501575 287.32899594306946\n",
      "31601580 288.2468795776367\n",
      "31701585 289.16980481147766\n",
      "31801590 290.08907079696655\n",
      "31901595 291.01279759407043\n",
      "32001600 291.92983388900757\n",
      "32101605 292.8561725616455\n",
      "32201610 293.78232979774475\n",
      "32301615 294.7030804157257\n",
      "32401620 295.6263518333435\n",
      "32501625 296.54749870300293\n",
      "32601630 297.4648027420044\n",
      "32701635 298.37340807914734\n",
      "32801640 299.3034334182739\n",
      "32901645 300.2278108596802\n",
      "33001650 301.14843130111694\n",
      "33101655 302.07398295402527\n",
      "33201660 302.9926793575287\n",
      "33301665 303.92354679107666\n",
      "33401670 304.85318088531494\n",
      "33501675 305.7733542919159\n",
      "33601680 306.70328664779663\n",
      "33701685 307.6294832229614\n",
      "33801690 308.5399765968323\n",
      "33901695 309.43615555763245\n",
      "34001700 310.3308787345886\n",
      "34101705 311.23635506629944\n",
      "34201710 312.1523654460907\n",
      "34301715 313.0585174560547\n",
      "34401720 313.98006200790405\n",
      "34501725 314.89853405952454\n",
      "34601730 315.81507325172424\n",
      "34701735 316.7371459007263\n",
      "34801740 317.65510416030884\n",
      "34901745 318.57678270339966\n",
      "35001750 319.5063235759735\n",
      "35101755 320.412232875824\n",
      "35201760 321.3341724872589\n",
      "35301765 322.24727058410645\n",
      "35401770 323.16653084754944\n",
      "35501775 324.0909221172333\n",
      "35601780 325.0305871963501\n",
      "35701785 325.95307993888855\n",
      "35801790 326.8765187263489\n",
      "35901795 327.79967164993286\n",
      "36001800 328.72244906425476\n",
      "36101805 329.6599748134613\n",
      "36201810 330.5921025276184\n",
      "36301815 331.52410912513733\n",
      "36401820 332.46034049987793\n",
      "36501825 333.3801727294922\n",
      "36601830 334.3051109313965\n",
      "36701835 335.22978019714355\n",
      "36801840 336.15700221061707\n",
      "36901845 337.07796835899353\n",
      "37001850 338.00994968414307\n",
      "37101855 338.926527261734\n",
      "37201860 339.859450340271\n",
      "37301865 340.79055619239807\n",
      "37401870 341.71770119667053\n",
      "37501875 342.64864587783813\n",
      "37601880 343.58113646507263\n",
      "37701885 344.50659823417664\n",
      "37801890 345.4339189529419\n",
      "37901895 346.35228276252747\n",
      "38001900 347.23796582221985\n",
      "38101905 348.13616013526917\n",
      "38201910 349.0435252189636\n",
      "38301915 349.9605643749237\n",
      "38401920 350.8854184150696\n",
      "38501925 351.7988107204437\n",
      "38601930 352.71943974494934\n",
      "38701935 353.65779304504395\n",
      "38801940 354.5761740207672\n",
      "38901945 355.50379180908203\n",
      "39001950 356.42702198028564\n",
      "39101955 357.35909485816956\n",
      "39201960 358.2783160209656\n",
      "39301965 359.1959569454193\n",
      "39401970 360.1265666484833\n",
      "39501975 361.0446083545685\n",
      "39601980 361.9632911682129\n",
      "39701985 362.8873920440674\n",
      "39801990 363.81074380874634\n",
      "39901995 364.73794984817505\n",
      "40002000 365.66376852989197\n",
      "40102005 366.5998513698578\n",
      "40202010 367.53376507759094\n",
      "40302015 368.467942237854\n",
      "40402020 369.4018840789795\n",
      "40502025 370.3241591453552\n",
      "40602030 371.2522723674774\n",
      "40702035 372.1645314693451\n",
      "40802040 373.0932297706604\n",
      "40902045 374.0165922641754\n",
      "41002050 374.9434435367584\n",
      "41102055 375.87618803977966\n",
      "41202060 376.77811908721924\n",
      "41302065 377.68853187561035\n",
      "41402070 378.61486053466797\n",
      "41502075 379.541836977005\n",
      "41602080 380.46351766586304\n",
      "41702085 381.38511204719543\n",
      "41802090 382.32679319381714\n",
      "41902095 383.24076986312866\n",
      "42002100 384.16377234458923\n",
      "42102105 385.0921537876129\n",
      "42202110 386.03256392478943\n",
      "42302115 386.9578833580017\n",
      "42402120 387.8910310268402\n"
     ]
    }
   ],
   "source": [
    "#en_list = []\n",
    "#ru_list = []\n",
    "#counter = 0\n",
    "#start = time.time()\n",
    "#regex_corner = re.compile('[<>?]')\n",
    "#with open('/srv/hd6/data/Poem2Poem/data/ParallelEnRu/OpenSubtitlesv2018/en-ru.tmx', 'r') as f:\n",
    "#    while True:\n",
    "#        line = f.readline()\n",
    "#        if not line:\n",
    "#            break\n",
    "#        if \":lang=\\\"en\\\"\" in line:\n",
    "#            good_tokens = regex_corner.sub(' ', line).split()[3:-2]\n",
    "#            en_list.append(' '.join(good_tokens))\n",
    "#            counter += 1\n",
    "#        elif \":lang=\\\"ru\\\"\" in line:\n",
    "#            good_tokens = regex_corner.sub(' ', line).split()[3:-2]\n",
    "#            ru_list.append(' '.join(good_tokens))\n",
    "#            counter += 1\n",
    "#        if counter % 100005 == 0:\n",
    "#            print(counter, time.time()-start)\n",
    "#            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21223864"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import WordPunctTokenizer\n",
    "#tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42ba28e1b328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/train.en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/train.ru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_list' is not defined"
     ]
    }
   ],
   "source": [
    "#with open('/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/train.en', 'w') as f:\n",
    "#    for sent in en_list:\n",
    "#        f.write(' '.join(tokenizer.tokenize(sent)) + '\\n')\n",
    "#        \n",
    "#with open('/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/train.ru', 'w') as f:\n",
    "#    for sent in ru_list:\n",
    "#        f.write(' '.join(tokenizer.tokenize(sent)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing.text_preprocessing import create_train_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning BPE...\n",
      "Writing train files...\n",
      "Learning BPE...\n",
      "Writing train files...\n"
     ]
    }
   ],
   "source": [
    "#create_train_bpe('/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "import numpy as np\n",
    "from models.utils import infer_length, infer_mask\n",
    "from models.utils import select_values_over_last_axis, compute_logits, compute_loss, compute_bleu\n",
    "from models.attention_model import BasicModel, AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7051195896135322106\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12510337977484714237\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/srv/hd5/data/vprov/Poems/data/OpenSubtitlesv2018/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstn=10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lines = []\n",
    "ru_lines = []\n",
    "with open(TRAIN_DIR + 'train.bpe.ru', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ru_lines.append(line)\n",
    "        if i > firstn:\n",
    "            break\n",
    "        \n",
    "with open(TRAIN_DIR + 'train.bpe.en', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        en_lines.append(line)\n",
    "        if i > firstn:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_out = np.array(ru_lines[:firstn])\n",
    "#data_inp = np.array(en_lines[:firstn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_inp.shape, data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: Looks like they did . DA@@ PH@@ N@@ E :\n",
      "\n",
      "out: Похоже , им это удалось .\n",
      "\n",
      "\n",
      "inp: Bill Carson !\n",
      "\n",
      "out: Билл Кар@@ сон !\n",
      "\n",
      "\n",
      "inp: She knows every corner of that room .\n",
      "\n",
      "out: Она знает каждый угол в той комнате .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(en_lines, ru_lines, test_size=10000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990002"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearTranslate(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc, config,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        self.name = name\n",
    "        self.inp_voc = inp_voc\n",
    "        self.out_voc = out_voc\n",
    "        self.hid_size=hid_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            # define model layers\n",
    "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
    "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
    "            with tf.variable_scope(\"forward\"):\n",
    "                self.enc0 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "            with tf.variable_scope(\"backward\"):\n",
    "                self.enc1 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "                \n",
    "            self.attention = AttentionLayer('Attention', hid_size *4, hid_size, hid_size)\n",
    "            self.dec_start = L.Dense(hid_size*2)\n",
    "            \n",
    "            self.dec0 = tf.nn.rnn_cell.LSTMCell(hid_size, forget_bias=1.0, state_is_tuple=False)\n",
    "            self.logits = L.Dense(len(out_voc))\n",
    "            \n",
    "            # prepare to translate_lines\n",
    "            self.inp = tf.placeholder('int32', [None, None])\n",
    "            self.initial_state = self.prev_state = self.encode(self.inp)\n",
    "            self.prev_tokens = tf.placeholder('int32', [None])\n",
    "            self.next_state, self.next_logits = self.decode(self.prev_state, self.prev_tokens)\n",
    "\n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode input sequence, create initial decoder states\n",
    "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
    "        mask = tf.expand_dims(tf.cast(infer_mask(inp, self.inp_voc.eos_ix), tf.bool), axis=2)\n",
    "\n",
    "        inp_mask = tf.tile(mask, (1, 1, self.hid_size))\n",
    "        #inp_mask = tf.cast(infer_mask(inp, self.inp_voc.eos_ix), tf.bool)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        \n",
    "        with tf.variable_scope('enc0'):\n",
    "            enc_outputs_t, enc_last_t = tf.nn.bidirectional_dynamic_rnn(\n",
    "                              self.enc0, self.enc1, inp_emb,\n",
    "                              sequence_length=inp_lengths,\n",
    "                              dtype=inp_emb.dtype)\n",
    "            #[batch, time, hid_size*4]\n",
    "            enc_outputs = tf.concat(enc_outputs_t, 2)\n",
    "            #[batch, hid_size * 4]\n",
    "            enc_last = tf.concat(enc_last_t, 1)\n",
    "            \n",
    "        dec_start = self.dec_start(enc_last)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        attn, first_attn_probas = self.attention(enc_outputs, dec_start, inp_mask)\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_outputs, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "\n",
    "    def decode(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
    "        \"\"\"\n",
    "        # Unpack your state: you will get tensors in the same order that you've packed in encode\n",
    "        [prev_dec, enc_outputs, inp_mask, first_attn_probas] = prev_state\n",
    "        \n",
    "        \n",
    "        # Perform decoder step\n",
    "        # * predict next attn response and probas given previous decoder state\n",
    "        # * use prev tokens and next attn response to update decoder states\n",
    "        # * predict logits\n",
    "        prev_emb = self.emb_out(prev_tokens[:,None])[:,0]\n",
    "        \n",
    "        next_attn_response, next_attn_probas = self.attention(enc_outputs, prev_dec, inp_mask)\n",
    "        \n",
    "        with tf.variable_scope('dec0'):\n",
    "            dec_input = tf.concat([prev_emb, next_attn_response], axis=-1)\n",
    "            new_dec_out, new_dec_state = self.dec0(dec_input, prev_dec)\n",
    "        \n",
    "        output_logits = self.logits(new_dec_out)\n",
    "        \n",
    "        # Pack new state:\n",
    "        # * replace previous decoder state with next one\n",
    "        # * copy encoder sequence and mask from prev_state\n",
    "        # * append new attention probas\n",
    "        \n",
    "        next_state = [new_dec_state, enc_outputs, inp_mask, next_attn_probas]\n",
    "        return next_state, output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.enc_dim=64\n",
    "#config.dec_dim=64\n",
    "#config.dec_layer_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f88ec301128>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f88ec301160>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f88ec301048>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "model = SpearTranslate('Spear', inp_voc, out_voc, config, emb_size=100, hid_size=256, attn_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "(?, ?, 10701)\n"
     ]
    }
   ],
   "source": [
    "inp = tf.placeholder('int32', [None, None])\n",
    "out = tf.placeholder('int32', [None, None])\n",
    "\n",
    "loss = compute_loss(model, inp, out, out_voc)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'train_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar(\"train_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': []}\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990002"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "INP_SIZE = len(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./tmp/tensorboard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_n = tf.Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc1e0197f0d4e85b689459af0cb48a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156093), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    for i in range(EPOCH):\n",
    "        #while cur_ix < INP_SIZE:\n",
    "        cur_ix = 0\n",
    "        for _ in tqdm_notebook(range(INP_SIZE//batch_size)):\n",
    "            step = i * INP_SIZE + cur_ix\n",
    "            feed_dict = {\n",
    "                inp: inp_voc.to_matrix(train_inp[cur_ix: cur_ix+batch_size]),\n",
    "                out: out_voc.to_matrix(train_out[cur_ix: cur_ix+batch_size]),\n",
    "            }\n",
    "            cur_ix += batch_size\n",
    "            \n",
    "            loss_t, _, summary = sess.run([loss, train_step, merged], feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            metrics['train_loss'].append((step, loss_t))\n",
    "            if step % 500 == 5:\n",
    "                bleus = []\n",
    "                cur_dev_ix = 0\n",
    "                dev_len = len(dev_inp)\n",
    "                while cur_dev_ix < dev_len:\n",
    "                    bleus.append(compute_bleu(model, \n",
    "                                              dev_inp[cur_dev_ix:cur_dev_ix+batch_size], \n",
    "                                              dev_out[cur_dev_ix:cur_dev_ix+batch_size], \n",
    "                                              sess, inp_voc, out_voc))\n",
    "                    cur_dev_ix += batch_size\n",
    "                \n",
    "                metrics['dev_bleu'].append((step, np.mean(bleus)))\n",
    "               \n",
    "                clear_output(True)\n",
    "                plt.figure(figsize=(12,4))\n",
    "                for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                    plt.subplot(1, len(metrics), i + 1)\n",
    "                    plt.title(name)\n",
    "                    plt.plot(*zip(*history))\n",
    "                    plt.grid()\n",
    "                plt.show()\n",
    "                print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-100:], axis=0)[1], flush=True)\n",
    "                print(\"EPOCH: \", i)\n",
    "                # Tensorboard\n",
    "                summary_n.value.add(tag=tagname, simple_value=metrics['dev_bleu'][-1])\n",
    "                train_writer.add_summary(summary_n, step)\n",
    "                train_writer.flush()\n",
    "            if step % 1000 == 0:\n",
    "                saver.save(sess, \"/tmp/model1.ckpt\")\n",
    "                with open('/tmp/metrics.pkl', 'wb') as f:\n",
    "                    pickle.dump(metrics, f)\n",
    "except Exception as e:\n",
    "    with open('errlog.txt', 'w') as f:\n",
    "        f.write(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
