{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "import pickle as cPickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim.models as g\n",
    "#from util import *\n",
    "# from sonnet_model import SonnetModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.corpus import cmudict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# constants\n",
    "pad_symbol = \"<pad>\"\n",
    "end_symbol = \"<eos>\"\n",
    "unk_symbol = \"<unk>\"\n",
    "dummy_symbols = [pad_symbol, end_symbol, unk_symbol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "###preprocessing options###\n",
    "word_minfreq=3\n",
    "\n",
    "###hyper-parameters###\n",
    "seed=0\n",
    "batch_size=32\n",
    "keep_prob=0.7\n",
    "epoch_size=1\n",
    "max_grad_norm=5\n",
    "#language model\n",
    "word_embedding_dim=100\n",
    "word_embedding_model=\"pretrain_word2vec/dim100/word2vec.bin\"\n",
    "lm_enc_dim=200\n",
    "lm_dec_dim=600\n",
    "lm_dec_layer_size=1\n",
    "lm_attend_dim=25\n",
    "lm_learning_rate=0.2\n",
    "#pentameter model\n",
    "char_embedding_dim=150\n",
    "pm_enc_dim=50\n",
    "pm_dec_dim=200\n",
    "pm_attend_dim=50\n",
    "pm_learning_rate=0.001\n",
    "repeat_loss_scale=1.0\n",
    "cov_loss_scale=1.0\n",
    "cov_loss_threshold=0.7\n",
    "sigma=1.00\n",
    "#rhyme model\n",
    "rm_dim=100\n",
    "rm_neg=5 #extra randomly sampled negative examples\n",
    "rm_delta=0.5\n",
    "rm_learning_rate=0.001\n",
    "\n",
    "###sonnet hyper-parameters###\n",
    "bptt_truncate=2 #number of sonnet lines to truncate bptt\n",
    "doc_lines=14 #total number of lines for a sonnet\n",
    "\n",
    "###misc###\n",
    "verbose=False\n",
    "save_model=True\n",
    "\n",
    "###input/output###\n",
    "output_dir=\"output\"\n",
    "train_data=\"datasets/gutenberg/sonnet_train.txt\"\n",
    "valid_data=\"datasets/gutenberg/sonnet_valid.txt\"\n",
    "test_data=\"datasets/gutenberg/sonnet_test.txt\"\n",
    "output_prefix=\"wmin%d_sd%d_bat%d_kp%.1f_eph%d_grd%d_wdim%d_lmedim%d_lmddim%d_lmdlayer%d_lmadim%d_lmlr%.1f_cdim%d_pmedim%d_pmddim%d_pmadim%d_pmlr%.1E_loss%.1f-%.1f-%.1f_sm%.2f_rmdim%d_rmn%d_rmd%.1f_rmlr%.1E_son%d-%d\" % \\\n",
    "    (word_minfreq, seed, batch_size, keep_prob, epoch_size, max_grad_norm, word_embedding_dim, lm_enc_dim,\n",
    "    lm_dec_dim, lm_dec_layer_size, lm_attend_dim, lm_learning_rate,\n",
    "    char_embedding_dim, pm_enc_dim, pm_dec_dim, pm_attend_dim, pm_learning_rate, repeat_loss_scale,\n",
    "    cov_loss_scale, cov_loss_threshold, sigma, rm_dim, rm_neg, rm_delta, rm_learning_rate, bptt_truncate, doc_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import codecs\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_vocab(corpus, word_minfreq, dummy_symbols):\n",
    "    idxword, idxchar = [], []\n",
    "    wordxid, charxid = defaultdict(int), defaultdict(int)\n",
    "    word_freq, char_freq = defaultdict(int), defaultdict(int)\n",
    "    wordxchar = defaultdict(list)\n",
    "\n",
    "    def update_dic(symbol, idxvocab, vocabxid):\n",
    "        if symbol not in vocabxid:\n",
    "            idxvocab.append(symbol)\n",
    "            vocabxid[symbol] = len(idxvocab) - 1 \n",
    "\n",
    "    for line_id, line in enumerate(codecs.open(corpus, \"r\", \"utf-8\")):\n",
    "        for word in line.strip().split():\n",
    "            word_freq[word] += 1\n",
    "        for char in line.strip():\n",
    "            char_freq[char] += 1\n",
    "\n",
    "    #add in dummy symbols into dictionaries\n",
    "    for s in dummy_symbols:\n",
    "        update_dic(s, idxword, wordxid)\n",
    "        update_dic(s, idxchar, charxid)\n",
    "\n",
    "    #remove low fequency words/chars\n",
    "    def collect_vocab(vocab_freq, idxvocab, vocabxid):\n",
    "        for w, f in sorted(list(vocab_freq.items()), key=operator.itemgetter(1), reverse=True):\n",
    "            if f < word_minfreq:\n",
    "                break\n",
    "            else:\n",
    "                update_dic(w, idxvocab, vocabxid)\n",
    "\n",
    "    collect_vocab(word_freq, idxword, wordxid)\n",
    "    collect_vocab(char_freq, idxchar, charxid)\n",
    "\n",
    "    #word id to [char ids]\n",
    "    dummy_symbols_set = set(dummy_symbols)\n",
    "    for wi, w in enumerate(idxword):\n",
    "        if w in dummy_symbols:\n",
    "            wordxchar[wi] = [wi]\n",
    "        else:\n",
    "            for c in w:\n",
    "                wordxchar[wi].append(charxid[c] if c in charxid else charxid[dummy_symbols[2]])\n",
    "\n",
    "    return idxword, wordxid, idxchar, charxid, wordxchar\n",
    "\n",
    "\n",
    "def only_symbol(word):\n",
    "    for c in word:\n",
    "        if c.isalpha():\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_punct(string):\n",
    "    return \" \".join(\"\".join([ item for item in string if (item.isalpha() or item == \" \") ]).split())\n",
    "\n",
    "\n",
    "def load_data(corpus, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol):\n",
    "    \"\"\"\n",
    "    Translate text data. Note that it works with 14 lines text\n",
    "    :param corpus:\n",
    "    :param wordxid:\n",
    "    :param idxword:\n",
    "    :param charxid:\n",
    "    :param idxchar:\n",
    "    :param pad_symbol:\n",
    "    :param end_symbol:\n",
    "    :param unk_symbol:\n",
    "    :return: word_data, char_data, rhyme_data, nwords, nchars\n",
    "    word_data: list of word indicies, order of sentences is reversed, order of words is reversed also\n",
    "    char_data: list of char indicies, order of sentences is reversed, order of words is same (normal)\n",
    "    rhyme_data: list of special words in chars presentation.\n",
    "    For every sonnet, since third sentence, divides on gtoups of 4 sentences. For every group gets\n",
    "    last words of sentence and stores.\n",
    "    [word on current sentences, other words in group, idx of sentence]\n",
    "    \"\"\"\n",
    "    nwords     = [] #number of words for each line\n",
    "    nchars     = [] #number of chars for each line\n",
    "    word_data  = [] #data[doc_id][0][line_id] = list of word ids; data[doc_id][1][line_id] = list of [char_ids]\n",
    "    char_data  = [] #data[line_id] = list of char ids\n",
    "    rhyme_data = [] #list of ( target_word, [candidate_words], target_word_line_id ); word is a list of characters\n",
    "\n",
    "    def word_to_char(word):\n",
    "        if word in set([pad_symbol, end_symbol, unk_symbol]):\n",
    "            return [ wordxid[word] ]\n",
    "        else:\n",
    "            return [ charxid[item] if item in charxid else charxid[unk_symbol] for item in word ]\n",
    "\n",
    "\n",
    "    for doc in codecs.open(corpus, \"r\", \"utf-8\"):\n",
    "\n",
    "        word_lines, char_lines = [[], []], []\n",
    "        last_words = []\n",
    "\n",
    "         #reverse the order of lines and words as we are generating from end to start\n",
    "        for line in reversed(doc.strip().split(end_symbol)):\n",
    "\n",
    "            if len(line.strip()) > 0:\n",
    "\n",
    "                word_seq = [ wordxid[item] if item in wordxid else wordxid[unk_symbol] \\\n",
    "                    for item in reversed(line.strip().split()) ] + [wordxid[end_symbol]]\n",
    "\n",
    "                char_seq = [ word_to_char(item) for item in reversed(line.strip().split()) ] + [word_to_char(end_symbol)]\n",
    "\n",
    "                word_lines[0].append(word_seq)\n",
    "                word_lines[1].append(char_seq)\n",
    "                char_lines.append([ charxid[item] if item in charxid else charxid[unk_symbol] \\\n",
    "                    for item in remove_punct(line.strip())])\n",
    "                nwords.append(len(word_lines[0][-1]))\n",
    "                nchars.append(len(char_lines[-1]))\n",
    "\n",
    "                last_words.append(line.strip().split()[-1])\n",
    "\n",
    "        if len(word_lines[0]) == 14: #14 lines for sonnets\n",
    "\n",
    "            word_data.append(word_lines)\n",
    "            char_data.extend(char_lines)\n",
    "\n",
    "            last_words = last_words[2:] #remove couplets (since they don't always rhyme)\n",
    "\n",
    "            for wi, w in enumerate(last_words):\n",
    "                rhyme_data.append((word_to_char(w),\n",
    "                                   [word_to_char(item)\n",
    "                                    for item_id, item in enumerate(last_words[(wi // 4) * 4:(wi // 4 + 1) * 4])\n",
    "                                    if item_id != (wi % 4)], (11 - wi)))\n",
    "\n",
    "    return word_data, char_data, rhyme_data, nwords, nchars\n",
    "            \n",
    "\n",
    "def print_stats(partition, word_data, rhyme_data, nwords, nchars):\n",
    "    print(partition, \"statistics:\")\n",
    "    print(\"  Number of documents         =\", len(word_data))\n",
    "    print(\"  Number of rhyme examples    =\", len(rhyme_data))\n",
    "    print(\"  Total number of word tokens =\", sum(nwords))\n",
    "    print(\"  Mean/min/max words per line = %.2f/%d/%d\" % (np.mean(nwords), min(nwords), max(nwords)))\n",
    "    print(\"  Total number of char tokens =\", sum(nchars))\n",
    "    print(\"  Mean/min/max chars per line = %.2f/%d/%d\" % (np.mean(nchars), min(nchars), max(nchars)))\n",
    "\n",
    "\n",
    "def init_embedding(model, idxword):\n",
    "    word_emb = []\n",
    "    for vi, v in enumerate(idxword):\n",
    "        if v in model:\n",
    "            word_emb.append(model[v])\n",
    "        else:\n",
    "            word_emb.append(np.random.uniform(-0.5/model.vector_size, 0.5/model.vector_size, [model.vector_size,]))\n",
    "    return np.array(word_emb)\n",
    "\n",
    "\n",
    "def pad(lst, max_len, pad_symbol):\n",
    "    if len(lst) > max_len:\n",
    "        print(\"\\nERROR: padding\")\n",
    "        print(\"length of list greater than maxlen; list =\", lst, \"; maxlen =\", max_len)\n",
    "        raise SystemExit\n",
    "    return lst + [pad_symbol] * (max_len - len(lst))\n",
    "\n",
    "\n",
    "def get_vowels():\n",
    "    return set([\"a\", \"e\", \"i\", \"o\", \"u\"])\n",
    "\n",
    "\n",
    "def coverage_mask(char_ids, idxchar):\n",
    "    vowels = get_vowels()\n",
    "    return [ float(idxchar[c] in vowels) for c in char_ids ]\n",
    "\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def create_word_batch(data, batch_size, lines_per_doc, nlines_per_batch, pad_symbol, end_symbol, unk_symbol, shuffle_data):\n",
    "    \"\"\"\n",
    "    Creates word batch\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param lines_per_doc:\n",
    "    :param nlines_per_batch:\n",
    "    :param pad_symbol:\n",
    "    :param end_symbol:\n",
    "    :param unk_symbol:\n",
    "    :param shuffle_data:\n",
    "    :return:\n",
    "    Generates sentence from couple of sonnet strings with features:\n",
    "    list (x, y, docs, doc_lens, doc_lines, xchar, xchar_lens, hist, hist_lens)\n",
    "    x: tokenized sentence with spec symbol at the end (1)\n",
    "    y: tokenized sentence with spec symbol at the begining (1)\n",
    "    docs: idx for every sonnet (document)\n",
    "    doc_lens: length for generated sentence (len of couple sonnet strings)\n",
    "    xchar: list of translated word -> chars sentence, len of every word`s chars list equals len of word. If word ends,\n",
    "    list appended by zeroes\n",
    "    xchar_lens: list of every word lens\n",
    "    hist: previous couple of strings\n",
    "    hist_lens: lens of prevous string couple\n",
    "    \"\"\"\n",
    "    docs_per_batch = len(data) // batch_size\n",
    "    batches = []\n",
    "    doc_ids = list(range(len(data)))\n",
    "    if shuffle_data:\n",
    "        random.shuffle(doc_ids)\n",
    "\n",
    "    if lines_per_doc % nlines_per_batch != 0:\n",
    "        print(\"\\nERROR:\")\n",
    "        print((\"lines_per_doc (%d) %% nlines_per_batch (%d) must equal 0\" % (lines_per_doc, nlines_per_batch)))\n",
    "        raise SystemExit\n",
    "\n",
    "    for i in range(docs_per_batch):\n",
    "\n",
    "        for j in range(lines_per_doc // nlines_per_batch):\n",
    "\n",
    "            docs       = []\n",
    "            doc_lens   = []\n",
    "            doc_lines  = []\n",
    "            x          = []\n",
    "            y          = []\n",
    "            xchar      = []\n",
    "            xchar_lens = []\n",
    "            hist       = []\n",
    "            hist_lens  = []\n",
    "\n",
    "            for k in range(batch_size):\n",
    "\n",
    "                d       = doc_ids[i*batch_size+k]\n",
    "                wordseq = flatten_list(data[d][0][j*nlines_per_batch:(j+1)*nlines_per_batch])\n",
    "                charseq = flatten_list(data[d][1][j*nlines_per_batch:(j+1)*nlines_per_batch])\n",
    "                histseq = flatten_list(data[d][0][:j*nlines_per_batch])\n",
    "    \n",
    "                x.append([end_symbol] + wordseq[:-1])\n",
    "                y.append(wordseq)\n",
    "\n",
    "                docs.append(d)\n",
    "                doc_lens.append(len(wordseq))\n",
    "                doc_lines.append(list(range(j*nlines_per_batch, (j+1)*nlines_per_batch)))\n",
    "\n",
    "                xchar.append([[end_symbol]] + charseq[:-1])\n",
    "                xchar_lens.append( [1] + [ len(item) for item in charseq[:-1] ])\n",
    "\n",
    "                hist.append(histseq if len(histseq) > 0 else [unk_symbol])\n",
    "                hist_lens.append(len(histseq) if len(histseq) > 0 else 1)\n",
    "\n",
    "            #pad the data\n",
    "            word_pad_len = max(doc_lens)\n",
    "            char_pad_len = max(flatten_list(xchar_lens))\n",
    "            hist_pad_len = max(hist_lens)\n",
    "            for k in range(batch_size):\n",
    "\n",
    "                x[k] = pad(x[k], word_pad_len, pad_symbol)\n",
    "                y[k] = pad(y[k], word_pad_len, pad_symbol)\n",
    "\n",
    "                xchar_lens[k].extend( [1]*(word_pad_len-len(xchar[k])) ) #add len for pad symbols\n",
    "\n",
    "                xchar[k] = pad(xchar[k], word_pad_len, [pad_symbol]) #pad the word lengths\n",
    "                xchar[k] = [pad(item, char_pad_len, pad_symbol) for item in xchar[k]] #pad the characters\n",
    "\n",
    "                hist[k] = pad(hist[k], hist_pad_len, pad_symbol)\n",
    "\n",
    "            batches.append((x, y, docs, doc_lens, doc_lines, xchar, xchar_lens, hist, hist_lens))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def create_char_batch(data, batch_size, pad_symbol, pentameter, idxchar, shuffle_data):\n",
    "    \"\"\"\n",
    "    Generates char batch\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param pad_symbol:\n",
    "    :param pentameter:\n",
    "    :param idxchar:\n",
    "    :param shuffle_data:\n",
    "    :return:\n",
    "    for every line returns [list of encoded chars, len of line, coverage_mask]\n",
    "    coverage mask - shows vowels in sentences\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    batch_len = len(data) // batch_size\n",
    "    print(\"BATCH_LEN\", batch_len)\n",
    "    if shuffle_data:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        enc_x    = []\n",
    "        enc_xlen = []\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            enc_x.append(data[i*batch_size+j])\n",
    "            enc_xlen.append(len(data[i*batch_size+j]))\n",
    "\n",
    "        xlen_max = max(enc_xlen)\n",
    "        cov_mask = np.zeros((batch_size, xlen_max)) #coverage mask\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            enc_x[j]    = pad(enc_x[j], xlen_max, pad_symbol)\n",
    "            cov_mask[j] = coverage_mask(enc_x[j], idxchar)\n",
    "            \n",
    "        batches.append((enc_x, enc_xlen, cov_mask))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def create_rhyme_batch(data, batch_size, pad_symbol, wordxchar, num_neg, shuffle_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param pad_symbol:\n",
    "    :param wordxchar:\n",
    "    :param num_neg:\n",
    "    :param shuffle_data:\n",
    "    :return: list of [xc, xclen, xid]\n",
    "    xc: like rhyme data - but special words extends by random word\n",
    "\n",
    "    \"\"\"\n",
    "    if shuffle_data:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(len(data) // batch_size):\n",
    "        x, xid, c  = [], [], []\n",
    "        xlen, clen = [], []\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            x.append(data[i*batch_size+j][0])\n",
    "            xid.append(data[i*batch_size+j][2])\n",
    "            xlen.append(len(data[i*batch_size+j][0]))\n",
    "            for context in data[i*batch_size+j][1]:\n",
    "                c.append(context)\n",
    "                clen.append(len(context))\n",
    "\n",
    "            # From preliminary\n",
    "            # experiments we found that we can improve\n",
    "            # the model by introducing additional non-rhyming\n",
    "            # or negative reference words. Negative reference\n",
    "            # words are sampled uniform randomly from the vocabulary,\n",
    "            # and the number of additional negative\n",
    "            # words is a hyper-parameter.\n",
    "            for _ in range(num_neg):\n",
    "                c.append(wordxchar[random.randrange(3, len(wordxchar))])\n",
    "                clen.append(len(c[-1]))\n",
    "\n",
    "        #merging target and context words\n",
    "        # (first batch_size = target words; following batch_size*(3+num_neg) = context words)\n",
    "        xc        = x + c\n",
    "        xclen     = xlen + clen\n",
    "        xclen_max = max(xclen)\n",
    "\n",
    "        #pad the target words and context words\n",
    "        for xci, xcv in enumerate(xc):\n",
    "            xc[xci] = pad(xcv, xclen_max, pad_symbol)\n",
    "\n",
    "        batches.append((xc, xclen, xid))\n",
    "\n",
    "    return batches\n",
    "        \n",
    "\n",
    "def print_lm_attention(bi, b, attentions, idxword, cf):\n",
    "\n",
    "    print((\"\\n\", \"=\"*100))\n",
    "    for ex in range(cf.batch_size)[-1:]:\n",
    "        xword = [ idxword[item] for item in b[1][ex] ]\n",
    "        hword = [ idxword[item] for item in b[7][ex] ]\n",
    "        print((\"\\nBatch ID =\", bi))\n",
    "        print((\"Example =\", ex))\n",
    "        print((\"x_word =\", \" \".join(xword)))\n",
    "        print((\"hist_word=\", \" \".join(hword)))\n",
    "        for xi, x in enumerate(xword):\n",
    "            print((\"\\nWord =\", x))\n",
    "            print((\"\\tSum dist =\", sum(attentions[ex][xi])))\n",
    "            attn_dist_sort = np.argsort(-attentions[ex][xi])\n",
    "            print(\"\\t\")\n",
    "            for hi in attn_dist_sort[:5]:\n",
    "                print((\"[%d]%s:%.3f  \" % (hi, hword[hi], attentions[ex][xi][hi])))\n",
    "\n",
    "\n",
    "\n",
    "def print_pm_attention(b, batch_size, costs, logits, attentions, mius, idxchar):\n",
    "\n",
    "    print((\"\\n\", \"=\"*100))\n",
    "    for ex in range(batch_size)[-10:]:\n",
    "        print((\"\\nSentence =\", ex))\n",
    "        print((\"x =\", b[0][ex]))\n",
    "        print((\"x len =\", b[1][ex]))\n",
    "        print((\"x char=\", \"\".join(idxchar[item] for item in b[0][ex])))\n",
    "        print((\"losses =\", costs[ex]))\n",
    "        print((\"pentameter output =\", logits[ex]))\n",
    "        print((\"coverage mask =\", b[2][ex]))\n",
    "        for attni, attn in enumerate(attentions):\n",
    "            print((\"attention at time step\", attni, \":\"))\n",
    "            print((\"\\tmiu_p =\", mius[attni][ex] * (b[1][ex] - 1.0)))\n",
    "            for xid in reversed(np.argsort(attn[ex])):\n",
    "                if attn[ex][xid] > 0.05:\n",
    "                    print((\"\\t%.3f %d %s\" % (attn[ex][xid], xid, (idxchar[b[0][ex][xid]]))))\n",
    "\n",
    "\n",
    "def print_rm_attention(b, batch_size, num_context, attentions, pad_id, idxchar):\n",
    "\n",
    "    print((\"\\n\", \"=\"*100))\n",
    "    for exid in range(batch_size)[-10:]:\n",
    "        print((\"\\nTarget word =\", \"\".join([idxchar[item] for item in b[0][exid] if item != pad_id])))\n",
    "        for ci, c in enumerate(b[0][(exid*num_context+batch_size):((exid+1)*num_context+batch_size)]):\n",
    "            print((\"\\t\", (\"%.2f\" % attentions[exid][ci]), \"=\", \"\".join([idxchar[item] for item in c if item != pad_id])))\n",
    "\n",
    "\n",
    "def get_word_stress(cmu, word):\n",
    "\n",
    "    stresses = set([])\n",
    "\n",
    "    def valid(stress):\n",
    "        for sti in range(len(stress)-1):\n",
    "            if abs(int(stress[sti]) - int(stress[sti+1])) != 1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    if word in cmu:\n",
    "        for res in cmu[word]:\n",
    "            stress = \"\"\n",
    "            for syl in res:\n",
    "                if syl[-1] == \"0\":\n",
    "                    stress += \"0\"\n",
    "                elif syl[-1] == \"1\" or syl[-1] == \"2\":\n",
    "                    stress += \"1\"\n",
    "\n",
    "            if valid(stress):\n",
    "                stresses.add(stress)\n",
    "\n",
    "    return stresses\n",
    "\n",
    "\n",
    "def update_stress_accs(accs, word_len, score):\n",
    "\n",
    "    word_buckets   = [4,8,float(\"inf\")]\n",
    "\n",
    "    for wi, wb in enumerate(word_buckets):\n",
    "        if word_len <= wb:\n",
    "            accs[wi].append(score)\n",
    "            break\n",
    "\n",
    "\n",
    "def eval_stress(accs, cmu, attns, pentameter, batch, idxchar, charxid, pad_symbol, cf):\n",
    "\n",
    "    attn_threshold = 0.2\n",
    "\n",
    "    for ex in range(cf.batch_size):\n",
    "\n",
    "        chars     = [idxchar[item] for item in batch[ex]]\n",
    "        space_ids = [-1] + [i for i, ch in enumerate(chars) if ch == \" \"] + \\\n",
    "            [batch[ex].index(charxid[pad_symbol]) if charxid[pad_symbol] in batch[ex] else len(batch[ex])]\n",
    "\n",
    "        for spi, sp in enumerate(space_ids[:-1]):\n",
    "\n",
    "            start  = sp+1\n",
    "            end    = space_ids[spi+1]\n",
    "            word   = \"\".join(chars[start:end])\n",
    "\n",
    "            gold_stress = get_word_stress(cmu, word)\n",
    "            sys_stress  = \"\"\n",
    "\n",
    "            if len(gold_stress) == 0:\n",
    "                continue\n",
    "\n",
    "            for attni, attn in enumerate(attns):\n",
    "\n",
    "                for ch in range(start, end):\n",
    "\n",
    "                    if attn[ex][ch] >= attn_threshold: \n",
    "                        sys_stress += str(pentameter[attni])\n",
    "                        break\n",
    "\n",
    "            update_stress_accs(accs, (end-start), float(sys_stress in gold_stress))\n",
    "\n",
    "\n",
    "def eval_rhyme(pr, thresholds, cmu, attns, b, idxchar, charxid, pad_symbol, cf, cmu_rhyme=None, cmu_norhyme=None,\n",
    "    em_vocab=None, em_theta=None):\n",
    "\n",
    "    def syllable_to_rhyme(syllable):\n",
    "\n",
    "        stresses = set([\"0\", \"1\", \"2\"])\n",
    "        r = []\n",
    "        for s in reversed(syllable):\n",
    "            if s[-1] in stresses:\n",
    "                r.append(s[:-1])\n",
    "                break\n",
    "            else:\n",
    "                r.append(s)\n",
    "\n",
    "        return \"-\".join(list(reversed(r)))\n",
    "\n",
    "    def get_rhyme(words):\n",
    "\n",
    "        rhymes = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_rhymes = set([])\n",
    "            if word in cmu:\n",
    "                for res in cmu[word]:\n",
    "                    word_rhymes.add(syllable_to_rhyme(res))\n",
    "            rhymes.append(word_rhymes)\n",
    "\n",
    "        return rhymes\n",
    "\n",
    "    def rhyme_score(target_rhymes, context_rhymes):\n",
    "        \n",
    "        if len(target_rhymes) == 0 or len(context_rhymes) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            for tr in target_rhymes:\n",
    "                if tr in context_rhymes: \n",
    "                    return 1.0\n",
    "            return 0.0\n",
    "\n",
    "    def last_syllable(word):\n",
    "\n",
    "        i = len(word)\n",
    "        for c in reversed(word):\n",
    "            i -= 1\n",
    "            if c in get_vowels():\n",
    "                break\n",
    "\n",
    "        return word[i:]\n",
    "\n",
    "    def em_rhyme_score(x, y):\n",
    "\n",
    "        if x in em_vocab and y in em_vocab:\n",
    "            xi = em_vocab.index(x)\n",
    "            yi = em_vocab.index(y)\n",
    "\n",
    "            return max(em_theta[xi][yi], em_theta[yi][xi])\n",
    "    \n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    num_c = 3 + cf.rm_neg\n",
    "    for ex in range(cf.batch_size):\n",
    "        target  = \"\".join([idxchar[item] for item in b[0][ex][:b[1][ex]]])\n",
    "        context = []\n",
    "        for ci, c in enumerate(b[0][(ex*num_c+cf.batch_size):(ex*num_c+cf.batch_size+3)]):\n",
    "            context.append(\"\".join([idxchar[item] for item in c[:b[1][ex*num_c+cf.batch_size+ci]]]))\n",
    "\n",
    "        target_rhyme  = get_rhyme([target])[0]\n",
    "        context_rhyme = get_rhyme(context)\n",
    "\n",
    "        for t in thresholds:\n",
    "            for ci, c in enumerate(context):\n",
    "                score = rhyme_score(target_rhyme, context_rhyme[ci])\n",
    "                system_score = attns[ex][ci] if em_vocab == None else em_rhyme_score(target, context[ci])\n",
    "\n",
    "                #precision\n",
    "                if system_score >= t:\n",
    "                    if score != None:\n",
    "                        pr[t][0].append(score)\n",
    "                        \n",
    "                #recall\n",
    "                if score == 1.0:\n",
    "                    pr[t][1].append(float(system_score >= t))\n",
    "            \n",
    "                if cmu_rhyme != None and cmu_norhyme != None:\n",
    "                    if score == 1.0:\n",
    "                        if (target, context[ci]) not in cmu_rhyme and (context[ci], target) not in cmu_rhyme:\n",
    "                            cmu_rhyme[(target, context[ci])] = system_score\n",
    "                    elif score == 0.0:\n",
    "                        if (target, context[ci]) not in cmu_norhyme and (context[ci], target) not in cmu_norhyme:\n",
    "                            cmu_norhyme[(target, context[ci])] = system_score\n",
    "\n",
    "\n",
    "def collect_rhyme_pattern(rhyme_pattern, attentions, b, batch_size, num_context, idxchar, pad_id):\n",
    "\n",
    "    def get_context_line_id(target_line_id, context_id):\n",
    "        p = target_line_id % 4\n",
    "        q = 3-context_id\n",
    "        if q <= p:\n",
    "            q -= 1\n",
    "        return q\n",
    "\n",
    "    #print \"\\n\", \"=\"*100\n",
    "    for exid in range(batch_size):\n",
    "        target_line_id = b[2][exid]\n",
    "        for ci, c in enumerate(b[0][(exid*num_context+batch_size):((exid+1)*num_context+batch_size)][:3]):\n",
    "            context_line_id = get_context_line_id(target_line_id, ci)\n",
    "            rhyme_pattern[target_line_id][context_line_id].append(attentions[exid][ci])\n",
    "\n",
    "\n",
    "def postprocess_sentence(line):\n",
    "    cleaned_sent = \"\"\n",
    "    for w in line.strip().split():\n",
    "        spacing = \" \"\n",
    "        if w.startswith(\"'\") or only_symbol(w):\n",
    "            spacing = \"\"\n",
    "        cleaned_sent += spacing + w\n",
    "    return cleaned_sent.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "# from util import *\n",
    "from collections import Counter\n",
    "#from rnn_cell import ExtendedMultiRNNCell\n",
    "\n",
    "class SonnetModel(object):\n",
    "\n",
    "    def get_last_hidden(self, h, xlen):\n",
    "\n",
    "        ids = tf.range(tf.shape(xlen)[0])\n",
    "        gather_ids = tf.concat(axis=1, values=[tf.expand_dims(ids, 1), tf.expand_dims(xlen-1, 1)])\n",
    "        return tf.gather_nd(h, gather_ids)\n",
    "\n",
    "\n",
    "    def gated_layer(self, s, h, sdim, hdim):\n",
    "\n",
    "        update_w = tf.get_variable(\"update_w\", [sdim+hdim, hdim])\n",
    "        update_b = tf.get_variable(\"update_b\", [hdim], initializer=tf.constant_initializer(1.0))\n",
    "        reset_w  = tf.get_variable(\"reset_w\", [sdim+hdim, hdim])\n",
    "        reset_b  = tf.get_variable(\"reset_b\", [hdim], initializer=tf.constant_initializer(1.0))\n",
    "        c_w      = tf.get_variable(\"c_w\", [sdim+hdim, hdim])\n",
    "        c_b      = tf.get_variable(\"c_b\", [hdim], initializer=tf.constant_initializer())\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(tf.concat(axis=1, values=[s, h]), update_w) + update_b)\n",
    "        r = tf.sigmoid(tf.matmul(tf.concat(axis=1, values=[s, h]), reset_w) + reset_b)\n",
    "        c = tf.tanh(tf.matmul(tf.concat(axis=1, values=[s, r*h]), c_w) + c_b)\n",
    "        \n",
    "        return (1-z)*h + z*c\n",
    "\n",
    "\n",
    "    def selective_encoding(self, h, s, hdim):\n",
    "\n",
    "        h1 = tf.shape(h)[0]\n",
    "        h2 = tf.shape(h)[1]\n",
    "        h_ = tf.reshape(h, [-1, hdim])\n",
    "        s_ = tf.reshape(tf.tile(s, [1, h2]), [-1, hdim])\n",
    "\n",
    "        attend_w = tf.get_variable(\"attend_w\", [hdim*2, hdim])\n",
    "        attend_b = tf.get_variable(\"attend_b\", [hdim], initializer=tf.constant_initializer())\n",
    "\n",
    "        g = tf.sigmoid(tf.matmul(tf.concat(axis=1, values=[h_, s_]), attend_w) + attend_b)\n",
    "\n",
    "        return tf.reshape(h_* g, [h1, h2, -1])\n",
    "\n",
    "\n",
    "    def __init__(self, is_training, batch_size, word_type_size, char_type_size, space_id, pad_id, cf):\n",
    "\n",
    "        self.config = cf\n",
    "\n",
    "        ###########\n",
    "        #constants#\n",
    "        ###########\n",
    "        self.pentameter = [0,1]*5\n",
    "        self.pentameter_len = len(self.pentameter)\n",
    "\n",
    "        ##############\n",
    "        #placeholders#\n",
    "        ##############\n",
    "\n",
    "        #language model placeholders\n",
    "        self.lm_x    = tf.placeholder(tf.int32, [None, None])\n",
    "        self.lm_xlen = tf.placeholder(tf.int32, [None])\n",
    "        self.lm_y    = tf.placeholder(tf.int32, [None, None])\n",
    "        self.lm_hist = tf.placeholder(tf.int32, [None, None])\n",
    "        self.lm_hlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        #pentameter model placeholders\n",
    "        self.pm_enc_x    = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pm_enc_xlen = tf.placeholder(tf.int32, [None])\n",
    "        self.pm_cov_mask = tf.placeholder(tf.float32, [None, None])\n",
    "\n",
    "        #rhyme model placeholders\n",
    "        self.rm_num_context = tf.placeholder(tf.int32)\n",
    "\n",
    "        ##################\n",
    "        #pentameter model#\n",
    "        ##################\n",
    "#         with tf.variable_scope(\"pentameter_model\"):\n",
    "#             self.init_pm(is_training, batch_size, char_type_size, space_id, pad_id)\n",
    "\n",
    "        ################\n",
    "        #language model#\n",
    "        ################\n",
    "        with tf.variable_scope(\"language_model\"):\n",
    "            self.init_lm(is_training, batch_size, word_type_size)\n",
    "\n",
    "        #############\n",
    "        #rhyme model#\n",
    "        #############\n",
    "#         with tf.variable_scope(\"rhyme_model\"):\n",
    "#             self.init_rm(is_training, batch_size, char_type_size)\n",
    "\n",
    "       \n",
    "    # -- language model network\n",
    "    def init_lm(self, is_training, batch_size, word_type_size):\n",
    "\n",
    "        cf = self.config\n",
    "\n",
    "        #shared word embeddings (used by encoder and decoder)\n",
    "        self.word_embedding = tf.get_variable(\"word_embedding\", [word_type_size, cf.word_embedding_dim],\n",
    "            initializer=tf.random_uniform_initializer(-0.05/cf.word_embedding_dim, 0.05/cf.word_embedding_dim))\n",
    "    \n",
    "        #########\n",
    "        #decoder#\n",
    "        #########\n",
    "\n",
    "        #define lstm cells\n",
    "        lm_dec_cell = tf.nn.rnn_cell.LSTMCell(cf.lm_dec_dim, use_peepholes=True, forget_bias=1.0)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            lm_dec_cell = tf.nn.rnn_cell.DropoutWrapper(lm_dec_cell, output_keep_prob=cf.keep_prob)\n",
    "        self.lm_dec_cell = tf.nn.rnn_cell.MultiRNNCell([lm_dec_cell] * cf.lm_dec_layer_size)\n",
    "\n",
    "        #initial states\n",
    "        self.lm_initial_state = self.lm_dec_cell.zero_state(batch_size, tf.float32)\n",
    "        state = self.lm_initial_state\n",
    "\n",
    "        #pad symbol vocab ID = 0; create mask = 1.0 where vocab ID > 0 else 0.0\n",
    "        lm_mask = tf.cast(tf.greater(self.lm_x, tf.zeros(tf.shape(self.lm_x), dtype=tf.int32)), dtype=tf.float32)\n",
    "\n",
    "        #embedding lookup\n",
    "        word_inputs = tf.nn.embedding_lookup(self.word_embedding, self.lm_x)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            word_inputs = tf.nn.dropout(word_inputs, cf.keep_prob)\n",
    "\n",
    "        #process character encodings\n",
    "        #concat last hidden state of fw RNN with first hidden state of bw RNN\n",
    "        fw_hidden = self.get_last_hidden(self.char_encodings[0], self.pm_enc_xlen)\n",
    "        char_inputs = tf.concat(axis=1, values=[fw_hidden, self.char_encodings[1][:,0,:]])\n",
    "        char_inputs = tf.reshape(char_inputs, [batch_size, -1, cf.pm_enc_dim*2]) #reshape into same dimension as inputs\n",
    "        \n",
    "        #concat word and char encodings\n",
    "        inputs = tf.concat(axis=2, values=[word_inputs, char_inputs])\n",
    "        #inputs = word_inputs\n",
    "\n",
    "        #apply mask to zero out pad embeddings\n",
    "        inputs = inputs * tf.expand_dims(lm_mask, -1)\n",
    "\n",
    "        #dynamic rnn\n",
    "        dec_outputs, final_state = tf.nn.dynamic_rnn(self.lm_dec_cell, inputs, sequence_length=self.lm_xlen, \\\n",
    "            dtype=tf.float32, initial_state=self.lm_initial_state)\n",
    "        self.lm_final_state = final_state\n",
    "\n",
    "        #########################\n",
    "        #encoder (history words)#\n",
    "        #########################\n",
    "\n",
    "        #embedding lookup\n",
    "        hist_inputs = tf.nn.embedding_lookup(self.word_embedding, self.lm_hist)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            hist_inputs = tf.nn.dropout(hist_inputs, cf.keep_prob)\n",
    "\n",
    "        #encoder lstm cell\n",
    "        lm_enc_cell = tf.nn.rnn_cell.LSTMCell(cf.lm_enc_dim, forget_bias=1.0)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            lm_enc_cell = tf.nn.rnn_cell.DropoutWrapper(lm_enc_cell, output_keep_prob=cf.keep_prob)\n",
    "\n",
    "        #history word encodings\n",
    "        hist_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=lm_enc_cell, cell_bw=lm_enc_cell,\n",
    "            inputs=hist_inputs, sequence_length=self.lm_hlen, dtype=tf.float32)\n",
    "\n",
    "        #full history encoding\n",
    "        full_encoding = tf.concat(axis=1, values=[hist_outputs[0][:,-1,:], hist_outputs[1][:,0,:]])\n",
    "\n",
    "        #concat fw and bw hidden states\n",
    "        hist_outputs = tf.concat(axis=2, values=hist_outputs)\n",
    "\n",
    "        #selective encoding\n",
    "        with tf.variable_scope(\"selective_encoding\"):\n",
    "            hist_outputs = self.selective_encoding(hist_outputs, full_encoding, cf.lm_enc_dim*2)\n",
    "\n",
    "        #attention (concat)\n",
    "        with tf.variable_scope(\"lm_attention\"):\n",
    "            attend_w = tf.get_variable(\"attend_w\", [cf.lm_enc_dim*2+cf.lm_dec_dim, cf.lm_attend_dim])\n",
    "            attend_b = tf.get_variable(\"attend_b\", [cf.lm_attend_dim], initializer=tf.constant_initializer())\n",
    "            attend_v = tf.get_variable(\"attend_v\", [cf.lm_attend_dim, 1])\n",
    "\n",
    "        enc_steps = tf.shape(hist_outputs)[1]\n",
    "        dec_steps = tf.shape(dec_outputs)[1]\n",
    "\n",
    "        #prepare encoder and decoder\n",
    "        hist_outputs_t = tf.tile(hist_outputs, [1, dec_steps, 1])\n",
    "        dec_outputs_t  = tf.reshape(tf.tile(dec_outputs, [1, 1, enc_steps]),\n",
    "            [batch_size, -1, cf.lm_dec_dim])\n",
    "\n",
    "        #compute e\n",
    "        hist_dec_concat = tf.concat(axis=1, values=[tf.reshape(hist_outputs_t, [-1, cf.lm_enc_dim*2]),\n",
    "            tf.reshape(dec_outputs_t, [-1, cf.lm_dec_dim])])\n",
    "        e = tf.matmul(tf.tanh(tf.matmul(hist_dec_concat, attend_w) + attend_b), attend_v)\n",
    "        e = tf.reshape(e, [-1, enc_steps])\n",
    "\n",
    "        #mask out pad symbols to compute alpha and weighted sum of history words\n",
    "        alpha    = tf.reshape(tf.nn.softmax(e), [batch_size, -1, 1])\n",
    "        context  = tf.reduce_sum(tf.reshape(alpha * hist_outputs_t,\n",
    "            [batch_size,dec_steps,enc_steps,-1]), 2)\n",
    "\n",
    "        #save attention weights\n",
    "        self.lm_attentions = tf.reshape(alpha, [batch_size, dec_steps, enc_steps])\n",
    "\n",
    "        ##############\n",
    "        #output layer#\n",
    "        ##############\n",
    "\n",
    "        #reshape both into [batch_size*len, hidden_dim]\n",
    "        dec_outputs = tf.reshape(dec_outputs, [-1, cf.lm_dec_dim])\n",
    "        context     = tf.reshape(context, [-1, cf.lm_enc_dim*2])\n",
    "        \n",
    "        #combine context and decoder hidden state with a gated unit\n",
    "        with tf.variable_scope(\"gated_unit\"):\n",
    "            hidden = self.gated_layer(context, dec_outputs, cf.lm_enc_dim*2, cf.lm_dec_dim)\n",
    "            #hidden = dec_outputs\n",
    "\n",
    "        #output embeddings\n",
    "        lm_output_proj = tf.get_variable(\"lm_output_proj\", [cf.word_embedding_dim, cf.lm_dec_dim])\n",
    "        lm_softmax_b   = tf.get_variable(\"lm_softmax_b\", [word_type_size], initializer=tf.constant_initializer())\n",
    "        lm_softmax_w   = tf.transpose(tf.tanh(tf.matmul(self.word_embedding, lm_output_proj)))\n",
    "\n",
    "        #compute logits and cost\n",
    "        lm_logits     = tf.matmul(hidden, lm_softmax_w) + lm_softmax_b\n",
    "        lm_crossent   = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(self.lm_y, [-1]))\n",
    "        lm_crossent_m = lm_crossent * tf.reshape(lm_mask, [-1])\n",
    "        self.lm_cost  = tf.reduce_sum(lm_crossent_m) / batch_size\n",
    "\n",
    "        if not is_training:\n",
    "            self.lm_probs = tf.nn.softmax(lm_logits)\n",
    "            return\n",
    "\n",
    "        #run optimiser and backpropagate (clipped) gradients for lm loss\n",
    "        lm_tvars = tf.trainable_variables()\n",
    "        lm_grads, _ = tf.clip_by_global_norm(tf.gradients(self.lm_cost, lm_tvars), cf.max_grad_norm)\n",
    "        self.lm_train_op = tf.train.AdagradOptimizer(cf.lm_learning_rate).apply_gradients(zip(lm_grads, lm_tvars))\n",
    "\n",
    "\n",
    "#     # -- pentameter model network\n",
    "#     def init_pm(self, is_training, batch_size, char_type_size, space_id, pad_id):\n",
    "\n",
    "#         cf = self.config\n",
    "\n",
    "#         #character embeddings\n",
    "#         self.char_embedding = tf.get_variable(\"char_embedding\", [char_type_size, cf.char_embedding_dim],\n",
    "#             initializer=tf.random_uniform_initializer(-0.05/cf.char_embedding_dim, 0.05/cf.char_embedding_dim))\n",
    "\n",
    "#         #get bidirectional rnn states of the encoder\n",
    "#         enc_cell = tf.nn.rnn_cell.LSTMCell(cf.pm_enc_dim)#, forget_bias=1.0)\n",
    "#         if is_training and cf.keep_prob < 1.0:\n",
    "#             enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=cf.keep_prob)\n",
    "\n",
    "#         char_inputs    = tf.nn.embedding_lookup(self.char_embedding, self.pm_enc_x)\n",
    "#         enc_hiddens, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell, cell_bw=enc_cell, inputs=char_inputs,\n",
    "#             sequence_length=self.pm_enc_xlen, dtype=tf.float32)\n",
    "\n",
    "#         #save enc_hiddens\n",
    "#         self.char_encodings = enc_hiddens\n",
    "\n",
    "#         #reshape enc_hiddens\n",
    "#         enc_hiddens  = tf.reshape(tf.concat(axis=2, values=enc_hiddens), [-1, cf.pm_enc_dim*2]) #[batch_size*num_steps, hidden]\n",
    "\n",
    "#         #get decoder hidden states\n",
    "#         dec_cell = tf.nn.rnn_cell.LSTMCell(cf.pm_dec_dim)#, forget_bias=1.0)\n",
    "\n",
    "#         if is_training and cf.keep_prob < 1.0:\n",
    "#             dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=cf.keep_prob)\n",
    "\n",
    "#         #compute loss for pentameter\n",
    "#         self.pm_costs     = self.compute_pm_loss(is_training, batch_size, enc_hiddens, dec_cell, space_id, pad_id)\n",
    "#         self.pm_mean_cost = tf.reduce_sum(self.pm_costs) / batch_size\n",
    "\n",
    "#         if not is_training:\n",
    "#             return\n",
    "\n",
    "#         #run optimiser and backpropagate (clipped) gradients for pm loss\n",
    "#         pm_tvars         = tf.trainable_variables()\n",
    "#         pm_grads, _      = tf.clip_by_global_norm(tf.gradients(self.pm_mean_cost, pm_tvars), cf.max_grad_norm)\n",
    "#         self.pm_train_op = tf.train.AdamOptimizer(cf.pm_learning_rate).apply_gradients(zip(pm_grads, pm_tvars))\n",
    "\n",
    "\n",
    "#     # -- rhyme model network\n",
    "#     def init_rm(self, is_training, batch_size, char_type_size):\n",
    "\n",
    "#         cf = self.config\n",
    "\n",
    "#         #get char encodings\n",
    "#         rnn_cell = tf.nn.rnn_cell.LSTMCell(cf.rm_dim)#, forget_bias=1.0)\n",
    "#         if is_training and cf.keep_prob < 1.0:\n",
    "#             rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, output_keep_prob=cf.keep_prob)\n",
    "\n",
    "#         initial_state = rnn_cell.zero_state(tf.shape(self.pm_enc_x)[0], tf.float32)\n",
    "#         char_enc, _   = tf.nn.dynamic_rnn(rnn_cell, tf.nn.embedding_lookup(self.char_embedding, self.pm_enc_x),\n",
    "#             sequence_length=self.pm_enc_xlen, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "#         #get last hidden states\n",
    "#         char_enc = self.get_last_hidden(char_enc, self.pm_enc_xlen)\n",
    "        \n",
    "#         #slice it into target_words and context words\n",
    "#         target  = char_enc[:batch_size,:]\n",
    "#         context = char_enc[batch_size:,:]\n",
    "\n",
    "#         target_tiled   = tf.reshape(tf.tile(target, [1,self.rm_num_context]), [-1, cf.rm_dim])\n",
    "#         target_context = tf.concat(axis=1, values=[target_tiled, context])\n",
    "\n",
    "#         #cosine similarity\n",
    "#         e = tf.reduce_sum(tf.nn.l2_normalize(target_tiled, 1) * tf.nn.l2_normalize(context, 1), 1)\n",
    "#         e = tf.reshape(e, [batch_size, -1])\n",
    "\n",
    "#         #save the attentions\n",
    "#         self.rm_attentions = e\n",
    "\n",
    "#         #max margin loss\n",
    "#         min_cos = tf.nn.top_k(e, 2)[0][:, -1] #second highest cos similarity\n",
    "#         max_cos = tf.reduce_max(e, 1)\n",
    "#         self.rm_cost = tf.reduce_mean(tf.maximum(0.0, cf.rm_delta - max_cos + min_cos))\n",
    "\n",
    "#         if not is_training:\n",
    "#             return\n",
    "        \n",
    "#         self.rm_train_op = tf.train.AdamOptimizer(cf.rm_learning_rate).minimize(self.rm_cost)\n",
    "\n",
    "\n",
    "#     # -- compute pentameter model loss, given a pentameter input\n",
    "#     def compute_pm_loss(self, is_training, batch_size, enc_hiddens, dec_cell, space_id, pad_id):\n",
    "\n",
    "#         cf             = self.config\n",
    "#         xlen_max       = tf.reduce_max(self.pm_enc_xlen)\n",
    "\n",
    "#         #use decoder hidden states to select encoder hidden states to predict stress for next time step\n",
    "#         repeat_loss    = tf.zeros([batch_size])\n",
    "#         attentions     = tf.zeros([batch_size, xlen_max]) #historical attention weights\n",
    "#         prev_miu       = tf.zeros([batch_size,1])\n",
    "#         outputs        = []\n",
    "#         attention_list = []\n",
    "#         miu_list       = []\n",
    "\n",
    "#         #initial inputs (learnable) and state\n",
    "#         initial_inputs = tf.get_variable(\"dec_init_input\", [cf.pm_enc_dim*2])\n",
    "#         inputs         = tf.reshape(tf.tile(initial_inputs, [batch_size]), [batch_size, -1])\n",
    "#         state          = dec_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "#         #manual unroll of time steps because attention depends on previous attention weights\n",
    "#         with tf.variable_scope(\"RNN\"):\n",
    "#             for time_step in range(self.pentameter_len):\n",
    "\n",
    "#                 if time_step > 0:\n",
    "#                     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "#                 def attend(enc_hiddens, dec_hidden, attn_hist, prev_miu):\n",
    "#                     with tf.variable_scope(\"pm_attention\"):\n",
    "#                         attend_w = tf.get_variable(\"attend_w\", [cf.pm_enc_dim*2+cf.pm_dec_dim, cf.pm_attend_dim])\n",
    "#                         attend_b = tf.get_variable(\"attend_b\", [cf.pm_attend_dim], initializer=tf.constant_initializer())\n",
    "#                         attend_v = tf.get_variable(\"attend_v\", [cf.pm_attend_dim, 1])\n",
    "#                         miu_w    = tf.get_variable(\"miu_w\", [cf.pm_dec_dim+1, cf.pm_attend_dim])\n",
    "#                         miu_b    = tf.get_variable(\"miu_b\", [cf.pm_attend_dim], initializer=tf.constant_initializer())\n",
    "#                         miu_v    = tf.get_variable(\"miu_v\", [cf.pm_attend_dim, 1])\n",
    "                    \n",
    "#                     #position attention\n",
    "#                     miu     = tf.minimum(tf.sigmoid(tf.matmul(tf.tanh(tf.matmul(tf.concat(axis=1,\n",
    "#                         values=[dec_hidden, prev_miu]), miu_w) + miu_b), miu_v)) + prev_miu, tf.ones([batch_size, 1]))\n",
    "#                     miu_p   = miu * tf.reshape(tf.cast(self.pm_enc_xlen-1, tf.float32), [-1, 1])\n",
    "#                     pos     = tf.cast(tf.reshape(tf.tile(tf.range(xlen_max), [batch_size]), [batch_size, -1]),\n",
    "#                         dtype=tf.float32)\n",
    "#                     pos_lp  = -(pos - miu_p)**2 / (2 * tf.reshape(tf.tile([tf.square(cf.sigma)], [batch_size]),\n",
    "#                         [batch_size,-1]))\n",
    "            \n",
    "#                     #char encoding attention\n",
    "#                     pos_weight = tf.reshape(tf.exp(pos_lp), [-1, 1])\n",
    "#                     inp_concat = tf.concat(axis=1, values=[enc_hiddens * pos_weight,\n",
    "#                         tf.reshape(tf.tile(dec_hidden, [1,xlen_max]), [-1,cf.pm_dec_dim])])\n",
    "#                     x       = self.pm_enc_x\n",
    "#                     e       = tf.matmul(tf.tanh(tf.matmul(inp_concat, attend_w) + attend_b), attend_v)\n",
    "#                     e       = tf.reshape(e, [batch_size, xlen_max])\n",
    "#                     mask1   = tf.cast(tf.equal(x, tf.fill(tf.shape(x), pad_id)), dtype=tf.float32)\n",
    "#                     mask2   = tf.cast(tf.equal(x, tf.fill(tf.shape(x), space_id)), dtype=tf.float32)\n",
    "#                     e_mask  = tf.maximum(mask1, mask2)\n",
    "#                     e_mask *= tf.constant(-1e20)\n",
    "\n",
    "#                     #combine alpha with position probability\n",
    "#                     alpha   = tf.nn.softmax(e + e_mask + pos_lp)\n",
    "#                     #alpha   = tf.nn.softmax(e + e_mask)\n",
    "\n",
    "#                     #weighted sum\n",
    "#                     c       = tf.reduce_sum(tf.expand_dims(alpha, 2)*tf.reshape(enc_hiddens,\n",
    "#                         [batch_size, xlen_max, cf.pm_enc_dim*2]), 1)\n",
    "\n",
    "#                     return c, alpha, miu\n",
    "\n",
    "#                 dec_hidden, state               = dec_cell(inputs, state)\n",
    "#                 enc_hiddens_sum, attn, prev_miu = attend(enc_hiddens, dec_hidden, attentions, prev_miu)\n",
    "\n",
    "#                 repeat_loss += tf.reduce_sum(tf.minimum(attentions, attn), 1)\n",
    "#                 attentions  += attn\n",
    "#                 inputs       = enc_hiddens_sum\n",
    "\n",
    "#                 attention_list.append(attn)\n",
    "#                 miu_list.append(prev_miu)\n",
    "#                 outputs.append(enc_hiddens_sum)\n",
    "\n",
    "#         #reshape output into [batch_size*num_steps,hidden_size]\n",
    "#         outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, cf.pm_enc_dim*2])\n",
    "\n",
    "#         #compute loss\n",
    "#         pm_softmax_w = tf.get_variable(\"pm_softmax_w\", [cf.pm_enc_dim*2, 1])\n",
    "#         pm_softmax_b = tf.get_variable(\"pm_softmax_b\", [1], initializer=tf.constant_initializer())\n",
    "#         pm_logit     = tf.squeeze(tf.matmul(outputs, pm_softmax_w) + pm_softmax_b)\n",
    "#         pm_crossent  = tf.nn.sigmoid_cross_entropy_with_logits(logits=pm_logit,\n",
    "#             labels=tf.tile(tf.cast(self.pentameter, tf.float32), [batch_size]))\n",
    "#         cov_loss     = tf.reduce_sum(tf.nn.relu(self.pm_cov_mask*cf.cov_loss_threshold - attentions), 1)\n",
    "#         pm_cost      = tf.reduce_sum(tf.reshape(pm_crossent, [batch_size, -1]), 1) + \\\n",
    "#             cf.repeat_loss_scale*repeat_loss + cf.cov_loss_scale*cov_loss\n",
    "\n",
    "#         #save some variables\n",
    "#         self.pm_logits     = tf.sigmoid(tf.reshape(pm_logit, [batch_size, -1]))\n",
    "#         self.pm_attentions = attention_list\n",
    "#         self.mius          = miu_list\n",
    "\n",
    "#         return pm_cost\n",
    "\n",
    "\n",
    "    # -- sample a word given probability distribution (with option to normalise the distribution with temperature)\n",
    "    # -- temperature = 0 means argmax\n",
    "    def sample_word(self, sess, probs, temperature, unk_symbol_id, pad_symbol_id, wordxchar, idxword,\n",
    "        rm_target_pos, rm_target_neg, rm_threshold_pos, avoid_words):\n",
    "\n",
    "        def rhyme_pair_to_char(rhyme_pair):\n",
    "            char_ids, char_id_len = [], []\n",
    "\n",
    "            for word in rhyme_pair:\n",
    "                char_ids.append(wordxchar[word])\n",
    "                char_id_len.append(len(char_ids[-1]))\n",
    "\n",
    "            #pad char_ids\n",
    "            for ci, c in enumerate(char_ids):\n",
    "                char_ids[ci] = pad(c, max(char_id_len), pad_symbol_id)\n",
    "\n",
    "            return char_ids, char_id_len\n",
    "\n",
    "        def rhyme_cos(x, y):\n",
    "            rm_char, rm_char_len = rhyme_pair_to_char([x, y])\n",
    "\n",
    "            feed_dict = {self.pm_enc_x: rm_char, self.pm_enc_xlen: rm_char_len, self.rm_num_context: 1}\n",
    "            rm_attns  = sess.run(self.rm_attentions, feed_dict)\n",
    "\n",
    "            return rm_attns[0][0]\n",
    "\n",
    "        rm_threshold_neg = 0.7 #non-rhyming words A and B shouldn't have similarity larger than this threshold\n",
    "\n",
    "        if temperature == 0:\n",
    "            return np.argmax(probs)\n",
    "\n",
    "        probs = probs.astype(np.float64) #convert to float64 for higher precision\n",
    "        probs = np.log(probs) / temperature\n",
    "        probs = np.exp(probs) / math.fsum(np.exp(probs))\n",
    "\n",
    "        #avoid unk_symbol_id if possible\n",
    "        sampled = None\n",
    "        pw      = idxword[rm_target_pos] if rm_target_pos != None else \"None\"\n",
    "        nw      = idxword[rm_target_neg] if rm_target_neg != None else \"None\"\n",
    "        for i in range(1000):\n",
    "            sampled = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "\n",
    "            #resample if it's a word to avoid\n",
    "            if sampled in avoid_words:\n",
    "                continue\n",
    "\n",
    "            #if it needs to rhyme, resample until we find a rhyming word\n",
    "            if rm_threshold_pos != 0.0 and rm_target_pos != None and rhyme_cos(sampled, rm_target_pos) < rm_threshold_pos:\n",
    "                continue\n",
    "\n",
    "            if rm_target_neg != None and rhyme_cos(sampled, rm_target_neg) > rm_threshold_neg:\n",
    "                continue\n",
    "\n",
    "            return sampled\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    # -- generate a sentence by sampling one word at a time\n",
    "    def sample_sent(self, sess, state, x, hist, hlen, xchar, xchar_len, avoid_symbols, stopwords,temp_min, temp_max,\n",
    "        unk_symbol_id, pad_symbol_id, end_symbol_id, space_id, idxchar, charxid, idxword, wordxchar,\n",
    "        rm_target_pos, rm_target_neg, rm_threshold, last_words, max_words):\n",
    "\n",
    "        def filter_stop_symbol(word_ids):\n",
    "            cleaned = set([])\n",
    "            for w in word_ids:\n",
    "                if w not in (stopwords | set([pad_symbol_id, end_symbol_id])) and not only_symbol(idxword[w]):\n",
    "                    cleaned.add(w)\n",
    "            return cleaned\n",
    "\n",
    "        def get_freq_words(word_ids, freq_threshold):\n",
    "            words     = []\n",
    "            word_freq = Counter(word_ids)\n",
    "            for k, v in word_freq.items():\n",
    "                #if v >= freq_threshold and not only_symbol(idxword[k]) and k != end_symbol_id:\n",
    "                if v >= freq_threshold and k != end_symbol_id:\n",
    "                    words.append(k)\n",
    "            return set(words)\n",
    "\n",
    "        sent   = []\n",
    "\n",
    "        while True:\n",
    "            probs, state = sess.run([self.lm_probs, self.lm_final_state],\n",
    "                {self.lm_x: x, self.lm_initial_state: state, self.lm_xlen: [1],\n",
    "                self.lm_hist: hist, self.lm_hlen: hlen,\n",
    "                self.pm_enc_x: xchar, self.pm_enc_xlen: xchar_len})\n",
    "\n",
    "            #avoid words previously generated            \n",
    "            avoid_words = filter_stop_symbol(sent + hist[0])\n",
    "            freq_words  = get_freq_words(sent + hist[0], 2) #avoid any words that occur >= N times\n",
    "            avoid_words = avoid_words | freq_words | set(sent[-3:] + last_words + avoid_symbols + [unk_symbol_id])\n",
    "            #avoid_words = set(sent[-3:] + last_words + avoid_symbols + [unk_symbol_id])\n",
    "\n",
    "            word = self.sample_word(sess, probs[0], np.random.uniform(temp_min, temp_max), unk_symbol_id,\n",
    "                pad_symbol_id, wordxchar, idxword, rm_target_pos, rm_target_neg, rm_threshold, avoid_words)\n",
    "\n",
    "            if word != None:\n",
    "                sent.append(word)\n",
    "                x             = [[ sent[-1] ]]\n",
    "                xchar         = [wordxchar[sent[-1]]]\n",
    "                xchar_len     = [len(xchar[0])]\n",
    "                rm_target_pos = None\n",
    "                rm_target_neg = None\n",
    "            else:\n",
    "                return None, None, None\n",
    "\n",
    "            if sent[-1] == end_symbol_id or len(sent) >= max_words:\n",
    "\n",
    "                if len(sent) > 1:\n",
    "                    pm_loss  = self.eval_pm_loss(sess, sent, end_symbol_id, space_id, idxchar, charxid, idxword, wordxchar)\n",
    "                    return sent, state, pm_loss\n",
    "                else:\n",
    "                    return None, None, None\n",
    "    \n",
    "\n",
    "    # -- compute pentameter loss\n",
    "    def eval_pm_loss(self, sess, sent, end_symbol, space_id, idxchar, charxid, idxword, wordxchar):\n",
    "\n",
    "        def sent_to_char(words):\n",
    "            char_ids = []\n",
    "\n",
    "            for word in reversed(words):\n",
    "                if word != end_symbol:\n",
    "                    char_ids.extend(wordxchar[word])\n",
    "                    char_ids.append(space_id)\n",
    "\n",
    "            #remove punctuation\n",
    "            chars = \"\".join([ idxchar[item] for item in char_ids])\n",
    "            char_ids = [charxid[item] for item in remove_punct(chars)]\n",
    "\n",
    "            return char_ids\n",
    "\n",
    "        #pentameter check\n",
    "        pm_char  = sent_to_char(sent)\n",
    "        cov_mask = coverage_mask(pm_char, idxchar)\n",
    "\n",
    "        #create pseudo batch\n",
    "        b = ([pm_char], [len(pm_char)], [cov_mask])\n",
    "\n",
    "        feed_dict = {self.pm_enc_x: b[0], self.pm_enc_xlen: b[1], self.pm_cov_mask: b[2]}\n",
    "        pm_attns, pm_costs, logits, mius = sess.run([self.pm_attentions, self.pm_costs, self.pm_logits,\n",
    "            self.mius], feed_dict)\n",
    "\n",
    "        return pm_costs[0]\n",
    "\n",
    "\n",
    "    # -- quatrain generation\n",
    "    def generate(self, sess, idxword, idxchar, charxid, wordxchar, pad_symbol_id, end_symbol_id, unk_symbol_id, space_id,\n",
    "        avoid_symbols, stopwords, temp_min, temp_max, max_lines, max_words, sent_sample, rm_threshold, verbose=False):\n",
    "\n",
    "        def reset():\n",
    "            rhyme_aabb  = ([None, 0, None, 2], [None, None, 1, None])\n",
    "            rhyme_abab  = ([None, None, 0, 1], [None, 0, None, None])\n",
    "            rhyme_abba  = ([None, None, 1, 0], [None, 0, None, None])\n",
    "            rhyme_pttn  = random.choice([rhyme_aabb, rhyme_abab, rhyme_abba])\n",
    "            state       = sess.run(self.lm_dec_cell.zero_state(1, tf.float32))\n",
    "            prev_state  = state\n",
    "            x           = [[end_symbol_id]]\n",
    "            xchar       = [wordxchar[end_symbol_id]]\n",
    "            xchar_len   = [1]\n",
    "            sonnet      = []\n",
    "            sent_probs  = []\n",
    "            last_words  = []\n",
    "            total_words = 0\n",
    "            total_lines = 0\n",
    "            \n",
    "            return state, prev_state, x, xchar, xchar_len, sonnet, sent_probs, last_words, total_words, total_lines, \\\n",
    "                rhyme_pttn[0], rhyme_pttn[1]\n",
    "\n",
    "        end_symbol = idxword[end_symbol_id]\n",
    "        sent_temp  = 0.1 #sentence sampling temperature\n",
    "\n",
    "        state, prev_state, x, xchar, xchar_len, sonnet, sent_probs, last_words, total_words, total_lines, \\\n",
    "            rhyme_pttn_pos, rhyme_pttn_neg = reset()\n",
    "\n",
    "        #verbose prints during generation\n",
    "        if verbose:\n",
    "            sys.stdout.write(\"  Number of generated lines = 0/4\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        while total_words < max_words and total_lines < max_lines:\n",
    "\n",
    "            #add history context\n",
    "            if len(sonnet) == 0 or sonnet.count(end_symbol_id) < 1:\n",
    "                hist = [[unk_symbol_id] + [pad_symbol_id]*5]\n",
    "            else:\n",
    "                hist = [sonnet + [pad_symbol_id]*5]\n",
    "            hlen = [len(hist[0])]\n",
    "\n",
    "            \"\"\"\n",
    "            print \"\\n\\n\", \"=\"*80\n",
    "            print \"state =\", state[0][1][0][:10]\n",
    "            print \"prev state =\", prev_state[0][1][0][:10]\n",
    "            print \"x =\", x, idxword[x[0][0]]\n",
    "            print \"hist =\", hist, \" \".join(idxword[item] for item in hist[0])\n",
    "            print \"sonnet =\", sonnet, \" \".join(idxword[item] for item in sonnet)\n",
    "            \"\"\"\n",
    "\n",
    "            #get rhyme targets for the 'first' word\n",
    "            rm_target_pos, rm_target_neg = None, None\n",
    "            if rhyme_pttn_pos[total_lines] != None:\n",
    "                rm_target_pos = last_words[rhyme_pttn_pos[total_lines]]\n",
    "            if rhyme_pttn_neg[total_lines] != None:\n",
    "                rm_target_neg = last_words[rhyme_pttn_neg[total_lines]]\n",
    "\n",
    "            #genereate N sentences and sample from them (using softmax(-one_pl) as probability)\n",
    "            all_sent, all_state, all_pl = [], [], []\n",
    "            for _ in range(sent_sample):\n",
    "                one_sent, one_state, one_pl = self.sample_sent(sess, state, x, hist, hlen, xchar, xchar_len,\n",
    "                    avoid_symbols, stopwords, temp_min, temp_max, unk_symbol_id, pad_symbol_id, end_symbol_id, space_id,\n",
    "                    idxchar, charxid, idxword, wordxchar, rm_target_pos, rm_target_neg, rm_threshold, last_words, max_words)\n",
    "                if one_sent != None:\n",
    "                    all_sent.append(one_sent)\n",
    "                    all_state.append(one_state)\n",
    "                    all_pl.append(-one_pl)\n",
    "                else:\n",
    "                    all_sent = []\n",
    "                    break\n",
    "\n",
    "            #unable to generate sentences; reset whole quatrain\n",
    "            if len(all_sent) == 0:\n",
    "\n",
    "                state, prev_state, x, xchar, xchar_len, sonnet, sent_probs, last_words, total_words, total_lines, \\\n",
    "                    rhyme_pttn_pos, rhyme_pttn_neg = reset()\n",
    "\n",
    "            else:\n",
    "\n",
    "                #convert pm_loss to probability using softmax\n",
    "                probs = np.exp(np.array(all_pl)/sent_temp)\n",
    "                probs = probs.astype(np.float64) #convert to float64 for higher precision\n",
    "                probs = probs / math.fsum(probs)\n",
    "\n",
    "                #sample a sentence\n",
    "                sent_id = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "                sent    = all_sent[sent_id]\n",
    "                state   = all_state[sent_id]\n",
    "                pl      = all_pl[sent_id]\n",
    "\n",
    "                total_words += len(sent)\n",
    "                total_lines += 1\n",
    "                prev_state   = state\n",
    "\n",
    "                sonnet.extend(sent)\n",
    "                sent_probs.append(-pl)\n",
    "                last_words.append(sent[0])\n",
    "\n",
    "            if verbose:\n",
    "                sys.stdout.write(\"  Number of generated lines = %d/4\\r\" % (total_lines))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        #postprocessing\n",
    "        sonnet = sonnet[:-1] if sonnet[-1] == end_symbol_id else sonnet\n",
    "        sonnet = [ postprocess_sentence(item) for item in \\\n",
    "            \" \".join(list(reversed([ idxword[item] for item in sonnet ]))).strip().split(end_symbol) ]\n",
    "\n",
    "        return sonnet, list(reversed(sent_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(string):\n",
    "    return \" \".join(\"\".join([ item for item in string if (item.isalpha() or item == \" \") ]).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import codecs\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_vocab(corpus, word_minfreq, dummy_symbols):\n",
    "    idxword, idxchar = [], []\n",
    "    wordxid, charxid = defaultdict(int), defaultdict(int)\n",
    "    word_freq, char_freq = defaultdict(int), defaultdict(int)\n",
    "    wordxchar = defaultdict(list)\n",
    "\n",
    "    def update_dic(symbol, idxvocab, vocabxid):\n",
    "        if symbol not in vocabxid:\n",
    "            idxvocab.append(symbol)\n",
    "            vocabxid[symbol] = len(idxvocab) - 1 \n",
    "\n",
    "    for line_id, line in enumerate(codecs.open(corpus, \"r\", \"utf-8\")):\n",
    "        for word in line.strip().split():\n",
    "            word_freq[word] += 1\n",
    "        for char in line.strip():\n",
    "            char_freq[char] += 1\n",
    "\n",
    "    #add in dummy symbols into dictionaries\n",
    "    for s in dummy_symbols:\n",
    "        update_dic(s, idxword, wordxid)\n",
    "        update_dic(s, idxchar, charxid)\n",
    "\n",
    "    #remove low fequency words/chars\n",
    "    def collect_vocab(vocab_freq, idxvocab, vocabxid):\n",
    "        for w, f in sorted(list(vocab_freq.items()), key=operator.itemgetter(1), reverse=True):\n",
    "            if f < word_minfreq:\n",
    "                break\n",
    "            else:\n",
    "                update_dic(w, idxvocab, vocabxid)\n",
    "\n",
    "    collect_vocab(word_freq, idxword, wordxid)\n",
    "    collect_vocab(char_freq, idxchar, charxid)\n",
    "\n",
    "    #word id to [char ids]\n",
    "    dummy_symbols_set = set(dummy_symbols)\n",
    "    for wi, w in enumerate(idxword):\n",
    "        if w in dummy_symbols:\n",
    "            wordxchar[wi] = [wi]\n",
    "        else:\n",
    "            for c in w:\n",
    "                wordxchar[wi].append(charxid[c] if c in charxid else charxid[dummy_symbols[2]])\n",
    "\n",
    "    return idxword, wordxid, idxchar, charxid, wordxchar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First pass to collect word and character vocabulary...\n",
      "\n",
      "Word type size = 8398\n",
      "\n",
      "Char type size = 77\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "print(\"\\nFirst pass to collect word and character vocabulary...\")\n",
    "idxword, wordxid, idxchar, charxid, wordxchar = load_vocab(cf.train_data, cf.word_minfreq, dummy_symbols)\n",
    "print(\"\\nWord type size =\", len(idxword))\n",
    "print(\"\\nChar type size =\", len(idxchar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del load_data\n",
    "def load_data(corpus, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol):\n",
    "    nwords     = [] #number of words for each line\n",
    "    nchars     = [] #number of chars for each line\n",
    "    word_data  = [] #data[doc_id][0][line_id] = list of word ids; data[doc_id][1][line_id] = list of [char_ids]\n",
    "    char_data  = [] #data[line_id] = list of char ids\n",
    "    rhyme_data = [] #list of ( target_word, [candidate_words], target_word_line_id ); word is a list of characters\n",
    "\n",
    "    def word_to_char(word):\n",
    "        if word in set([pad_symbol, end_symbol, unk_symbol]):\n",
    "            return [ wordxid[word] ]\n",
    "        else:\n",
    "            return [ charxid[item] if item in charxid else charxid[unk_symbol] for item in word ]\n",
    "\n",
    "\n",
    "    for doc in codecs.open(corpus, \"r\", \"utf-8\"):\n",
    "\n",
    "        word_lines, char_lines = [[], []], []\n",
    "        last_words = []\n",
    "\n",
    "         #reverse the order of lines and words as we are generating from end to start\n",
    "        for line in reversed(doc.strip().split(end_symbol)):\n",
    "            print(line)\n",
    "\n",
    "            if len(line.strip()) > 0:\n",
    "\n",
    "                word_seq = [ wordxid[item] if item in wordxid else wordxid[unk_symbol] \\\n",
    "                    for item in reversed(line.strip().split()) ] + [wordxid[end_symbol]]\n",
    "\n",
    "                char_seq = [ word_to_char(item) for item in reversed(line.strip().split()) ] + [word_to_char(end_symbol)]\n",
    "\n",
    "                word_lines[0].append(word_seq)\n",
    "                word_lines[1].append(char_seq)\n",
    "                char_lines.append([ charxid[item] if item in charxid else charxid[unk_symbol] \\\n",
    "                    for item in remove_punct(line.strip())])\n",
    "                nwords.append(len(word_lines[0][-1]))\n",
    "                nchars.append(len(char_lines[-1]))\n",
    "\n",
    "                last_words.append(line.strip().split()[-1])\n",
    "                \n",
    "                print('1', last_words)\n",
    "\n",
    "        if len(word_lines[0]) == 14: #14 lines for sonnets\n",
    "\n",
    "            word_data.append(word_lines)\n",
    "            char_data.extend(char_lines)\n",
    "\n",
    "            last_words = last_words[2:] #remove couplets (since they don't always rhyme)\n",
    "\n",
    "            for wi, w in enumerate(last_words):\n",
    "                print('+++', wi, w)\n",
    "                rhyme_data.append((word_to_char(w),\n",
    "                                   [word_to_char(item)\n",
    "                                    for item_id, item in enumerate(last_words[(wi // 4) * 4:(wi // 4 + 1) * 4])\n",
    "                                    if item_id != (wi % 4)], (11 - wi)))\n",
    "\n",
    "    return word_data, char_data, rhyme_data, nwords, nchars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(partition, word_data, rhyme_data, nwords, nchars):\n",
    "    print(partition, \"statistics:\")\n",
    "    print(\"  Number of documents         =\", len(word_data))\n",
    "    print(\"  Number of rhyme examples    =\", len(rhyme_data))\n",
    "    print(\"  Total number of word tokens =\", sum(nwords))\n",
    "    print(\"  Mean/min/max words per line = %.2f/%d/%d\" % (np.mean(nwords), min(nwords), max(nwords)))\n",
    "    print(\"  Total number of char tokens =\", sum(nchars))\n",
    "    print(\"  Mean/min/max chars per line = %.2f/%d/%d\" % (np.mean(nchars), min(nchars), max(nchars)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading word embedding model...\n",
      "\n",
      "First pass to collect word and character vocabulary...\n",
      "\n",
      "Word type size = 8398\n",
      "\n",
      "Char type size = 77\n",
      "\n",
      "Loading train and valid data...\n",
      "\n",
      " a curse now dumb upon the lips of night \n",
      "1 ['night']\n",
      " uttered by death in hate of heaven and light \n",
      "1 ['night', 'light']\n",
      " hell yawns on him whose life was as a word \n",
      "1 ['night', 'light', 'word']\n",
      " take him , for he is yours , o night and death \n",
      "1 ['night', 'light', 'word', 'death']\n",
      " that seemed so long but weak and wasted breath \n",
      "1 ['night', 'light', 'word', 'death', 'breath']\n",
      " the bond is cancelled , and the prayer is heard \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard']\n",
      " in all the days whereof his eye takes ken \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken']\n",
      " that shall behold not so abhorred an one \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one']\n",
      " nor this face look upon the living sun \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun']\n",
      " shall this curse come upon the sins of men \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun', 'men']\n",
      " we grudge not now , who know that not again \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun', 'men', 'again']\n",
      " your king , your priest , your saviour , and your son \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun', 'men', 'again', 'son']\n",
      " when in man 's sight he stood not yet undone \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun', 'men', 'again', 'son', 'undone']\n",
      "o night and death , to whom we grudged him then \n",
      "1 ['night', 'light', 'word', 'death', 'breath', 'heard', 'ken', 'one', 'sun', 'men', 'again', 'son', 'undone', 'then']\n",
      "+++ 0 word\n",
      "+++ 1 death\n",
      "+++ 2 breath\n",
      "+++ 3 heard\n",
      "+++ 4 ken\n",
      "+++ 5 one\n",
      "+++ 6 sun\n",
      "+++ 7 men\n",
      "+++ 8 again\n",
      "+++ 9 son\n",
      "+++ 10 undone\n",
      "+++ 11 then\n",
      "\n",
      " that he might there its inmost thoughts unroll \n",
      "1 ['unroll']\n",
      " before his own she laid her very soul \n",
      "1 ['unroll', 'soul']\n",
      " whose restless strength had swayed her fragile form \n",
      "1 ['unroll', 'soul', 'form']\n",
      " of that wild spirit born amid the storm \n",
      "1 ['unroll', 'soul', 'form', 'storm']\n",
      " she told him of those visions never dim \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim']\n",
      " and there with rapt eyes looking up to him \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him']\n",
      " with blushing , beauteous charms of maidenhood \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood']\n",
      " and undismayed she moved to where he stood \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood']\n",
      " feared not the chidings of his hasty youth \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth']\n",
      " she who had scorned the tempest dark and wild \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth', 'wild']\n",
      " she met his eyes that searched her own for truth \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth', 'wild', 'truth']\n",
      " then fearlessly as some undaunted child \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth', 'wild', 'truth', 'child']\n",
      " to think that to reprove her he should dare \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth', 'wild', 'truth', 'child', 'dare']\n",
      "she looked at him with almost haughty air \n",
      "1 ['unroll', 'soul', 'form', 'storm', 'dim', 'him', 'maidenhood', 'stood', 'youth', 'wild', 'truth', 'child', 'dare', 'air']\n",
      "+++ 0 form\n",
      "+++ 1 storm\n",
      "+++ 2 dim\n",
      "+++ 3 him\n",
      "+++ 4 maidenhood\n",
      "+++ 5 stood\n",
      "+++ 6 youth\n",
      "+++ 7 wild\n",
      "+++ 8 truth\n",
      "+++ 9 child\n",
      "+++ 10 dare\n",
      "+++ 11 air\n",
      "\n",
      " with whose ideas reason never suits \n",
      "1 ['suits']\n",
      " such my conclusion , spite of fools or brutes \n",
      "1 ['suits', 'brutes']\n",
      " then how can cuckoldom an evil be \n",
      "1 ['suits', 'brutes', 'be']\n",
      " no difference assuredly you see \n",
      "1 ['suits', 'brutes', 'be', 'see']\n",
      " the features , too , they as before appear \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear']\n",
      " not e'en a spot ; there 's nothing half so clear \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear']\n",
      " and from it do you aught amiss retain \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain']\n",
      " pray , do n't it presently at ease remain \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain']\n",
      " less gently rather than 's your usual case \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain', 'case']\n",
      " your hat upon your head for instance place \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain', 'case', 'place']\n",
      " is only in the mind : mere spright of air \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain', 'case', 'place', 'air']\n",
      " oft weighs you down with soul corroding care \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain', 'case', 'place', 'air', 'care']\n",
      " that evil such as this , and which you say \n",
      "1 ['suits', 'brutes', 'be', 'see', 'appear', 'clear', 'retain', 'remain', 'case', 'place"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 5000 exceeded with 43570 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global wordxid, idxword, charxid, idxchar, \\\n",
    "wordxchar, train_lm, train_pm, train_rm\n",
    "\n",
    "random.seed(cf.seed)\n",
    "np.random.seed(cf.seed)\n",
    "\n",
    "if cf.word_embedding_model:\n",
    "    print(\"\\nLoading word embedding model...\")\n",
    "    mword = g.Word2Vec.load(cf.word_embedding_model)\n",
    "    cf.word_embedding_dim = mword.vector_size\n",
    "\n",
    "# load vocab\n",
    "print(\"\\nFirst pass to collect word and character vocabulary...\")\n",
    "idxword, wordxid, idxchar, charxid, wordxchar = load_vocab(cf.train_data, cf.word_minfreq, dummy_symbols)\n",
    "print(\"\\nWord type size =\", len(idxword))\n",
    "print(\"\\nChar type size =\", len(idxchar))\n",
    "\n",
    "\n",
    "# load train and valid data\n",
    "print(\"\\nLoading train and valid data...\")\n",
    "\n",
    "train_word_data, train_char_data, train_rhyme_data, train_nwords, train_nchars = \\\n",
    "    load_data(cf.train_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "valid_word_data, valid_char_data, valid_rhyme_data, valid_nwords, valid_nchars = \\\n",
    "    load_data(cf.valid_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "print_stats(\"\\nTrain\", train_word_data, train_rhyme_data, train_nwords, train_nchars)\n",
    "print_stats(\"\\nValid\", valid_word_data, valid_rhyme_data, valid_nwords, valid_nchars)\n",
    "\n",
    "# load test data if it's given\n",
    "if cf.test_data:\n",
    "    test_word_data, test_char_data, test_rhyme_data, test_nwords, test_nchars = \\\n",
    "        load_data(cf.test_data, wordxid, idxword, charxid, idxchar, pad_symbol, end_symbol, unk_symbol)\n",
    "    print_stats(\"\\nTest\", test_word_data, test_rhyme_data, test_nwords, test_nchars)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(sess, word_batches, char_batches, rhyme_batches, model, pname, is_training):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # lm variables\n",
    "    lm_costs = 0.0\n",
    "    total_words = 0\n",
    "    zero_state = sess.run(model.lm_initial_state)\n",
    "    model_state = None\n",
    "    prev_doc = -1\n",
    "    lm_train_op = model.lm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    # pm variables\n",
    "    pm_costs = 0.0\n",
    "    pm_train_op = model.pm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    # rm variables\n",
    "    rm_costs = 0.0\n",
    "    rm_train_op = model.rm_train_op if is_training else tf.no_op()\n",
    "\n",
    "    # mix lm and pm batches\n",
    "    mixed_batch_types = [0] * len(word_batches) + [1] * len(char_batches) + [2] * len(rhyme_batches)\n",
    "    random.shuffle(mixed_batch_types)\n",
    "    mixed_batches = [word_batches, char_batches, rhyme_batches]\n",
    "\n",
    "    word_batch_id = 0\n",
    "    char_batch_id = 0\n",
    "    rhyme_batch_id = 0\n",
    "\n",
    "    # cmu pronounciation dictionary for stress and rhyme evaluation\n",
    "    cmu = cmudict.dict()\n",
    "\n",
    "    # stress prediction\n",
    "    stress_acc = [[], [], []]  # buckets for char length: [1-4], [5-8], [9-inf]\n",
    "\n",
    "    # rhyme predition\n",
    "    rhyme_pr = {}  # precision/recall for each rhyme threshold\n",
    "    for rt in rhyme_thresholds:\n",
    "        rhyme_pr[rt] = [[], []]\n",
    "\n",
    "    # rhyme pattern\n",
    "    rhyme_pattern = []  # collection of cosine similarities\n",
    "    for i in range(12):\n",
    "        rhyme_pattern.append([])\n",
    "        for j in range(4):\n",
    "            if i % 4 == j:\n",
    "                rhyme_pattern[i].append([-2.0])\n",
    "            else:\n",
    "                rhyme_pattern[i].append([])\n",
    "                \n",
    "    print('stress_acc', stress_acc)\n",
    "    print('rhyme_pr', rhyme_pr)\n",
    "    print('rhyme_pattern', rhyme_pattern)\n",
    "    \n",
    "    print('mixed_batch_types', mixed_batch_types)\n",
    "    print('len mixed_batch_types', len(mixed_batch_types))\n",
    "\n",
    "    for bi, batch_type in tqdm(enumerate(mixed_batch_types)):\n",
    "\n",
    "        if batch_type == 0 and train_lm:\n",
    "\n",
    "            b = mixed_batches[batch_type][word_batch_id]\n",
    "\n",
    "            # reset model state if it's a different set of documents\n",
    "            if prev_doc != b[2][0]:\n",
    "                model_state = zero_state\n",
    "                prev_doc = b[2][0]\n",
    "\n",
    "            # preprocess character input to [batch_size*doc_len, char_len]\n",
    "            pm_enc_x = np.array(b[5]).reshape((cf.batch_size * max(b[3]), -1))\n",
    "\n",
    "            feed_dict = {model.lm_x: b[0], \n",
    "                         model.lm_y: b[1], \n",
    "                         model.lm_xlen: b[3], \n",
    "                         model.pm_enc_x: pm_enc_x,\n",
    "                         model.pm_enc_xlen: np.array(b[6]).reshape((-1)), \n",
    "                         model.lm_initial_state: model_state,\n",
    "                         model.lm_hist: b[7], \n",
    "                         model.lm_hlen: b[8]}\n",
    "\n",
    "            cost, model_state, attns, _ = sess.run([model.lm_cost, \n",
    "                                                    model.lm_final_state, \n",
    "                                                    model.lm_attentions, \n",
    "                                                    lm_train_op],\n",
    "                                                   feed_dict)\n",
    "\n",
    "            lm_costs += cost * cf.batch_size  # keep track of full cost\n",
    "            total_words += sum(b[3])\n",
    "\n",
    "            word_batch_id += 1\n",
    "\n",
    "#         elif batch_type == 1 and train_pm:\n",
    "\n",
    "#             b = mixed_batches[batch_type][char_batch_id]\n",
    "\n",
    "#             feed_dict = {model.pm_enc_x: b[0], model.pm_enc_xlen: b[1], model.pm_cov_mask: b[2]}\n",
    "#             cost, attns, _, = sess.run([model.pm_mean_cost, model.pm_attentions, pm_train_op], feed_dict)\n",
    "#             pm_costs += cost\n",
    "\n",
    "#             char_batch_id += 1\n",
    "\n",
    "#             if not is_training:\n",
    "#                 eval_stress(stress_acc, cmu, attns, model.pentameter, b[0], idxchar, charxid, pad_symbol, cf)\n",
    "\n",
    "#         elif batch_type == 2 and train_rm:\n",
    "\n",
    "#             b = mixed_batches[batch_type][rhyme_batch_id]\n",
    "#             num_c = 3 + cf.rm_neg\n",
    "\n",
    "#             feed_dict = {model.pm_enc_x: b[0], model.pm_enc_xlen: b[1], model.rm_num_context: num_c}\n",
    "#             cost, attns, _ = sess.run([model.rm_cost, model.rm_attentions, rm_train_op], feed_dict)\n",
    "#             rm_costs += cost\n",
    "\n",
    "#             rhyme_batch_id += 1\n",
    "\n",
    "#             # if rhyme_batch_id < 10 and not is_training:\n",
    "#             #    print_rm_attention(b, cf.batch_size, num_c, attns, charxid[pad_symbol], idxchar)\n",
    "\n",
    "#             if not is_training:\n",
    "#                 eval_rhyme(rhyme_pr, rhyme_thresholds, cmu, attns, b, idxchar, charxid, pad_symbol, cf)\n",
    "#             else:\n",
    "#                 collect_rhyme_pattern(rhyme_pattern, attns, b, cf.batch_size, num_c, idxchar, charxid[pad_symbol])\n",
    "\n",
    "        if (((bi % 10) == 0) and cf.verbose) or (bi == len(mixed_batch_types) - 1):\n",
    "\n",
    "            partition = \"  \" + pname\n",
    "            sent_end = \"\\n\" if bi == (len(mixed_batch_types) - 1) else \"\\r\"\n",
    "            speed = (bi + 1) / (time.time() - start_time)\n",
    "\n",
    "            sys.stdout.write(\"%s %d/%d: lm ppl = %.1f; pm loss = %.2f; rm loss = %.2f; batch/sec = %.1f%s\" % \\\n",
    "                             (partition, bi + 1, len(mixed_batch_types), np.exp(lm_costs / max(total_words, 1)),\n",
    "                              pm_costs / max(char_batch_id, 1), rm_costs / max(rhyme_batch_id, 1), speed, sent_end))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if not is_training and (bi == len(mixed_batch_types) - 1):\n",
    "\n",
    "                if train_pm:\n",
    "                    all_acc = [item for sublist in stress_acc for item in sublist]\n",
    "                    stress_acc.append(all_acc)\n",
    "                    for acci, acc in enumerate(stress_acc):\n",
    "                        sys.stdout.write(\"    Stress acc [%d]   = %.3f (%d)\\n\" % (acci, np.mean(acc), len(acc)))\n",
    "\n",
    "                if train_rm:\n",
    "                    for t in rhyme_thresholds:\n",
    "                        p = np.mean(rhyme_pr[t][0])\n",
    "                        r = np.mean(rhyme_pr[t][1])\n",
    "                        f = 2 * p * r / (p + r) if (p != 0.0 and r != 0.0) else 0.0\n",
    "                        sys.stdout.write(\"    Rhyme P/R/F@%.1f  = %.3f / %.3f / %.3f\\n\" % (t, p, r, f))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    # return avg batch loss for lm, pm and rm\n",
    "    return lm_costs / max(word_batch_id, 1), pm_costs / max(char_batch_id, 1), rm_costs / max(rhyme_batch_id, 1), \\\n",
    "           rhyme_pattern, (np.mean(stress_acc[-1]) if not is_training else 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def pad(lst, max_len, pad_symbol):\n",
    "    if len(lst) > max_len:\n",
    "        print(\"\\nERROR: padding\")\n",
    "        print(\"length of list greater than maxlen; list =\", lst, \"; maxlen =\", max_len)\n",
    "        raise SystemExit\n",
    "    return lst + [pad_symbol] * (max_len - len(lst))\n",
    "\n",
    "def coverage_mask(char_ids, idxchar):\n",
    "    vowels = get_vowels()\n",
    "    return [ float(idxchar[c] in vowels) for c in char_ids ]\n",
    "\n",
    "def get_vowels():\n",
    "    return set([\"a\", \"e\", \"i\", \"o\", \"u\"])\n",
    "\n",
    "\n",
    "\n",
    "def collect_rhyme_pattern(rhyme_pattern, attentions, b, batch_size, num_context, idxchar, pad_id):\n",
    "\n",
    "    def get_context_line_id(target_line_id, context_id):\n",
    "        p = target_line_id % 4\n",
    "        q = 3-context_id\n",
    "        if q <= p:\n",
    "            q -= 1\n",
    "        return q\n",
    "\n",
    "    #print \"\\n\", \"=\"*100\n",
    "    for exid in range(batch_size):\n",
    "        target_line_id = b[2][exid]\n",
    "        for ci, c in enumerate(b[0][(exid*num_context+batch_size):((exid+1)*num_context+batch_size)][:3]):\n",
    "            context_line_id = get_context_line_id(target_line_id, ci)\n",
    "            rhyme_pattern[target_line_id][context_line_id].append(attentions[exid][ci])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SonnetModel' object has no attribute 'init_pm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8b76ec288cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n\u001b[0;32m---> 23\u001b[0;31m                              charxid[\" \"], charxid[pad_symbol], cf)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#     with tf.variable_scope(\"model\", reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c3b036d3239e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, batch_size, word_type_size, char_type_size, space_id, pad_id, cf)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pentameter_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_pm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_type_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SonnetModel' object has no attribute 'init_pm'"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "\n",
    "        #         if cf.save_model:\n",
    "        #             if not os.path.exists(os.path.join(cf.output_dir, cf.output_prefix)):\n",
    "        #                 os.makedirs(os.path.join(cf.output_dir, cf.output_prefix))\n",
    "        #             # create saver object to save model\n",
    "        #             saver = tf.train.Saver(max_to_keep=0)\n",
    "\n",
    "        # train model\n",
    "        \n",
    "        \n",
    "# training parameters\n",
    "train_lm = True\n",
    "train_pm = True\n",
    "train_rm = True\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    tf.set_random_seed(cf.seed)\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n",
    "                             charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\n",
    "#                              charxid[\" \"], charxid[pad_symbol], cf)\n",
    "#     with tf.variable_scope(\"model\", reuse=True):\n",
    "#         mgen = SonnetModel(False, 1, len(idxword), len(\n",
    "#             idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # initialise word embedding\n",
    "    if cf.word_embedding_model:\n",
    "        word_emb = init_embedding(mword, idxword)\n",
    "        sess.run(mtrain.word_embedding.assign(word_emb))\n",
    "    # train model\n",
    "    prev_lm_loss, prev_pm_loss, prev_rm_loss, rhyme_pattern = None, None, None, None\n",
    "    for i in range(1):\n",
    "\n",
    "        print(\"\\nEpoch =\", i + 1)\n",
    "\n",
    "        # create batches for language model\n",
    "        train_word_batch = create_word_batch(train_word_data, cf.batch_size,\n",
    "                                             cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "                                             wordxid[end_symbol], wordxid[unk_symbol], True)\n",
    "#         valid_word_batch = create_word_batch(train_word_data, 1,\n",
    "#                                              cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "#                                              wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "\n",
    "#         # create batches for pentameter model\n",
    "        train_char_batch = create_char_batch(train_char_data, cf.batch_size,\n",
    "                                             charxid[pad_symbol], mtrain.pentameter, idxchar, True)\n",
    "#         valid_char_batch = create_char_batch(valid_char_data, cf.batch_size,\n",
    "#                                              charxid[pad_symbol], mtrain.pentameter, idxchar, False)\n",
    "\n",
    "#         # create batches for rhyme model\n",
    "        train_rhyme_batch = create_rhyme_batch(train_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "                                               cf.rm_neg, True)\n",
    "#         valid_rhyme_batch = create_rhyme_batch(valid_rhyme_data, cf.batch_size, charxid[pad_symbol], wordxchar,\n",
    "#                                                cf.rm_neg, False)\n",
    "\n",
    "        # train an epoch\n",
    "        _, _, _, new_rhyme_pattern, _ = run_epoch(sess, train_word_batch, train_char_batch,\n",
    "                                                  train_rhyme_batch, mtrain, \"TRAIN\", True)\n",
    "#         lm_loss, pm_loss, rm_loss, _, sacc = run_epoch(sess, valid_word_batch, valid_char_batch,\n",
    "#                                                        valid_rhyme_batch, mvalid, \"VALID\", False)\n",
    "\n",
    "#         # create batch for test model and run an epoch if it's given\n",
    "#         if cf.test_data:\n",
    "#             test_word_batch = create_word_batch(test_word_data, cf.batch_size,\n",
    "#                                                 cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol],\n",
    "#                                                 wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "#             test_char_batch = create_char_batch(test_char_data, cf.batch_size,\n",
    "#                                                 charxid[pad_symbol], mtrain.pentameter, idxchar, False)\n",
    "#             test_rhyme_batch = create_rhyme_batch(test_rhyme_data, cf.batch_size,\n",
    "#                                                   charxid[pad_symbol], wordxchar, cf.rm_neg, False)\n",
    "#             run_epoch(sess, test_word_batch, test_char_batch,\n",
    "#                       test_rhyme_batch, mvalid, \"TEST\", False)\n",
    "\n",
    "#         # if pm performance is really poor, re-initialize network weights\n",
    "#         if train_pm and sacc < stress_acc_threshold and prev_pm_loss == None:\n",
    "#             print(\n",
    "#                 \"\\n  Valid stress accuracy performance is very poor; re-initializing network with random weights...\")\n",
    "#             tf.global_variables_initializer().run()\n",
    "#             continue\n",
    "\n",
    "#         # save model\n",
    "#         if cf.save_model:\n",
    "#             if prev_lm_loss == None or prev_pm_loss == None or prev_rm_loss == None or \\\n",
    "#                     ((lm_loss <= prev_lm_loss or not train_lm) and\n",
    "#                      (pm_loss <= prev_pm_loss * reset_scale or not train_pm) and\n",
    "#                      (rm_loss <= prev_rm_loss * reset_scale or not train_rm)):\n",
    "#                 saver.save(sess, os.path.join(\n",
    "#                     cf.output_dir, cf.output_prefix, \"model.ckpt\"))\n",
    "#                 prev_lm_loss, prev_pm_loss, prev_rm_loss = lm_loss, pm_loss, rm_loss\n",
    "#                 rhyme_pattern = new_rhyme_pattern\n",
    "#             else:\n",
    "#                 saver.restore(sess, os.path.join(\n",
    "#                     cf.output_dir, cf.output_prefix, \"model.ckpt\"))\n",
    "#                 print(\n",
    "#                     \"New valid performance is worse; restoring previous parameters...\")\n",
    "#                 print(\"  lm loss: %.5f --> %.5f\" % (prev_lm_loss, lm_loss))\n",
    "#                 print(\"  pm loss: %.5f --> %.5f\" % (prev_pm_loss, pm_loss))\n",
    "#                 print(\"  rm loss: %.5f --> %.5f\" % (prev_rm_loss, rm_loss))\n",
    "#                 sys.stdout.flush()\n",
    "#         else:\n",
    "#             rhyme_pattern = new_rhyme_pattern\n",
    "\n",
    "#     # print global rhyme pattern\n",
    "#     if train_rm:\n",
    "#         print(\"\\nAggregated Rhyme Pattern:\")\n",
    "#         for i in range(len(rhyme_pattern)):\n",
    "#             if i % 4 == 0:\n",
    "#                 print(\"\\n  Quatrain\", i / 4, \":\")\n",
    "#             print(\"    Line %02d =\" % i),\n",
    "#             for j in range(len(rhyme_pattern[i])):\n",
    "#                 print(\"%.2f\" % np.mean(rhyme_pattern[i][j])).rjust(7),\n",
    "#         print\n",
    "\n",
    "#     # save vocab information and config\n",
    "#     if cf.save_model:\n",
    "#         # vocab\n",
    "#         cPickle.dump((idxword, idxchar, wordxchar),\n",
    "#                      open(os.path.join(cf.output_dir, cf.output_prefix, \"vocabs.pickle\"), \"w\"))\n",
    "\n",
    "#         # create a dictionary object for config\n",
    "#         cf_dict = {}\n",
    "#         for k, v in vars(cf).items():\n",
    "#             if not k.startswith(\"__\"):\n",
    "#                 cf_dict[k] = v\n",
    "#         cPickle.dump(cf_dict, open(os.path.join(\n",
    "#             cf.output_dir, cf.output_prefix, \"config.pickle\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
